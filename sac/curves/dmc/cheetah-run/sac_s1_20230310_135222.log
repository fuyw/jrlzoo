2023-03-10 13:52:22 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: cheetah-run
eval_episodes: 10
eval_freq: 5000
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: orthogonal
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
seed: 1
start_timesteps: 10000
tau: 0.005

2023-03-10 13:52:33 - 
[#Step 10000] eval_reward: 1.571, eval_time: 3

2023-03-10 13:52:48 - 
[#Step 20000] eval_reward: 11.732, eval_step: 1000, eval_time: 3, time: 0.436
	actor_loss: -47.789, critic_loss: 0.039, alpha_loss: 0.655
	q1: 47.428, target_q: 47.460, logp: -3.978, alpha: 0.094
	batch_reward: 0.004, batch_reward_max: 0.091, batch_reward_min: 0.000

2023-03-10 13:53:02 - 
[#Step 30000] eval_reward: 103.449, eval_step: 1000, eval_time: 3, time: 0.659
	actor_loss: -33.712, critic_loss: 0.025, alpha_loss: 0.053
	q1: 33.685, target_q: 33.684, logp: -1.935, alpha: 0.011
	batch_reward: 0.012, batch_reward_max: 0.159, batch_reward_min: 0.000

2023-03-10 13:53:15 - 
[#Step 40000] eval_reward: 78.493, eval_step: 1000, eval_time: 3, time: 0.882
	actor_loss: -23.846, critic_loss: 0.027, alpha_loss: -0.002
	q1: 23.772, target_q: 23.746, logp: 3.286, alpha: 0.006
	batch_reward: 0.025, batch_reward_max: 0.289, batch_reward_min: 0.000

2023-03-10 13:53:28 - 
[#Step 50000] eval_reward: 102.585, eval_step: 1000, eval_time: 3, time: 1.105
	actor_loss: -18.454, critic_loss: 0.031, alpha_loss: -0.000
	q1: 18.386, target_q: 18.354, logp: 3.014, alpha: 0.005
	batch_reward: 0.030, batch_reward_max: 0.261, batch_reward_min: 0.000

2023-03-10 13:53:42 - 
[#Step 60000] eval_reward: 179.310, eval_step: 1000, eval_time: 3, time: 1.327
	actor_loss: -15.607, critic_loss: 0.036, alpha_loss: 0.002
	q1: 15.535, target_q: 15.543, logp: 2.715, alpha: 0.005
	batch_reward: 0.045, batch_reward_max: 0.303, batch_reward_min: 0.000

2023-03-10 13:53:55 - 
[#Step 70000] eval_reward: 199.876, eval_step: 1000, eval_time: 3, time: 1.555
	actor_loss: -14.082, critic_loss: 0.071, alpha_loss: -0.001
	q1: 14.025, target_q: 13.970, logp: 3.123, alpha: 0.006
	batch_reward: 0.060, batch_reward_max: 0.350, batch_reward_min: 0.000

2023-03-10 13:54:09 - 
[#Step 80000] eval_reward: 249.184, eval_step: 1000, eval_time: 3, time: 1.778
	actor_loss: -14.709, critic_loss: 0.053, alpha_loss: 0.001
	q1: 14.644, target_q: 14.641, logp: 2.901, alpha: 0.006
	batch_reward: 0.081, batch_reward_max: 0.386, batch_reward_min: 0.000

2023-03-10 13:54:22 - 
[#Step 90000] eval_reward: 305.871, eval_step: 1000, eval_time: 3, time: 2.001
	actor_loss: -14.720, critic_loss: 0.109, alpha_loss: -0.004
	q1: 14.696, target_q: 14.725, logp: 3.691, alpha: 0.006
	batch_reward: 0.096, batch_reward_max: 0.374, batch_reward_min: 0.000

2023-03-10 13:54:35 - 
[#Step 100000] eval_reward: 328.715, eval_step: 1000, eval_time: 3, time: 2.223
	actor_loss: -14.432, critic_loss: 0.067, alpha_loss: -0.000
	q1: 14.350, target_q: 14.373, logp: 3.003, alpha: 0.007
	batch_reward: 0.096, batch_reward_max: 0.416, batch_reward_min: 0.000

2023-03-10 13:54:49 - 
[#Step 110000] eval_reward: 359.232, eval_step: 1000, eval_time: 3, time: 2.445
	actor_loss: -16.860, critic_loss: 0.081, alpha_loss: 0.002
	q1: 16.792, target_q: 16.856, logp: 2.720, alpha: 0.007
	batch_reward: 0.132, batch_reward_max: 0.451, batch_reward_min: 0.000

2023-03-10 13:55:02 - 
[#Step 120000] eval_reward: 369.555, eval_step: 1000, eval_time: 3, time: 2.671
	actor_loss: -18.320, critic_loss: 0.092, alpha_loss: 0.001
	q1: 18.241, target_q: 18.267, logp: 2.934, alpha: 0.008
	batch_reward: 0.146, batch_reward_max: 0.462, batch_reward_min: 0.000

2023-03-10 13:55:15 - 
[#Step 130000] eval_reward: 369.923, eval_step: 1000, eval_time: 3, time: 2.891
	actor_loss: -19.196, critic_loss: 0.085, alpha_loss: 0.003
	q1: 19.139, target_q: 19.133, logp: 2.670, alpha: 0.009
	batch_reward: 0.154, batch_reward_max: 0.528, batch_reward_min: 0.000

2023-03-10 13:55:29 - 
[#Step 140000] eval_reward: 413.041, eval_step: 1000, eval_time: 3, time: 3.117
	actor_loss: -20.448, critic_loss: 0.109, alpha_loss: 0.005
	q1: 20.388, target_q: 20.415, logp: 2.455, alpha: 0.009
	batch_reward: 0.163, batch_reward_max: 0.510, batch_reward_min: 0.000

2023-03-10 13:55:42 - 
[#Step 150000] eval_reward: 442.152, eval_step: 1000, eval_time: 3, time: 3.340
	actor_loss: -22.444, critic_loss: 0.080, alpha_loss: 0.004
	q1: 22.385, target_q: 22.370, logp: 2.579, alpha: 0.009
	batch_reward: 0.183, batch_reward_max: 0.549, batch_reward_min: 0.000

2023-03-10 13:55:56 - 
[#Step 160000] eval_reward: 457.410, eval_step: 1000, eval_time: 3, time: 3.566
	actor_loss: -24.173, critic_loss: 0.131, alpha_loss: 0.004
	q1: 24.137, target_q: 24.093, logp: 2.606, alpha: 0.011
	batch_reward: 0.205, batch_reward_max: 0.579, batch_reward_min: 0.000

2023-03-10 13:56:09 - 
[#Step 170000] eval_reward: 362.131, eval_step: 1000, eval_time: 3, time: 3.789
	actor_loss: -23.805, critic_loss: 0.155, alpha_loss: 0.004
	q1: 23.790, target_q: 23.821, logp: 2.621, alpha: 0.011
	batch_reward: 0.205, batch_reward_max: 0.589, batch_reward_min: 0.000

2023-03-10 13:56:23 - 
[#Step 180000] eval_reward: 504.234, eval_step: 1000, eval_time: 3, time: 4.015
	actor_loss: -26.609, critic_loss: 0.109, alpha_loss: -0.009
	q1: 26.583, target_q: 26.583, logp: 3.798, alpha: 0.011
	batch_reward: 0.226, batch_reward_max: 0.579, batch_reward_min: 0.000

2023-03-10 13:56:36 - 
[#Step 190000] eval_reward: 511.650, eval_step: 1000, eval_time: 3, time: 4.238
	actor_loss: -27.612, critic_loss: 0.134, alpha_loss: 0.002
	q1: 27.597, target_q: 27.625, logp: 2.863, alpha: 0.012
	batch_reward: 0.247, batch_reward_max: 0.621, batch_reward_min: 0.000

2023-03-10 13:56:50 - 
[#Step 200000] eval_reward: 538.804, eval_step: 1000, eval_time: 3, time: 4.462
	actor_loss: -29.147, critic_loss: 0.264, alpha_loss: -0.006
	q1: 29.132, target_q: 29.121, logp: 3.505, alpha: 0.012
	batch_reward: 0.265, batch_reward_max: 0.661, batch_reward_min: 0.000

2023-03-10 13:56:50 - Saving checkpoint at step: 1
2023-03-10 13:56:50 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/actor_1
2023-03-10 13:56:50 - Saving checkpoint at step: 1
2023-03-10 13:56:50 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/critic_1
2023-03-10 13:57:03 - 
[#Step 210000] eval_reward: 562.899, eval_step: 1000, eval_time: 3, time: 4.688
	actor_loss: -29.909, critic_loss: 0.249, alpha_loss: 0.000
	q1: 29.900, target_q: 29.827, logp: 2.974, alpha: 0.012
	batch_reward: 0.261, batch_reward_max: 0.669, batch_reward_min: 0.000

2023-03-10 13:57:17 - 
[#Step 220000] eval_reward: 572.017, eval_step: 1000, eval_time: 3, time: 4.909
	actor_loss: -31.108, critic_loss: 0.117, alpha_loss: 0.007
	q1: 31.076, target_q: 30.990, logp: 2.475, alpha: 0.013
	batch_reward: 0.270, batch_reward_max: 0.734, batch_reward_min: 0.000

2023-03-10 13:57:30 - 
[#Step 230000] eval_reward: 581.196, eval_step: 1000, eval_time: 3, time: 5.134
	actor_loss: -33.837, critic_loss: 0.127, alpha_loss: 0.009
	q1: 33.806, target_q: 33.784, logp: 2.334, alpha: 0.013
	batch_reward: 0.302, batch_reward_max: 0.741, batch_reward_min: 0.000

2023-03-10 13:57:43 - 
[#Step 240000] eval_reward: 625.883, eval_step: 1000, eval_time: 3, time: 5.358
	actor_loss: -34.765, critic_loss: 0.128, alpha_loss: 0.004
	q1: 34.768, target_q: 34.796, logp: 2.719, alpha: 0.014
	batch_reward: 0.306, batch_reward_max: 0.688, batch_reward_min: 0.000

2023-03-10 13:57:57 - 
[#Step 250000] eval_reward: 644.917, eval_step: 1000, eval_time: 3, time: 5.583
	actor_loss: -38.299, critic_loss: 0.145, alpha_loss: -0.002
	q1: 38.294, target_q: 38.305, logp: 3.114, alpha: 0.014
	batch_reward: 0.348, batch_reward_max: 0.711, batch_reward_min: 0.000

2023-03-10 13:58:11 - 
[#Step 260000] eval_reward: 633.029, eval_step: 1000, eval_time: 3, time: 5.810
	actor_loss: -36.470, critic_loss: 0.153, alpha_loss: 0.009
	q1: 36.486, target_q: 36.436, logp: 2.316, alpha: 0.014
	batch_reward: 0.317, batch_reward_max: 0.753, batch_reward_min: 0.000

2023-03-10 13:58:24 - 
[#Step 270000] eval_reward: 644.802, eval_step: 1000, eval_time: 3, time: 6.034
	actor_loss: -39.003, critic_loss: 0.231, alpha_loss: -0.007
	q1: 38.974, target_q: 39.014, logp: 3.450, alpha: 0.015
	batch_reward: 0.340, batch_reward_max: 0.786, batch_reward_min: 0.000

2023-03-10 13:58:38 - 
[#Step 280000] eval_reward: 679.746, eval_step: 1000, eval_time: 3, time: 6.258
	actor_loss: -38.089, critic_loss: 0.326, alpha_loss: 0.009
	q1: 38.099, target_q: 38.108, logp: 2.419, alpha: 0.015
	batch_reward: 0.333, batch_reward_max: 0.821, batch_reward_min: 0.000

2023-03-10 13:58:51 - 
[#Step 290000] eval_reward: 687.668, eval_step: 1000, eval_time: 3, time: 6.484
	actor_loss: -41.415, critic_loss: 0.206, alpha_loss: -0.003
	q1: 41.366, target_q: 41.262, logp: 3.169, alpha: 0.015
	batch_reward: 0.368, batch_reward_max: 0.840, batch_reward_min: 0.000

2023-03-10 13:59:05 - 
[#Step 300000] eval_reward: 708.504, eval_step: 1000, eval_time: 3, time: 6.711
	actor_loss: -42.688, critic_loss: 0.205, alpha_loss: 0.010
	q1: 42.670, target_q: 42.660, logp: 2.322, alpha: 0.015
	batch_reward: 0.376, batch_reward_max: 0.828, batch_reward_min: 0.000

2023-03-10 13:59:18 - 
[#Step 310000] eval_reward: 710.147, eval_step: 1000, eval_time: 3, time: 6.937
	actor_loss: -43.891, critic_loss: 0.158, alpha_loss: 0.003
	q1: 43.821, target_q: 43.839, logp: 2.807, alpha: 0.015
	batch_reward: 0.377, batch_reward_max: 0.850, batch_reward_min: 0.000

2023-03-10 13:59:32 - 
[#Step 320000] eval_reward: 744.438, eval_step: 1000, eval_time: 3, time: 7.162
	actor_loss: -45.523, critic_loss: 0.296, alpha_loss: -0.001
	q1: 45.502, target_q: 45.401, logp: 3.082, alpha: 0.015
	batch_reward: 0.400, batch_reward_max: 0.847, batch_reward_min: 0.000

2023-03-10 13:59:45 - 
[#Step 330000] eval_reward: 724.119, eval_step: 1000, eval_time: 3, time: 7.385
	actor_loss: -45.941, critic_loss: 0.145, alpha_loss: 0.003
	q1: 45.921, target_q: 45.924, logp: 2.809, alpha: 0.015
	batch_reward: 0.391, batch_reward_max: 0.866, batch_reward_min: 0.000

2023-03-10 13:59:59 - 
[#Step 340000] eval_reward: 750.111, eval_step: 1000, eval_time: 3, time: 7.613
	actor_loss: -48.002, critic_loss: 0.154, alpha_loss: -0.005
	q1: 47.999, target_q: 48.004, logp: 3.360, alpha: 0.015
	batch_reward: 0.430, batch_reward_max: 0.874, batch_reward_min: 0.000

2023-03-10 14:00:12 - 
[#Step 350000] eval_reward: 762.531, eval_step: 1000, eval_time: 3, time: 7.837
	actor_loss: -48.497, critic_loss: 0.158, alpha_loss: 0.003
	q1: 48.444, target_q: 48.466, logp: 2.823, alpha: 0.015
	batch_reward: 0.416, batch_reward_max: 0.873, batch_reward_min: 0.000

2023-03-10 14:00:26 - 
[#Step 360000] eval_reward: 761.217, eval_step: 1000, eval_time: 3, time: 8.062
	actor_loss: -51.717, critic_loss: 0.182, alpha_loss: 0.001
	q1: 51.694, target_q: 51.612, logp: 2.910, alpha: 0.015
	batch_reward: 0.459, batch_reward_max: 0.880, batch_reward_min: 0.000

2023-03-10 14:00:39 - 
[#Step 370000] eval_reward: 757.654, eval_step: 1000, eval_time: 3, time: 8.288
	actor_loss: -49.768, critic_loss: 0.126, alpha_loss: 0.006
	q1: 49.726, target_q: 49.760, logp: 2.615, alpha: 0.015
	batch_reward: 0.424, batch_reward_max: 0.906, batch_reward_min: 0.000

2023-03-10 14:00:53 - 
[#Step 380000] eval_reward: 773.781, eval_step: 1000, eval_time: 3, time: 8.516
	actor_loss: -50.946, critic_loss: 0.159, alpha_loss: -0.001
	q1: 50.925, target_q: 50.896, logp: 3.085, alpha: 0.015
	batch_reward: 0.446, batch_reward_max: 0.888, batch_reward_min: 0.000

2023-03-10 14:01:06 - 
[#Step 390000] eval_reward: 780.100, eval_step: 1000, eval_time: 3, time: 8.740
	actor_loss: -53.112, critic_loss: 0.145, alpha_loss: 0.002
	q1: 53.119, target_q: 53.093, logp: 2.854, alpha: 0.015
	batch_reward: 0.476, batch_reward_max: 0.926, batch_reward_min: 0.000

2023-03-10 14:01:20 - 
[#Step 400000] eval_reward: 781.810, eval_step: 1000, eval_time: 3, time: 8.963
	actor_loss: -54.748, critic_loss: 0.113, alpha_loss: 0.001
	q1: 54.745, target_q: 54.787, logp: 2.915, alpha: 0.015
	batch_reward: 0.500, batch_reward_max: 0.896, batch_reward_min: 0.000

2023-03-10 14:01:20 - Saving checkpoint at step: 2
2023-03-10 14:01:20 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/actor_2
2023-03-10 14:01:20 - Saving checkpoint at step: 2
2023-03-10 14:01:20 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/critic_2
2023-03-10 14:01:33 - 
[#Step 410000] eval_reward: 772.033, eval_step: 1000, eval_time: 3, time: 9.187
	actor_loss: -56.138, critic_loss: 0.133, alpha_loss: -0.003
	q1: 56.138, target_q: 56.122, logp: 3.194, alpha: 0.015
	batch_reward: 0.504, batch_reward_max: 0.891, batch_reward_min: 0.000

2023-03-10 14:01:46 - 
[#Step 420000] eval_reward: 779.570, eval_step: 1000, eval_time: 3, time: 9.407
	actor_loss: -55.740, critic_loss: 0.218, alpha_loss: -0.002
	q1: 55.749, target_q: 55.737, logp: 3.157, alpha: 0.015
	batch_reward: 0.504, batch_reward_max: 0.892, batch_reward_min: 0.000

2023-03-10 14:02:00 - 
[#Step 430000] eval_reward: 692.160, eval_step: 1000, eval_time: 3, time: 9.635
	actor_loss: -53.822, critic_loss: 0.153, alpha_loss: -0.006
	q1: 53.824, target_q: 53.762, logp: 3.393, alpha: 0.016
	batch_reward: 0.472, batch_reward_max: 0.910, batch_reward_min: 0.000

2023-03-10 14:02:13 - 
[#Step 440000] eval_reward: 769.831, eval_step: 1000, eval_time: 3, time: 9.856
	actor_loss: -55.245, critic_loss: 0.354, alpha_loss: 0.001
	q1: 55.249, target_q: 55.249, logp: 2.935, alpha: 0.016
	batch_reward: 0.491, batch_reward_max: 0.927, batch_reward_min: 0.000

2023-03-10 14:02:27 - 
[#Step 450000] eval_reward: 711.429, eval_step: 1000, eval_time: 3, time: 10.080
	actor_loss: -55.112, critic_loss: 0.203, alpha_loss: 0.001
	q1: 55.050, target_q: 55.111, logp: 2.967, alpha: 0.015
	batch_reward: 0.498, batch_reward_max: 0.902, batch_reward_min: 0.000

2023-03-10 14:02:40 - 
[#Step 460000] eval_reward: 762.677, eval_step: 1000, eval_time: 3, time: 10.303
	actor_loss: -54.810, critic_loss: 0.173, alpha_loss: 0.000
	q1: 54.788, target_q: 54.856, logp: 2.993, alpha: 0.015
	batch_reward: 0.484, batch_reward_max: 0.943, batch_reward_min: 0.000

2023-03-10 14:02:54 - 
[#Step 470000] eval_reward: 791.893, eval_step: 1000, eval_time: 3, time: 10.528
	actor_loss: -55.677, critic_loss: 0.253, alpha_loss: 0.004
	q1: 55.664, target_q: 55.683, logp: 2.734, alpha: 0.015
	batch_reward: 0.497, batch_reward_max: 0.915, batch_reward_min: 0.000

2023-03-10 14:03:07 - 
[#Step 480000] eval_reward: 788.462, eval_step: 1000, eval_time: 3, time: 10.753
	actor_loss: -55.260, critic_loss: 0.583, alpha_loss: -0.001
	q1: 55.222, target_q: 55.217, logp: 3.040, alpha: 0.016
	batch_reward: 0.488, batch_reward_max: 0.937, batch_reward_min: 0.000

2023-03-10 14:03:20 - 
[#Step 490000] eval_reward: 783.092, eval_step: 1000, eval_time: 3, time: 10.973
	actor_loss: -58.181, critic_loss: 0.159, alpha_loss: -0.000
	q1: 58.208, target_q: 58.222, logp: 3.026, alpha: 0.016
	batch_reward: 0.535, batch_reward_max: 0.928, batch_reward_min: 0.000

2023-03-10 14:03:34 - 
[#Step 500000] eval_reward: 795.261, eval_step: 1000, eval_time: 3, time: 11.196
	actor_loss: -58.217, critic_loss: 0.435, alpha_loss: -0.005
	q1: 58.198, target_q: 58.104, logp: 3.294, alpha: 0.016
	batch_reward: 0.534, batch_reward_max: 0.938, batch_reward_min: 0.000

2023-03-10 14:03:47 - 
[#Step 510000] eval_reward: 793.114, eval_step: 1000, eval_time: 3, time: 11.418
	actor_loss: -56.134, critic_loss: 0.139, alpha_loss: 0.001
	q1: 56.094, target_q: 56.038, logp: 2.922, alpha: 0.016
	batch_reward: 0.505, batch_reward_max: 0.935, batch_reward_min: 0.000

2023-03-10 14:04:01 - 
[#Step 520000] eval_reward: 766.119, eval_step: 1000, eval_time: 3, time: 11.643
	actor_loss: -61.407, critic_loss: 0.202, alpha_loss: -0.004
	q1: 61.415, target_q: 61.438, logp: 3.260, alpha: 0.016
	batch_reward: 0.562, batch_reward_max: 0.938, batch_reward_min: 0.000

2023-03-10 14:04:14 - 
[#Step 530000] eval_reward: 790.745, eval_step: 1000, eval_time: 3, time: 11.866
	actor_loss: -60.651, critic_loss: 0.148, alpha_loss: 0.001
	q1: 60.610, target_q: 60.618, logp: 2.960, alpha: 0.016
	batch_reward: 0.556, batch_reward_max: 0.925, batch_reward_min: 0.000

2023-03-10 14:04:27 - 
[#Step 540000] eval_reward: 800.878, eval_step: 1000, eval_time: 3, time: 12.090
	actor_loss: -59.075, critic_loss: 0.148, alpha_loss: -0.000
	q1: 59.087, target_q: 59.075, logp: 3.018, alpha: 0.015
	batch_reward: 0.542, batch_reward_max: 0.940, batch_reward_min: 0.000

2023-03-10 14:04:41 - 
[#Step 550000] eval_reward: 803.821, eval_step: 1000, eval_time: 3, time: 12.314
	actor_loss: -61.070, critic_loss: 0.139, alpha_loss: 0.003
	q1: 61.099, target_q: 61.142, logp: 2.834, alpha: 0.016
	batch_reward: 0.570, batch_reward_max: 0.973, batch_reward_min: 0.000

2023-03-10 14:04:54 - 
[#Step 560000] eval_reward: 798.738, eval_step: 1000, eval_time: 3, time: 12.539
	actor_loss: -64.005, critic_loss: 0.190, alpha_loss: -0.012
	q1: 64.018, target_q: 64.030, logp: 3.739, alpha: 0.016
	batch_reward: 0.608, batch_reward_max: 0.929, batch_reward_min: 0.000

2023-03-10 14:05:08 - 
[#Step 570000] eval_reward: 758.112, eval_step: 1000, eval_time: 3, time: 12.765
	actor_loss: -61.253, critic_loss: 0.174, alpha_loss: 0.008
	q1: 61.229, target_q: 61.205, logp: 2.484, alpha: 0.016
	batch_reward: 0.562, batch_reward_max: 0.982, batch_reward_min: 0.000

2023-03-10 14:05:22 - 
[#Step 580000] eval_reward: 780.804, eval_step: 1000, eval_time: 3, time: 12.994
	actor_loss: -60.883, critic_loss: 0.219, alpha_loss: -0.010
	q1: 60.854, target_q: 60.886, logp: 3.590, alpha: 0.016
	batch_reward: 0.554, batch_reward_max: 0.973, batch_reward_min: 0.000

2023-03-10 14:05:35 - 
[#Step 590000] eval_reward: 772.098, eval_step: 1000, eval_time: 3, time: 13.215
	actor_loss: -62.374, critic_loss: 0.173, alpha_loss: 0.001
	q1: 62.369, target_q: 62.393, logp: 2.944, alpha: 0.016
	batch_reward: 0.575, batch_reward_max: 0.935, batch_reward_min: 0.000

2023-03-10 14:05:48 - 
[#Step 600000] eval_reward: 792.344, eval_step: 1000, eval_time: 3, time: 13.437
	actor_loss: -60.833, critic_loss: 0.151, alpha_loss: 0.003
	q1: 60.825, target_q: 60.951, logp: 2.783, alpha: 0.016
	batch_reward: 0.553, batch_reward_max: 0.951, batch_reward_min: 0.000

2023-03-10 14:05:48 - Saving checkpoint at step: 3
2023-03-10 14:05:48 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/actor_3
2023-03-10 14:05:48 - Saving checkpoint at step: 3
2023-03-10 14:05:48 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/critic_3
2023-03-10 14:06:02 - 
[#Step 610000] eval_reward: 575.253, eval_step: 1000, eval_time: 3, time: 13.661
	actor_loss: -63.091, critic_loss: 0.212, alpha_loss: -0.009
	q1: 63.049, target_q: 63.152, logp: 3.544, alpha: 0.016
	batch_reward: 0.583, batch_reward_max: 0.926, batch_reward_min: 0.000

2023-03-10 14:06:15 - 
[#Step 620000] eval_reward: 804.105, eval_step: 1000, eval_time: 3, time: 13.882
	actor_loss: -63.786, critic_loss: 0.141, alpha_loss: 0.006
	q1: 63.823, target_q: 63.800, logp: 2.626, alpha: 0.015
	batch_reward: 0.594, batch_reward_max: 0.972, batch_reward_min: 0.000

2023-03-10 14:06:28 - 
[#Step 630000] eval_reward: 790.960, eval_step: 1000, eval_time: 3, time: 14.107
	actor_loss: -63.038, critic_loss: 0.371, alpha_loss: 0.003
	q1: 63.114, target_q: 63.068, logp: 2.810, alpha: 0.016
	batch_reward: 0.586, batch_reward_max: 0.946, batch_reward_min: 0.000

2023-03-10 14:06:42 - 
[#Step 640000] eval_reward: 784.693, eval_step: 1000, eval_time: 3, time: 14.329
	actor_loss: -62.613, critic_loss: 0.245, alpha_loss: -0.005
	q1: 62.608, target_q: 62.623, logp: 3.317, alpha: 0.016
	batch_reward: 0.582, batch_reward_max: 0.942, batch_reward_min: 0.000

2023-03-10 14:06:55 - 
[#Step 650000] eval_reward: 822.454, eval_step: 1000, eval_time: 3, time: 14.553
	actor_loss: -65.120, critic_loss: 0.229, alpha_loss: -0.009
	q1: 65.124, target_q: 65.071, logp: 3.539, alpha: 0.016
	batch_reward: 0.616, batch_reward_max: 0.958, batch_reward_min: 0.000

2023-03-10 14:07:09 - 
[#Step 660000] eval_reward: 831.988, eval_step: 1000, eval_time: 3, time: 14.778
	actor_loss: -64.137, critic_loss: 0.359, alpha_loss: -0.003
	q1: 64.117, target_q: 64.303, logp: 3.220, alpha: 0.016
	batch_reward: 0.594, batch_reward_max: 0.954, batch_reward_min: 0.000

2023-03-10 14:07:22 - 
[#Step 670000] eval_reward: 800.171, eval_step: 1000, eval_time: 3, time: 15.003
	actor_loss: -64.207, critic_loss: 0.131, alpha_loss: -0.002
	q1: 64.177, target_q: 64.186, logp: 3.147, alpha: 0.016
	batch_reward: 0.596, batch_reward_max: 0.963, batch_reward_min: 0.000

2023-03-10 14:07:36 - 
[#Step 680000] eval_reward: 830.240, eval_step: 1000, eval_time: 3, time: 15.227
	actor_loss: -64.579, critic_loss: 0.233, alpha_loss: -0.006
	q1: 64.599, target_q: 64.596, logp: 3.350, alpha: 0.016
	batch_reward: 0.600, batch_reward_max: 0.962, batch_reward_min: 0.000

2023-03-10 14:07:49 - 
[#Step 690000] eval_reward: 798.514, eval_step: 1000, eval_time: 3, time: 15.452
	actor_loss: -63.204, critic_loss: 0.174, alpha_loss: 0.001
	q1: 63.263, target_q: 63.193, logp: 2.962, alpha: 0.016
	batch_reward: 0.586, batch_reward_max: 0.942, batch_reward_min: 0.000

2023-03-10 14:08:03 - 
[#Step 700000] eval_reward: 780.964, eval_step: 1000, eval_time: 3, time: 15.676
	actor_loss: -66.628, critic_loss: 0.228, alpha_loss: -0.007
	q1: 66.644, target_q: 66.652, logp: 3.421, alpha: 0.015
	batch_reward: 0.631, batch_reward_max: 0.964, batch_reward_min: 0.000

2023-03-10 14:08:16 - 
[#Step 710000] eval_reward: 812.469, eval_step: 1000, eval_time: 3, time: 15.902
	actor_loss: -68.173, critic_loss: 0.184, alpha_loss: -0.005
	q1: 68.237, target_q: 68.178, logp: 3.328, alpha: 0.015
	batch_reward: 0.652, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:08:30 - 
[#Step 720000] eval_reward: 788.796, eval_step: 1000, eval_time: 3, time: 16.129
	actor_loss: -64.512, critic_loss: 0.227, alpha_loss: 0.007
	q1: 64.551, target_q: 64.565, logp: 2.577, alpha: 0.015
	batch_reward: 0.611, batch_reward_max: 0.968, batch_reward_min: 0.000

2023-03-10 14:08:43 - 
[#Step 730000] eval_reward: 821.498, eval_step: 1000, eval_time: 3, time: 16.354
	actor_loss: -65.218, critic_loss: 0.170, alpha_loss: -0.002
	q1: 65.202, target_q: 65.126, logp: 3.118, alpha: 0.015
	batch_reward: 0.606, batch_reward_max: 0.964, batch_reward_min: 0.000

2023-03-10 14:08:57 - 
[#Step 740000] eval_reward: 830.844, eval_step: 1000, eval_time: 3, time: 16.580
	actor_loss: -65.354, critic_loss: 0.810, alpha_loss: -0.006
	q1: 65.354, target_q: 65.380, logp: 3.360, alpha: 0.016
	batch_reward: 0.610, batch_reward_max: 0.976, batch_reward_min: 0.000

2023-03-10 14:09:10 - 
[#Step 750000] eval_reward: 831.653, eval_step: 1000, eval_time: 3, time: 16.805
	actor_loss: -66.386, critic_loss: 0.202, alpha_loss: 0.003
	q1: 66.360, target_q: 66.295, logp: 2.808, alpha: 0.016
	batch_reward: 0.619, batch_reward_max: 0.958, batch_reward_min: 0.000

2023-03-10 14:09:24 - 
[#Step 760000] eval_reward: 829.127, eval_step: 1000, eval_time: 3, time: 17.028
	actor_loss: -66.564, critic_loss: 0.549, alpha_loss: -0.006
	q1: 66.630, target_q: 66.730, logp: 3.418, alpha: 0.015
	batch_reward: 0.628, batch_reward_max: 0.939, batch_reward_min: 0.000

2023-03-10 14:09:37 - 
[#Step 770000] eval_reward: 836.240, eval_step: 1000, eval_time: 3, time: 17.250
	actor_loss: -66.635, critic_loss: 0.333, alpha_loss: -0.000
	q1: 66.657, target_q: 66.673, logp: 3.033, alpha: 0.015
	batch_reward: 0.628, batch_reward_max: 0.953, batch_reward_min: 0.000

2023-03-10 14:09:50 - 
[#Step 780000] eval_reward: 835.669, eval_step: 1000, eval_time: 3, time: 17.474
	actor_loss: -66.703, critic_loss: 0.117, alpha_loss: 0.002
	q1: 66.725, target_q: 66.771, logp: 2.862, alpha: 0.015
	batch_reward: 0.635, batch_reward_max: 0.971, batch_reward_min: 0.000

2023-03-10 14:10:04 - 
[#Step 790000] eval_reward: 812.224, eval_step: 1000, eval_time: 3, time: 17.701
	actor_loss: -66.048, critic_loss: 0.146, alpha_loss: 0.007
	q1: 66.043, target_q: 66.029, logp: 2.526, alpha: 0.015
	batch_reward: 0.625, batch_reward_max: 0.959, batch_reward_min: 0.000

2023-03-10 14:10:18 - 
[#Step 800000] eval_reward: 825.815, eval_step: 1000, eval_time: 3, time: 17.926
	actor_loss: -64.607, critic_loss: 0.144, alpha_loss: 0.007
	q1: 64.657, target_q: 64.628, logp: 2.577, alpha: 0.016
	batch_reward: 0.594, batch_reward_max: 0.998, batch_reward_min: 0.000

2023-03-10 14:10:18 - Saving checkpoint at step: 4
2023-03-10 14:10:18 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/actor_4
2023-03-10 14:10:18 - Saving checkpoint at step: 4
2023-03-10 14:10:18 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/critic_4
2023-03-10 14:10:31 - 
[#Step 810000] eval_reward: 826.328, eval_step: 1000, eval_time: 3, time: 18.152
	actor_loss: -64.450, critic_loss: 0.183, alpha_loss: 0.000
	q1: 64.439, target_q: 64.401, logp: 2.982, alpha: 0.015
	batch_reward: 0.591, batch_reward_max: 0.981, batch_reward_min: 0.000

2023-03-10 14:10:45 - 
[#Step 820000] eval_reward: 843.040, eval_step: 1000, eval_time: 3, time: 18.379
	actor_loss: -66.983, critic_loss: 0.245, alpha_loss: -0.004
	q1: 66.995, target_q: 66.970, logp: 3.252, alpha: 0.015
	batch_reward: 0.626, batch_reward_max: 0.964, batch_reward_min: 0.000

2023-03-10 14:10:58 - 
[#Step 830000] eval_reward: 737.802, eval_step: 1000, eval_time: 3, time: 18.603
	actor_loss: -68.232, critic_loss: 0.631, alpha_loss: -0.003
	q1: 68.367, target_q: 68.374, logp: 3.175, alpha: 0.015
	batch_reward: 0.638, batch_reward_max: 0.959, batch_reward_min: 0.000

2023-03-10 14:11:12 - 
[#Step 840000] eval_reward: 828.770, eval_step: 1000, eval_time: 3, time: 18.829
	actor_loss: -67.731, critic_loss: 0.278, alpha_loss: 0.003
	q1: 67.803, target_q: 67.696, logp: 2.827, alpha: 0.015
	batch_reward: 0.638, batch_reward_max: 0.977, batch_reward_min: 0.000

2023-03-10 14:11:25 - 
[#Step 850000] eval_reward: 813.369, eval_step: 1000, eval_time: 3, time: 19.051
	actor_loss: -68.080, critic_loss: 0.252, alpha_loss: -0.006
	q1: 68.068, target_q: 68.163, logp: 3.414, alpha: 0.015
	batch_reward: 0.646, batch_reward_max: 0.997, batch_reward_min: 0.000

2023-03-10 14:11:39 - 
[#Step 860000] eval_reward: 833.858, eval_step: 1000, eval_time: 3, time: 19.278
	actor_loss: -68.723, critic_loss: 0.329, alpha_loss: -0.004
	q1: 68.768, target_q: 68.798, logp: 3.290, alpha: 0.015
	batch_reward: 0.649, batch_reward_max: 0.980, batch_reward_min: 0.000

2023-03-10 14:11:52 - 
[#Step 870000] eval_reward: 845.292, eval_step: 1000, eval_time: 3, time: 19.505
	actor_loss: -68.090, critic_loss: 0.245, alpha_loss: -0.011
	q1: 68.123, target_q: 68.150, logp: 3.743, alpha: 0.015
	batch_reward: 0.639, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:12:06 - 
[#Step 880000] eval_reward: 854.977, eval_step: 1000, eval_time: 3, time: 19.734
	actor_loss: -68.856, critic_loss: 0.383, alpha_loss: 0.000
	q1: 68.834, target_q: 68.870, logp: 2.998, alpha: 0.015
	batch_reward: 0.657, batch_reward_max: 0.997, batch_reward_min: 0.000

2023-03-10 14:12:20 - 
[#Step 890000] eval_reward: 840.880, eval_step: 1000, eval_time: 3, time: 19.960
	actor_loss: -66.993, critic_loss: 0.121, alpha_loss: -0.005
	q1: 66.987, target_q: 67.101, logp: 3.346, alpha: 0.015
	batch_reward: 0.624, batch_reward_max: 0.980, batch_reward_min: 0.000

2023-03-10 14:12:33 - 
[#Step 900000] eval_reward: 862.156, eval_step: 1000, eval_time: 3, time: 20.186
	actor_loss: -69.484, critic_loss: 0.171, alpha_loss: 0.003
	q1: 69.514, target_q: 69.524, logp: 2.780, alpha: 0.015
	batch_reward: 0.655, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:12:47 - 
[#Step 910000] eval_reward: 845.612, eval_step: 1000, eval_time: 3, time: 20.412
	actor_loss: -69.037, critic_loss: 0.130, alpha_loss: -0.006
	q1: 69.043, target_q: 68.982, logp: 3.442, alpha: 0.014
	batch_reward: 0.646, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:13:00 - 
[#Step 920000] eval_reward: 835.911, eval_step: 1000, eval_time: 3, time: 20.637
	actor_loss: -68.188, critic_loss: 0.244, alpha_loss: 0.003
	q1: 68.193, target_q: 68.231, logp: 2.823, alpha: 0.014
	batch_reward: 0.639, batch_reward_max: 0.977, batch_reward_min: 0.000

2023-03-10 14:13:14 - 
[#Step 930000] eval_reward: 818.840, eval_step: 1000, eval_time: 3, time: 20.863
	actor_loss: -68.426, critic_loss: 0.161, alpha_loss: 0.000
	q1: 68.419, target_q: 68.411, logp: 2.985, alpha: 0.014
	batch_reward: 0.644, batch_reward_max: 0.962, batch_reward_min: 0.000

2023-03-10 14:13:27 - 
[#Step 940000] eval_reward: 851.826, eval_step: 1000, eval_time: 3, time: 21.089
	actor_loss: -70.420, critic_loss: 0.422, alpha_loss: 0.001
	q1: 70.417, target_q: 70.453, logp: 2.898, alpha: 0.014
	batch_reward: 0.664, batch_reward_max: 0.978, batch_reward_min: 0.000

2023-03-10 14:13:41 - 
[#Step 950000] eval_reward: 848.185, eval_step: 1000, eval_time: 3, time: 21.311
	actor_loss: -70.725, critic_loss: 1.029, alpha_loss: -0.002
	q1: 70.704, target_q: 70.606, logp: 3.107, alpha: 0.014
	batch_reward: 0.673, batch_reward_max: 0.990, batch_reward_min: 0.000

2023-03-10 14:13:49 - 
[#Step 955000] eval_reward: 867.579, eval_step: 1000, eval_time: 3, time: 21.444
	actor_loss: -68.851, critic_loss: 0.117, alpha_loss: 0.002
	q1: 68.892, target_q: 68.884, logp: 2.829, alpha: 0.014
	batch_reward: 0.645, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:13:57 - 
[#Step 960000] eval_reward: 828.883, eval_step: 1000, eval_time: 3, time: 21.579
	actor_loss: -68.239, critic_loss: 0.151, alpha_loss: 0.006
	q1: 68.257, target_q: 68.162, logp: 2.593, alpha: 0.014
	batch_reward: 0.636, batch_reward_max: 0.980, batch_reward_min: 0.000

2023-03-10 14:14:05 - 
[#Step 965000] eval_reward: 867.355, eval_step: 1000, eval_time: 3, time: 21.712
	actor_loss: -70.407, critic_loss: 0.973, alpha_loss: -0.000
	q1: 70.395, target_q: 70.336, logp: 3.036, alpha: 0.014
	batch_reward: 0.664, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:14:13 - 
[#Step 970000] eval_reward: 867.325, eval_step: 1000, eval_time: 3, time: 21.845
	actor_loss: -69.782, critic_loss: 0.367, alpha_loss: -0.008
	q1: 69.840, target_q: 69.843, logp: 3.540, alpha: 0.014
	batch_reward: 0.663, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:14:21 - 
[#Step 975000] eval_reward: 866.485, eval_step: 1000, eval_time: 3, time: 21.978
	actor_loss: -70.349, critic_loss: 1.351, alpha_loss: -0.002
	q1: 70.322, target_q: 70.304, logp: 3.118, alpha: 0.014
	batch_reward: 0.658, batch_reward_max: 0.972, batch_reward_min: 0.000

2023-03-10 14:14:29 - 
[#Step 980000] eval_reward: 834.442, eval_step: 1000, eval_time: 3, time: 22.113
	actor_loss: -73.036, critic_loss: 0.174, alpha_loss: -0.001
	q1: 73.070, target_q: 73.077, logp: 3.062, alpha: 0.014
	batch_reward: 0.699, batch_reward_max: 1.000, batch_reward_min: 0.000

2023-03-10 14:14:37 - 
[#Step 985000] eval_reward: 855.624, eval_step: 1000, eval_time: 3, time: 22.246
	actor_loss: -70.145, critic_loss: 0.145, alpha_loss: -0.003
	q1: 70.204, target_q: 70.120, logp: 3.191, alpha: 0.015
	batch_reward: 0.660, batch_reward_max: 0.979, batch_reward_min: 0.000

2023-03-10 14:14:45 - 
[#Step 990000] eval_reward: 867.581, eval_step: 1000, eval_time: 3, time: 22.381
	actor_loss: -71.224, critic_loss: 0.122, alpha_loss: 0.006
	q1: 71.230, target_q: 71.297, logp: 2.591, alpha: 0.015
	batch_reward: 0.678, batch_reward_max: 0.993, batch_reward_min: 0.000

2023-03-10 14:14:53 - 
[#Step 995000] eval_reward: 835.338, eval_step: 1000, eval_time: 3, time: 22.515
	actor_loss: -71.072, critic_loss: 0.551, alpha_loss: -0.006
	q1: 71.095, target_q: 71.092, logp: 3.406, alpha: 0.014
	batch_reward: 0.668, batch_reward_max: 0.985, batch_reward_min: 0.000

2023-03-10 14:15:01 - 
[#Step 1000000] eval_reward: 863.703, eval_step: 1000, eval_time: 3, time: 22.650
	actor_loss: -70.760, critic_loss: 1.520, alpha_loss: 0.001
	q1: 70.812, target_q: 70.713, logp: 2.893, alpha: 0.014
	batch_reward: 0.665, batch_reward_max: 0.983, batch_reward_min: 0.000

2023-03-10 14:15:01 - Saving checkpoint at step: 5
2023-03-10 14:15:01 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/actor_5
2023-03-10 14:15:01 - Saving checkpoint at step: 5
2023-03-10 14:15:01 - Saved checkpoint at saved_models/cheetah-run/sac_s1_20230310_135222/critic_5
