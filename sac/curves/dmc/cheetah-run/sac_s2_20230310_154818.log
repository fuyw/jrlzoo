2023-03-10 15:48:18 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: cheetah-run
eval_episodes: 10
eval_freq: 5000
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: orthogonal
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
seed: 2
start_timesteps: 10000
tau: 0.005

2023-03-10 15:48:28 - 
[#Step 10000] eval_reward: 18.757, eval_time: 3

2023-03-10 15:48:43 - 
[#Step 20000] eval_reward: 7.122, eval_step: 1000, eval_time: 3, time: 0.430
	actor_loss: -48.179, critic_loss: 0.044, alpha_loss: 0.655
	q1: 47.857, target_q: 47.862, logp: -3.977, alpha: 0.094
	batch_reward: 0.004, batch_reward_max: 0.051, batch_reward_min: 0.000

2023-03-10 15:48:57 - 
[#Step 30000] eval_reward: 60.397, eval_step: 1000, eval_time: 3, time: 0.651
	actor_loss: -34.045, critic_loss: 0.025, alpha_loss: 0.043
	q1: 33.981, target_q: 33.985, logp: -0.944, alpha: 0.011
	batch_reward: 0.011, batch_reward_max: 0.200, batch_reward_min: 0.000

2023-03-10 15:49:10 - 
[#Step 40000] eval_reward: 59.008, eval_step: 1000, eval_time: 3, time: 0.873
	actor_loss: -23.317, critic_loss: 0.019, alpha_loss: 0.001
	q1: 23.260, target_q: 23.275, logp: 2.793, alpha: 0.005
	batch_reward: 0.018, batch_reward_max: 0.266, batch_reward_min: 0.000

2023-03-10 15:49:24 - 
[#Step 50000] eval_reward: 59.938, eval_step: 1000, eval_time: 3, time: 1.097
	actor_loss: -18.001, critic_loss: 0.054, alpha_loss: -0.000
	q1: 17.937, target_q: 17.911, logp: 3.074, alpha: 0.005
	batch_reward: 0.031, batch_reward_max: 0.197, batch_reward_min: 0.000

2023-03-10 15:49:37 - 
[#Step 60000] eval_reward: 81.105, eval_step: 1000, eval_time: 3, time: 1.319
	actor_loss: -14.239, critic_loss: 0.035, alpha_loss: -0.002
	q1: 14.173, target_q: 14.135, logp: 3.424, alpha: 0.005
	batch_reward: 0.027, batch_reward_max: 0.229, batch_reward_min: 0.000

2023-03-10 15:49:50 - 
[#Step 70000] eval_reward: 203.700, eval_step: 1000, eval_time: 3, time: 1.544
	actor_loss: -12.612, critic_loss: 0.037, alpha_loss: -0.000
	q1: 12.557, target_q: 12.586, logp: 3.020, alpha: 0.005
	batch_reward: 0.044, batch_reward_max: 0.296, batch_reward_min: 0.000

2023-03-10 15:50:04 - 
[#Step 80000] eval_reward: 123.365, eval_step: 1000, eval_time: 3, time: 1.768
	actor_loss: -12.944, critic_loss: 0.043, alpha_loss: -0.002
	q1: 12.880, target_q: 12.893, logp: 3.249, alpha: 0.006
	batch_reward: 0.068, batch_reward_max: 0.366, batch_reward_min: 0.000

2023-03-10 15:50:17 - 
[#Step 90000] eval_reward: 182.754, eval_step: 1000, eval_time: 3, time: 1.995
	actor_loss: -12.344, critic_loss: 0.053, alpha_loss: 0.001
	q1: 12.305, target_q: 12.317, logp: 2.875, alpha: 0.006
	batch_reward: 0.074, batch_reward_max: 0.330, batch_reward_min: 0.000

2023-03-10 15:50:31 - 
[#Step 100000] eval_reward: 256.793, eval_step: 1000, eval_time: 3, time: 2.219
	actor_loss: -13.348, critic_loss: 0.060, alpha_loss: 0.001
	q1: 13.296, target_q: 13.296, logp: 2.853, alpha: 0.008
	batch_reward: 0.103, batch_reward_max: 0.405, batch_reward_min: 0.000

2023-03-10 15:50:44 - 
[#Step 110000] eval_reward: 357.491, eval_step: 1000, eval_time: 3, time: 2.439
	actor_loss: -15.331, critic_loss: 0.057, alpha_loss: 0.002
	q1: 15.291, target_q: 15.336, logp: 2.715, alpha: 0.008
	batch_reward: 0.116, batch_reward_max: 0.416, batch_reward_min: 0.000

2023-03-10 15:50:57 - 
[#Step 120000] eval_reward: 407.348, eval_step: 1000, eval_time: 3, time: 2.662
	actor_loss: -17.518, critic_loss: 0.072, alpha_loss: -0.000
	q1: 17.486, target_q: 17.482, logp: 3.036, alpha: 0.009
	batch_reward: 0.143, batch_reward_max: 0.476, batch_reward_min: 0.000

2023-03-10 15:51:11 - 
[#Step 130000] eval_reward: 413.064, eval_step: 1000, eval_time: 3, time: 2.884
	actor_loss: -20.164, critic_loss: 0.090, alpha_loss: -0.005
	q1: 20.115, target_q: 20.098, logp: 3.530, alpha: 0.010
	batch_reward: 0.166, batch_reward_max: 0.466, batch_reward_min: 0.000

2023-03-10 15:51:24 - 
[#Step 140000] eval_reward: 423.339, eval_step: 1000, eval_time: 3, time: 3.108
	actor_loss: -20.369, critic_loss: 0.074, alpha_loss: 0.004
	q1: 20.322, target_q: 20.374, logp: 2.668, alpha: 0.011
	batch_reward: 0.170, batch_reward_max: 0.485, batch_reward_min: 0.000

2023-03-10 15:51:38 - 
[#Step 150000] eval_reward: 429.497, eval_step: 1000, eval_time: 3, time: 3.333
	actor_loss: -22.808, critic_loss: 0.071, alpha_loss: 0.003
	q1: 22.767, target_q: 22.783, logp: 2.675, alpha: 0.011
	batch_reward: 0.189, batch_reward_max: 0.545, batch_reward_min: 0.000

2023-03-10 15:51:51 - 
[#Step 160000] eval_reward: 461.476, eval_step: 1000, eval_time: 3, time: 3.560
	actor_loss: -23.608, critic_loss: 0.084, alpha_loss: -0.000
	q1: 23.567, target_q: 23.568, logp: 3.042, alpha: 0.011
	batch_reward: 0.201, batch_reward_max: 0.512, batch_reward_min: 0.000

2023-03-10 15:52:05 - 
[#Step 170000] eval_reward: 455.998, eval_step: 1000, eval_time: 3, time: 3.783
	actor_loss: -24.183, critic_loss: 0.067, alpha_loss: 0.009
	q1: 24.161, target_q: 24.187, logp: 2.278, alpha: 0.012
	batch_reward: 0.204, batch_reward_max: 0.535, batch_reward_min: 0.000

2023-03-10 15:52:18 - 
[#Step 180000] eval_reward: 477.432, eval_step: 1000, eval_time: 3, time: 4.005
	actor_loss: -27.001, critic_loss: 0.077, alpha_loss: -0.003
	q1: 26.976, target_q: 26.951, logp: 3.234, alpha: 0.012
	batch_reward: 0.238, batch_reward_max: 0.552, batch_reward_min: 0.000

2023-03-10 15:52:31 - 
[#Step 190000] eval_reward: 477.480, eval_step: 1000, eval_time: 3, time: 4.230
	actor_loss: -28.428, critic_loss: 0.057, alpha_loss: 0.002
	q1: 28.395, target_q: 28.415, logp: 2.866, alpha: 0.012
	batch_reward: 0.250, batch_reward_max: 0.560, batch_reward_min: 0.000

2023-03-10 15:52:45 - 
[#Step 200000] eval_reward: 505.075, eval_step: 1000, eval_time: 3, time: 4.454
	actor_loss: -27.605, critic_loss: 0.071, alpha_loss: 0.001
	q1: 27.560, target_q: 27.541, logp: 2.915, alpha: 0.013
	batch_reward: 0.232, batch_reward_max: 0.566, batch_reward_min: 0.000

2023-03-10 15:52:45 - Saving checkpoint at step: 1
2023-03-10 15:52:45 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/actor_1
2023-03-10 15:52:45 - Saving checkpoint at step: 1
2023-03-10 15:52:45 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/critic_1
2023-03-10 15:52:58 - 
[#Step 210000] eval_reward: 496.135, eval_step: 1000, eval_time: 3, time: 4.679
	actor_loss: -29.641, critic_loss: 0.081, alpha_loss: 0.003
	q1: 29.649, target_q: 29.699, logp: 2.759, alpha: 0.013
	batch_reward: 0.269, batch_reward_max: 0.583, batch_reward_min: 0.000

2023-03-10 15:53:12 - 
[#Step 220000] eval_reward: 516.363, eval_step: 1000, eval_time: 3, time: 4.905
	actor_loss: -31.011, critic_loss: 0.087, alpha_loss: 0.001
	q1: 30.974, target_q: 30.974, logp: 2.919, alpha: 0.014
	batch_reward: 0.280, batch_reward_max: 0.588, batch_reward_min: 0.000

2023-03-10 15:53:25 - 
[#Step 230000] eval_reward: 532.447, eval_step: 1000, eval_time: 3, time: 5.128
	actor_loss: -33.004, critic_loss: 0.092, alpha_loss: -0.000
	q1: 32.970, target_q: 33.002, logp: 3.017, alpha: 0.014
	batch_reward: 0.303, batch_reward_max: 0.598, batch_reward_min: 0.000

2023-03-10 15:53:39 - 
[#Step 240000] eval_reward: 536.548, eval_step: 1000, eval_time: 3, time: 5.349
	actor_loss: -32.879, critic_loss: 0.076, alpha_loss: -0.003
	q1: 32.852, target_q: 32.820, logp: 3.218, alpha: 0.014
	batch_reward: 0.296, batch_reward_max: 0.605, batch_reward_min: 0.000

2023-03-10 15:53:52 - 
[#Step 250000] eval_reward: 541.106, eval_step: 1000, eval_time: 3, time: 5.576
	actor_loss: -34.587, critic_loss: 0.059, alpha_loss: -0.002
	q1: 34.580, target_q: 34.583, logp: 3.117, alpha: 0.014
	batch_reward: 0.316, batch_reward_max: 0.599, batch_reward_min: 0.000

2023-03-10 15:54:06 - 
[#Step 260000] eval_reward: 540.119, eval_step: 1000, eval_time: 3, time: 5.802
	actor_loss: -35.139, critic_loss: 0.114, alpha_loss: 0.000
	q1: 35.118, target_q: 35.132, logp: 2.998, alpha: 0.014
	batch_reward: 0.329, batch_reward_max: 0.614, batch_reward_min: 0.000

2023-03-10 15:54:19 - 
[#Step 270000] eval_reward: 564.045, eval_step: 1000, eval_time: 3, time: 6.022
	actor_loss: -36.241, critic_loss: 0.074, alpha_loss: -0.003
	q1: 36.225, target_q: 36.260, logp: 3.198, alpha: 0.014
	batch_reward: 0.344, batch_reward_max: 0.631, batch_reward_min: 0.000

2023-03-10 15:54:33 - 
[#Step 280000] eval_reward: 561.045, eval_step: 1000, eval_time: 3, time: 6.251
	actor_loss: -37.121, critic_loss: 0.078, alpha_loss: 0.000
	q1: 37.071, target_q: 37.179, logp: 2.989, alpha: 0.014
	batch_reward: 0.341, batch_reward_max: 0.684, batch_reward_min: 0.000

2023-03-10 15:54:46 - 
[#Step 290000] eval_reward: 549.064, eval_step: 1000, eval_time: 3, time: 6.477
	actor_loss: -37.973, critic_loss: 0.097, alpha_loss: -0.001
	q1: 37.964, target_q: 37.955, logp: 3.101, alpha: 0.014
	batch_reward: 0.355, batch_reward_max: 0.649, batch_reward_min: 0.000

2023-03-10 15:55:00 - 
[#Step 300000] eval_reward: 544.532, eval_step: 1000, eval_time: 3, time: 6.701
	actor_loss: -38.240, critic_loss: 0.076, alpha_loss: -0.002
	q1: 38.228, target_q: 38.251, logp: 3.128, alpha: 0.015
	batch_reward: 0.360, batch_reward_max: 0.681, batch_reward_min: 0.000

2023-03-10 15:55:13 - 
[#Step 310000] eval_reward: 547.108, eval_step: 1000, eval_time: 3, time: 6.924
	actor_loss: -39.270, critic_loss: 0.141, alpha_loss: -0.005
	q1: 39.236, target_q: 39.312, logp: 3.324, alpha: 0.015
	batch_reward: 0.372, batch_reward_max: 0.672, batch_reward_min: 0.000

2023-03-10 15:55:27 - 
[#Step 320000] eval_reward: 572.317, eval_step: 1000, eval_time: 3, time: 7.148
	actor_loss: -40.290, critic_loss: 0.094, alpha_loss: -0.002
	q1: 40.292, target_q: 40.251, logp: 3.135, alpha: 0.015
	batch_reward: 0.372, batch_reward_max: 0.684, batch_reward_min: 0.000

2023-03-10 15:55:40 - 
[#Step 330000] eval_reward: 577.809, eval_step: 1000, eval_time: 3, time: 7.372
	actor_loss: -40.448, critic_loss: 0.119, alpha_loss: 0.001
	q1: 40.425, target_q: 40.357, logp: 2.915, alpha: 0.015
	batch_reward: 0.373, batch_reward_max: 0.696, batch_reward_min: 0.000

2023-03-10 15:55:53 - 
[#Step 340000] eval_reward: 599.553, eval_step: 1000, eval_time: 3, time: 7.596
	actor_loss: -40.810, critic_loss: 0.080, alpha_loss: -0.001
	q1: 40.801, target_q: 40.753, logp: 3.061, alpha: 0.015
	batch_reward: 0.377, batch_reward_max: 0.703, batch_reward_min: 0.000

2023-03-10 15:56:07 - 
[#Step 350000] eval_reward: 602.914, eval_step: 1000, eval_time: 3, time: 7.818
	actor_loss: -41.636, critic_loss: 0.088, alpha_loss: 0.004
	q1: 41.617, target_q: 41.673, logp: 2.720, alpha: 0.015
	batch_reward: 0.378, batch_reward_max: 0.684, batch_reward_min: 0.000

2023-03-10 15:56:20 - 
[#Step 360000] eval_reward: 600.727, eval_step: 1000, eval_time: 3, time: 8.041
	actor_loss: -43.209, critic_loss: 0.085, alpha_loss: 0.001
	q1: 43.210, target_q: 43.172, logp: 2.930, alpha: 0.015
	batch_reward: 0.410, batch_reward_max: 0.706, batch_reward_min: 0.000

2023-03-10 15:56:34 - 
[#Step 370000] eval_reward: 612.407, eval_step: 1000, eval_time: 3, time: 8.265
	actor_loss: -42.260, critic_loss: 0.079, alpha_loss: 0.009
	q1: 42.298, target_q: 42.252, logp: 2.402, alpha: 0.015
	batch_reward: 0.400, batch_reward_max: 0.695, batch_reward_min: 0.000

2023-03-10 15:56:47 - 
[#Step 380000] eval_reward: 558.686, eval_step: 1000, eval_time: 3, time: 8.491
	actor_loss: -42.774, critic_loss: 0.093, alpha_loss: -0.004
	q1: 42.783, target_q: 42.801, logp: 3.256, alpha: 0.015
	batch_reward: 0.400, batch_reward_max: 0.732, batch_reward_min: 0.000

2023-03-10 15:57:00 - 
[#Step 390000] eval_reward: 622.075, eval_step: 1000, eval_time: 3, time: 8.713
	actor_loss: -43.972, critic_loss: 0.079, alpha_loss: -0.001
	q1: 43.955, target_q: 43.937, logp: 3.069, alpha: 0.015
	batch_reward: 0.411, batch_reward_max: 0.702, batch_reward_min: 0.000

2023-03-10 15:57:14 - 
[#Step 400000] eval_reward: 615.072, eval_step: 1000, eval_time: 3, time: 8.941
	actor_loss: -43.337, critic_loss: 0.089, alpha_loss: -0.003
	q1: 43.359, target_q: 43.397, logp: 3.215, alpha: 0.015
	batch_reward: 0.396, batch_reward_max: 0.714, batch_reward_min: 0.000

2023-03-10 15:57:14 - Saving checkpoint at step: 2
2023-03-10 15:57:14 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/actor_2
2023-03-10 15:57:14 - Saving checkpoint at step: 2
2023-03-10 15:57:14 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/critic_2
2023-03-10 15:57:28 - 
[#Step 410000] eval_reward: 615.419, eval_step: 1000, eval_time: 3, time: 9.165
	actor_loss: -44.848, critic_loss: 0.096, alpha_loss: -0.006
	q1: 44.853, target_q: 44.854, logp: 3.392, alpha: 0.015
	batch_reward: 0.426, batch_reward_max: 0.713, batch_reward_min: 0.000

2023-03-10 15:57:41 - 
[#Step 420000] eval_reward: 622.999, eval_step: 1000, eval_time: 3, time: 9.388
	actor_loss: -46.424, critic_loss: 0.167, alpha_loss: -0.006
	q1: 46.466, target_q: 46.410, logp: 3.373, alpha: 0.015
	batch_reward: 0.442, batch_reward_max: 0.720, batch_reward_min: 0.000

2023-03-10 15:57:54 - 
[#Step 430000] eval_reward: 620.595, eval_step: 1000, eval_time: 3, time: 9.607
	actor_loss: -45.939, critic_loss: 0.093, alpha_loss: -0.006
	q1: 45.890, target_q: 45.988, logp: 3.416, alpha: 0.015
	batch_reward: 0.427, batch_reward_max: 0.727, batch_reward_min: 0.000

2023-03-10 15:58:08 - 
[#Step 440000] eval_reward: 587.640, eval_step: 1000, eval_time: 3, time: 9.832
	actor_loss: -46.464, critic_loss: 0.088, alpha_loss: -0.001
	q1: 46.471, target_q: 46.458, logp: 3.054, alpha: 0.015
	batch_reward: 0.433, batch_reward_max: 0.715, batch_reward_min: 0.000

2023-03-10 15:58:21 - 
[#Step 450000] eval_reward: 633.536, eval_step: 1000, eval_time: 3, time: 10.057
	actor_loss: -48.168, critic_loss: 0.091, alpha_loss: 0.002
	q1: 48.151, target_q: 48.130, logp: 2.894, alpha: 0.015
	batch_reward: 0.458, batch_reward_max: 0.753, batch_reward_min: 0.000

2023-03-10 15:58:34 - 
[#Step 460000] eval_reward: 629.583, eval_step: 1000, eval_time: 3, time: 10.280
	actor_loss: -46.303, critic_loss: 0.093, alpha_loss: -0.003
	q1: 46.301, target_q: 46.337, logp: 3.197, alpha: 0.015
	batch_reward: 0.429, batch_reward_max: 0.725, batch_reward_min: 0.000

2023-03-10 15:58:48 - 
[#Step 470000] eval_reward: 634.338, eval_step: 1000, eval_time: 3, time: 10.508
	actor_loss: -46.552, critic_loss: 0.093, alpha_loss: 0.002
	q1: 46.582, target_q: 46.527, logp: 2.834, alpha: 0.015
	batch_reward: 0.431, batch_reward_max: 0.744, batch_reward_min: 0.000

2023-03-10 15:59:02 - 
[#Step 480000] eval_reward: 631.216, eval_step: 1000, eval_time: 3, time: 10.735
	actor_loss: -48.784, critic_loss: 0.085, alpha_loss: 0.005
	q1: 48.788, target_q: 48.773, logp: 2.676, alpha: 0.015
	batch_reward: 0.462, batch_reward_max: 0.727, batch_reward_min: 0.000

2023-03-10 15:59:15 - 
[#Step 490000] eval_reward: 643.358, eval_step: 1000, eval_time: 3, time: 10.959
	actor_loss: -47.742, critic_loss: 0.153, alpha_loss: -0.001
	q1: 47.740, target_q: 47.727, logp: 3.064, alpha: 0.015
	batch_reward: 0.455, batch_reward_max: 0.760, batch_reward_min: 0.000

2023-03-10 15:59:29 - 
[#Step 500000] eval_reward: 646.428, eval_step: 1000, eval_time: 3, time: 11.185
	actor_loss: -50.329, critic_loss: 0.075, alpha_loss: 0.004
	q1: 50.352, target_q: 50.380, logp: 2.749, alpha: 0.015
	batch_reward: 0.480, batch_reward_max: 0.750, batch_reward_min: 0.000

2023-03-10 15:59:42 - 
[#Step 510000] eval_reward: 649.231, eval_step: 1000, eval_time: 3, time: 11.410
	actor_loss: -47.782, critic_loss: 0.103, alpha_loss: 0.002
	q1: 47.779, target_q: 47.760, logp: 2.868, alpha: 0.015
	batch_reward: 0.440, batch_reward_max: 0.757, batch_reward_min: 0.000

2023-03-10 15:59:56 - 
[#Step 520000] eval_reward: 649.324, eval_step: 1000, eval_time: 3, time: 11.632
	actor_loss: -47.966, critic_loss: 0.062, alpha_loss: 0.008
	q1: 47.974, target_q: 47.940, logp: 2.450, alpha: 0.015
	batch_reward: 0.442, batch_reward_max: 0.745, batch_reward_min: 0.000

2023-03-10 16:00:09 - 
[#Step 530000] eval_reward: 673.652, eval_step: 1000, eval_time: 3, time: 11.856
	actor_loss: -48.393, critic_loss: 0.070, alpha_loss: 0.001
	q1: 48.416, target_q: 48.403, logp: 2.911, alpha: 0.015
	batch_reward: 0.453, batch_reward_max: 0.763, batch_reward_min: 0.000

2023-03-10 16:00:22 - 
[#Step 540000] eval_reward: 658.760, eval_step: 1000, eval_time: 3, time: 12.078
	actor_loss: -51.644, critic_loss: 0.095, alpha_loss: 0.000
	q1: 51.637, target_q: 51.731, logp: 2.978, alpha: 0.015
	batch_reward: 0.494, batch_reward_max: 0.780, batch_reward_min: 0.000

2023-03-10 16:00:36 - 
[#Step 550000] eval_reward: 669.211, eval_step: 1000, eval_time: 3, time: 12.301
	actor_loss: -50.958, critic_loss: 0.133, alpha_loss: 0.002
	q1: 50.998, target_q: 50.936, logp: 2.878, alpha: 0.015
	batch_reward: 0.481, batch_reward_max: 0.766, batch_reward_min: 0.000

2023-03-10 16:00:49 - 
[#Step 560000] eval_reward: 667.697, eval_step: 1000, eval_time: 3, time: 12.525
	actor_loss: -49.727, critic_loss: 0.118, alpha_loss: 0.002
	q1: 49.721, target_q: 49.741, logp: 2.856, alpha: 0.015
	batch_reward: 0.458, batch_reward_max: 0.761, batch_reward_min: 0.000

2023-03-10 16:01:03 - 
[#Step 570000] eval_reward: 671.001, eval_step: 1000, eval_time: 3, time: 12.750
	actor_loss: -51.488, critic_loss: 0.072, alpha_loss: 0.002
	q1: 51.519, target_q: 51.527, logp: 2.850, alpha: 0.015
	batch_reward: 0.492, batch_reward_max: 0.797, batch_reward_min: 0.000

2023-03-10 16:01:17 - 
[#Step 580000] eval_reward: 672.742, eval_step: 1000, eval_time: 3, time: 12.982
	actor_loss: -50.402, critic_loss: 0.115, alpha_loss: -0.003
	q1: 50.439, target_q: 50.451, logp: 3.184, alpha: 0.015
	batch_reward: 0.473, batch_reward_max: 0.776, batch_reward_min: 0.000

2023-03-10 16:01:30 - 
[#Step 590000] eval_reward: 678.638, eval_step: 1000, eval_time: 3, time: 13.208
	actor_loss: -52.211, critic_loss: 0.300, alpha_loss: -0.001
	q1: 52.243, target_q: 52.170, logp: 3.081, alpha: 0.015
	batch_reward: 0.502, batch_reward_max: 0.797, batch_reward_min: 0.000

2023-03-10 16:01:43 - 
[#Step 600000] eval_reward: 669.893, eval_step: 1000, eval_time: 3, time: 13.430
	actor_loss: -51.688, critic_loss: 0.641, alpha_loss: 0.003
	q1: 51.653, target_q: 51.634, logp: 2.817, alpha: 0.015
	batch_reward: 0.497, batch_reward_max: 0.808, batch_reward_min: 0.000

2023-03-10 16:01:43 - Saving checkpoint at step: 3
2023-03-10 16:01:43 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/actor_3
2023-03-10 16:01:43 - Saving checkpoint at step: 3
2023-03-10 16:01:43 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/critic_3
2023-03-10 16:01:57 - 
[#Step 610000] eval_reward: 678.525, eval_step: 1000, eval_time: 3, time: 13.654
	actor_loss: -52.305, critic_loss: 0.117, alpha_loss: 0.002
	q1: 52.340, target_q: 52.338, logp: 2.891, alpha: 0.015
	batch_reward: 0.500, batch_reward_max: 0.778, batch_reward_min: 0.000

2023-03-10 16:02:10 - 
[#Step 620000] eval_reward: 674.498, eval_step: 1000, eval_time: 3, time: 13.875
	actor_loss: -52.281, critic_loss: 0.084, alpha_loss: 0.001
	q1: 52.301, target_q: 52.350, logp: 2.907, alpha: 0.015
	batch_reward: 0.505, batch_reward_max: 0.807, batch_reward_min: 0.000

2023-03-10 16:02:23 - 
[#Step 630000] eval_reward: 699.952, eval_step: 1000, eval_time: 3, time: 14.097
	actor_loss: -54.315, critic_loss: 0.066, alpha_loss: -0.004
	q1: 54.342, target_q: 54.309, logp: 3.252, alpha: 0.015
	batch_reward: 0.522, batch_reward_max: 0.779, batch_reward_min: 0.000

2023-03-10 16:02:37 - 
[#Step 640000] eval_reward: 713.961, eval_step: 1000, eval_time: 3, time: 14.320
	actor_loss: -51.232, critic_loss: 0.123, alpha_loss: 0.000
	q1: 51.282, target_q: 51.289, logp: 2.997, alpha: 0.015
	batch_reward: 0.484, batch_reward_max: 0.809, batch_reward_min: 0.000

2023-03-10 16:02:50 - 
[#Step 650000] eval_reward: 677.143, eval_step: 1000, eval_time: 3, time: 14.543
	actor_loss: -51.813, critic_loss: 0.113, alpha_loss: 0.002
	q1: 51.798, target_q: 51.828, logp: 2.848, alpha: 0.015
	batch_reward: 0.491, batch_reward_max: 0.804, batch_reward_min: 0.000

2023-03-10 16:03:04 - 
[#Step 660000] eval_reward: 695.308, eval_step: 1000, eval_time: 3, time: 14.765
	actor_loss: -53.063, critic_loss: 0.110, alpha_loss: 0.000
	q1: 53.074, target_q: 53.100, logp: 2.980, alpha: 0.015
	batch_reward: 0.498, batch_reward_max: 0.817, batch_reward_min: 0.000

2023-03-10 16:03:17 - 
[#Step 670000] eval_reward: 707.949, eval_step: 1000, eval_time: 3, time: 14.989
	actor_loss: -52.360, critic_loss: 0.113, alpha_loss: 0.005
	q1: 52.365, target_q: 52.383, logp: 2.652, alpha: 0.015
	batch_reward: 0.496, batch_reward_max: 0.818, batch_reward_min: 0.000

2023-03-10 16:03:31 - 
[#Step 680000] eval_reward: 713.308, eval_step: 1000, eval_time: 3, time: 15.214
	actor_loss: -52.991, critic_loss: 0.088, alpha_loss: -0.003
	q1: 53.007, target_q: 53.028, logp: 3.207, alpha: 0.015
	batch_reward: 0.498, batch_reward_max: 0.830, batch_reward_min: 0.000

2023-03-10 16:03:44 - 
[#Step 690000] eval_reward: 712.503, eval_step: 1000, eval_time: 3, time: 15.436
	actor_loss: -53.993, critic_loss: 0.143, alpha_loss: -0.000
	q1: 53.970, target_q: 54.006, logp: 3.010, alpha: 0.016
	batch_reward: 0.511, batch_reward_max: 0.834, batch_reward_min: 0.000

2023-03-10 16:03:57 - 
[#Step 700000] eval_reward: 714.724, eval_step: 1000, eval_time: 3, time: 15.659
	actor_loss: -56.418, critic_loss: 0.112, alpha_loss: 0.001
	q1: 56.465, target_q: 56.392, logp: 2.950, alpha: 0.015
	batch_reward: 0.542, batch_reward_max: 0.845, batch_reward_min: 0.000

2023-03-10 16:04:11 - 
[#Step 710000] eval_reward: 720.550, eval_step: 1000, eval_time: 3, time: 15.883
	actor_loss: -54.363, critic_loss: 0.072, alpha_loss: 0.001
	q1: 54.349, target_q: 54.351, logp: 2.962, alpha: 0.015
	batch_reward: 0.511, batch_reward_max: 0.801, batch_reward_min: 0.000

2023-03-10 16:04:24 - 
[#Step 720000] eval_reward: 688.806, eval_step: 1000, eval_time: 3, time: 16.111
	actor_loss: -55.886, critic_loss: 0.108, alpha_loss: -0.010
	q1: 55.890, target_q: 55.920, logp: 3.640, alpha: 0.016
	batch_reward: 0.537, batch_reward_max: 0.857, batch_reward_min: 0.000

2023-03-10 16:04:38 - 
[#Step 730000] eval_reward: 715.902, eval_step: 1000, eval_time: 3, time: 16.336
	actor_loss: -56.238, critic_loss: 0.090, alpha_loss: -0.002
	q1: 56.258, target_q: 56.233, logp: 3.141, alpha: 0.015
	batch_reward: 0.541, batch_reward_max: 0.850, batch_reward_min: 0.000

2023-03-10 16:04:51 - 
[#Step 740000] eval_reward: 728.729, eval_step: 1000, eval_time: 3, time: 16.563
	actor_loss: -56.148, critic_loss: 0.119, alpha_loss: 0.000
	q1: 56.143, target_q: 56.138, logp: 2.974, alpha: 0.015
	batch_reward: 0.537, batch_reward_max: 0.860, batch_reward_min: 0.000

2023-03-10 16:05:05 - 
[#Step 750000] eval_reward: 719.673, eval_step: 1000, eval_time: 3, time: 16.790
	actor_loss: -56.036, critic_loss: 0.110, alpha_loss: 0.005
	q1: 56.028, target_q: 56.082, logp: 2.638, alpha: 0.015
	batch_reward: 0.532, batch_reward_max: 0.844, batch_reward_min: 0.000

2023-03-10 16:05:19 - 
[#Step 760000] eval_reward: 722.161, eval_step: 1000, eval_time: 3, time: 17.017
	actor_loss: -54.575, critic_loss: 0.079, alpha_loss: 0.006
	q1: 54.561, target_q: 54.594, logp: 2.585, alpha: 0.016
	batch_reward: 0.518, batch_reward_max: 0.868, batch_reward_min: 0.000

2023-03-10 16:05:32 - 
[#Step 770000] eval_reward: 717.712, eval_step: 1000, eval_time: 3, time: 17.241
	actor_loss: -58.484, critic_loss: 0.116, alpha_loss: -0.007
	q1: 58.553, target_q: 58.586, logp: 3.484, alpha: 0.015
	batch_reward: 0.566, batch_reward_max: 0.857, batch_reward_min: 0.000

2023-03-10 16:05:46 - 
[#Step 780000] eval_reward: 724.954, eval_step: 1000, eval_time: 3, time: 17.466
	actor_loss: -56.918, critic_loss: 0.079, alpha_loss: 0.003
	q1: 56.919, target_q: 56.888, logp: 2.819, alpha: 0.016
	batch_reward: 0.546, batch_reward_max: 0.842, batch_reward_min: 0.000

2023-03-10 16:05:59 - 
[#Step 790000] eval_reward: 744.996, eval_step: 1000, eval_time: 3, time: 17.691
	actor_loss: -56.846, critic_loss: 0.152, alpha_loss: 0.001
	q1: 56.863, target_q: 56.814, logp: 2.925, alpha: 0.015
	batch_reward: 0.531, batch_reward_max: 0.871, batch_reward_min: 0.000

2023-03-10 16:06:13 - 
[#Step 800000] eval_reward: 751.415, eval_step: 1000, eval_time: 3, time: 17.915
	actor_loss: -59.332, critic_loss: 0.204, alpha_loss: -0.005
	q1: 59.361, target_q: 59.362, logp: 3.324, alpha: 0.016
	batch_reward: 0.567, batch_reward_max: 0.880, batch_reward_min: 0.000

2023-03-10 16:06:13 - Saving checkpoint at step: 4
2023-03-10 16:06:13 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/actor_4
2023-03-10 16:06:13 - Saving checkpoint at step: 4
2023-03-10 16:06:13 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/critic_4
2023-03-10 16:06:26 - 
[#Step 810000] eval_reward: 731.880, eval_step: 1000, eval_time: 3, time: 18.139
	actor_loss: -56.904, critic_loss: 0.067, alpha_loss: 0.003
	q1: 56.898, target_q: 56.915, logp: 2.830, alpha: 0.016
	batch_reward: 0.533, batch_reward_max: 0.895, batch_reward_min: 0.000

2023-03-10 16:06:39 - 
[#Step 820000] eval_reward: 744.250, eval_step: 1000, eval_time: 3, time: 18.363
	actor_loss: -57.843, critic_loss: 0.422, alpha_loss: 0.003
	q1: 57.838, target_q: 57.805, logp: 2.805, alpha: 0.016
	batch_reward: 0.546, batch_reward_max: 0.878, batch_reward_min: 0.000

2023-03-10 16:06:53 - 
[#Step 830000] eval_reward: 744.515, eval_step: 1000, eval_time: 3, time: 18.585
	actor_loss: -56.899, critic_loss: 0.243, alpha_loss: 0.003
	q1: 56.911, target_q: 56.927, logp: 2.785, alpha: 0.015
	batch_reward: 0.530, batch_reward_max: 0.884, batch_reward_min: 0.000

2023-03-10 16:07:06 - 
[#Step 840000] eval_reward: 772.178, eval_step: 1000, eval_time: 3, time: 18.809
	actor_loss: -58.927, critic_loss: 0.102, alpha_loss: 0.000
	q1: 58.959, target_q: 58.995, logp: 2.984, alpha: 0.015
	batch_reward: 0.562, batch_reward_max: 0.888, batch_reward_min: 0.000

2023-03-10 16:07:20 - 
[#Step 850000] eval_reward: 753.812, eval_step: 1000, eval_time: 3, time: 19.031
	actor_loss: -58.403, critic_loss: 0.103, alpha_loss: 0.006
	q1: 58.370, target_q: 58.444, logp: 2.608, alpha: 0.016
	batch_reward: 0.558, batch_reward_max: 0.862, batch_reward_min: 0.000

2023-03-10 16:07:33 - 
[#Step 860000] eval_reward: 739.874, eval_step: 1000, eval_time: 3, time: 19.255
	actor_loss: -58.536, critic_loss: 0.087, alpha_loss: 0.002
	q1: 58.517, target_q: 58.549, logp: 2.847, alpha: 0.015
	batch_reward: 0.547, batch_reward_max: 0.880, batch_reward_min: 0.000

2023-03-10 16:07:46 - 
[#Step 870000] eval_reward: 767.061, eval_step: 1000, eval_time: 3, time: 19.478
	actor_loss: -58.163, critic_loss: 0.168, alpha_loss: -0.002
	q1: 58.135, target_q: 58.114, logp: 3.130, alpha: 0.016
	batch_reward: 0.542, batch_reward_max: 0.876, batch_reward_min: 0.000

2023-03-10 16:08:00 - 
[#Step 880000] eval_reward: 765.831, eval_step: 1000, eval_time: 3, time: 19.702
	actor_loss: -60.931, critic_loss: 0.083, alpha_loss: -0.009
	q1: 60.946, target_q: 60.981, logp: 3.602, alpha: 0.016
	batch_reward: 0.584, batch_reward_max: 0.905, batch_reward_min: 0.000

2023-03-10 16:08:13 - 
[#Step 890000] eval_reward: 757.067, eval_step: 1000, eval_time: 3, time: 19.928
	actor_loss: -60.365, critic_loss: 0.126, alpha_loss: -0.004
	q1: 60.375, target_q: 60.387, logp: 3.270, alpha: 0.015
	batch_reward: 0.581, batch_reward_max: 0.905, batch_reward_min: 0.000

2023-03-10 16:08:27 - 
[#Step 900000] eval_reward: 783.174, eval_step: 1000, eval_time: 3, time: 20.151
	actor_loss: -60.491, critic_loss: 0.101, alpha_loss: 0.006
	q1: 60.537, target_q: 60.492, logp: 2.588, alpha: 0.016
	batch_reward: 0.571, batch_reward_max: 0.892, batch_reward_min: 0.000

2023-03-10 16:08:40 - 
[#Step 910000] eval_reward: 753.237, eval_step: 1000, eval_time: 3, time: 20.372
	actor_loss: -60.489, critic_loss: 0.344, alpha_loss: -0.001
	q1: 60.484, target_q: 60.568, logp: 3.034, alpha: 0.016
	batch_reward: 0.567, batch_reward_max: 0.889, batch_reward_min: 0.000

2023-03-10 16:08:54 - 
[#Step 920000] eval_reward: 747.597, eval_step: 1000, eval_time: 3, time: 20.601
	actor_loss: -60.963, critic_loss: 0.131, alpha_loss: -0.002
	q1: 60.969, target_q: 60.958, logp: 3.111, alpha: 0.015
	batch_reward: 0.581, batch_reward_max: 0.908, batch_reward_min: 0.000

2023-03-10 16:09:07 - 
[#Step 930000] eval_reward: 783.215, eval_step: 1000, eval_time: 3, time: 20.826
	actor_loss: -60.339, critic_loss: 0.147, alpha_loss: 0.005
	q1: 60.331, target_q: 60.273, logp: 2.693, alpha: 0.016
	batch_reward: 0.573, batch_reward_max: 0.908, batch_reward_min: 0.000

2023-03-10 16:09:21 - 
[#Step 940000] eval_reward: 780.998, eval_step: 1000, eval_time: 3, time: 21.051
	actor_loss: -61.237, critic_loss: 0.267, alpha_loss: 0.003
	q1: 61.266, target_q: 61.217, logp: 2.814, alpha: 0.016
	batch_reward: 0.585, batch_reward_max: 0.906, batch_reward_min: 0.000

2023-03-10 16:09:34 - 
[#Step 950000] eval_reward: 781.006, eval_step: 1000, eval_time: 3, time: 21.275
	actor_loss: -61.707, critic_loss: 0.164, alpha_loss: 0.001
	q1: 61.721, target_q: 61.679, logp: 2.905, alpha: 0.015
	batch_reward: 0.583, batch_reward_max: 0.887, batch_reward_min: 0.000

2023-03-10 16:09:42 - 
[#Step 955000] eval_reward: 785.919, eval_step: 1000, eval_time: 3, time: 21.408
	actor_loss: -61.102, critic_loss: 0.109, alpha_loss: -0.003
	q1: 61.098, target_q: 61.161, logp: 3.162, alpha: 0.015
	batch_reward: 0.580, batch_reward_max: 0.919, batch_reward_min: 0.000

2023-03-10 16:09:50 - 
[#Step 960000] eval_reward: 770.770, eval_step: 1000, eval_time: 3, time: 21.543
	actor_loss: -59.798, critic_loss: 0.156, alpha_loss: 0.001
	q1: 59.827, target_q: 59.761, logp: 2.916, alpha: 0.016
	batch_reward: 0.556, batch_reward_max: 0.946, batch_reward_min: 0.000

2023-03-10 16:09:58 - 
[#Step 965000] eval_reward: 778.597, eval_step: 1000, eval_time: 3, time: 21.678
	actor_loss: -62.970, critic_loss: 0.127, alpha_loss: 0.010
	q1: 62.975, target_q: 62.975, logp: 2.337, alpha: 0.016
	batch_reward: 0.601, batch_reward_max: 0.904, batch_reward_min: 0.000

2023-03-10 16:10:06 - 
[#Step 970000] eval_reward: 794.541, eval_step: 1000, eval_time: 3, time: 21.812
	actor_loss: -61.427, critic_loss: 0.101, alpha_loss: -0.002
	q1: 61.499, target_q: 61.380, logp: 3.108, alpha: 0.015
	batch_reward: 0.581, batch_reward_max: 0.902, batch_reward_min: 0.000

2023-03-10 16:10:14 - 
[#Step 975000] eval_reward: 779.970, eval_step: 1000, eval_time: 3, time: 21.944
	actor_loss: -60.839, critic_loss: 0.171, alpha_loss: -0.009
	q1: 60.864, target_q: 60.929, logp: 3.546, alpha: 0.016
	batch_reward: 0.576, batch_reward_max: 0.913, batch_reward_min: 0.000

2023-03-10 16:10:22 - 
[#Step 980000] eval_reward: 773.931, eval_step: 1000, eval_time: 3, time: 22.078
	actor_loss: -62.390, critic_loss: 0.086, alpha_loss: 0.006
	q1: 62.422, target_q: 62.415, logp: 2.637, alpha: 0.016
	batch_reward: 0.598, batch_reward_max: 0.898, batch_reward_min: 0.000

2023-03-10 16:10:30 - 
[#Step 985000] eval_reward: 782.587, eval_step: 1000, eval_time: 3, time: 22.213
	actor_loss: -60.711, critic_loss: 0.169, alpha_loss: 0.004
	q1: 60.736, target_q: 60.667, logp: 2.742, alpha: 0.016
	batch_reward: 0.571, batch_reward_max: 0.895, batch_reward_min: 0.000

2023-03-10 16:10:39 - 
[#Step 990000] eval_reward: 786.850, eval_step: 1000, eval_time: 3, time: 22.350
	actor_loss: -61.545, critic_loss: 0.159, alpha_loss: 0.000
	q1: 61.569, target_q: 61.540, logp: 2.993, alpha: 0.016
	batch_reward: 0.582, batch_reward_max: 0.925, batch_reward_min: 0.000

2023-03-10 16:10:47 - 
[#Step 995000] eval_reward: 793.937, eval_step: 1000, eval_time: 3, time: 22.484
	actor_loss: -62.135, critic_loss: 0.085, alpha_loss: -0.002
	q1: 62.141, target_q: 62.125, logp: 3.150, alpha: 0.016
	batch_reward: 0.593, batch_reward_max: 0.923, batch_reward_min: 0.000

2023-03-10 16:10:55 - 
[#Step 1000000] eval_reward: 770.638, eval_step: 1000, eval_time: 3, time: 22.616
	actor_loss: -61.307, critic_loss: 0.113, alpha_loss: 0.006
	q1: 61.320, target_q: 61.401, logp: 2.614, alpha: 0.016
	batch_reward: 0.580, batch_reward_max: 0.931, batch_reward_min: 0.000

2023-03-10 16:10:55 - Saving checkpoint at step: 5
2023-03-10 16:10:55 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/actor_5
2023-03-10 16:10:55 - Saving checkpoint at step: 5
2023-03-10 16:10:55 - Saved checkpoint at saved_models/cheetah-run/sac_s2_20230310_154818/critic_5
