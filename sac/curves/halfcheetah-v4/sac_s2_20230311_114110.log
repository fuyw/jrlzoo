2023-03-11 11:41:10 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: HalfCheetah-v4
eval_episodes: 10
eval_freq: 5000
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: orthogonal
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
seed: 2
start_timesteps: 10000
tau: 0.005

2023-03-11 11:41:19 - 
[#Step 10000] eval_reward: -164.985, eval_time: 2

2023-03-11 11:41:33 - 
[#Step 20000] eval_reward: 1.801, eval_step: 1000, eval_time: 2, time: 0.388
	actor_loss: -39.303, critic_loss: 2.904, alpha_loss: 0.400
	q1: 38.751, target_q: 38.778, sampled_q: 39.215, logp: -0.846, alpha: 0.104
	batch_reward: -0.209, batch_reward_max: 1.738, batch_reward_min: -2.065

2023-03-11 11:41:45 - 
[#Step 30000] eval_reward: 1751.856, eval_step: 1000, eval_time: 2, time: 0.587
	actor_loss: -34.719, critic_loss: 3.738, alpha_loss: 0.008
	q1: 34.359, target_q: 34.584, sampled_q: 34.789, logp: 2.694, alpha: 0.026
	batch_reward: -0.021, batch_reward_max: 3.099, batch_reward_min: -2.934

2023-03-11 11:41:57 - 
[#Step 40000] eval_reward: 2820.332, eval_step: 1000, eval_time: 2, time: 0.782
	actor_loss: -53.439, critic_loss: 9.544, alpha_loss: -0.019
	q1: 52.889, target_q: 52.513, sampled_q: 53.627, logp: 3.344, alpha: 0.056
	batch_reward: 0.375, batch_reward_max: 4.072, batch_reward_min: -2.187

2023-03-11 11:42:09 - 
[#Step 50000] eval_reward: 2965.090, eval_step: 1000, eval_time: 2, time: 0.983
	actor_loss: -87.592, critic_loss: 8.949, alpha_loss: -0.024
	q1: 87.232, target_q: 87.323, sampled_q: 87.871, logp: 3.286, alpha: 0.085
	batch_reward: 0.820, batch_reward_max: 4.584, batch_reward_min: -1.739

2023-03-11 11:42:21 - 
[#Step 60000] eval_reward: 3688.607, eval_step: 1000, eval_time: 2, time: 1.180
	actor_loss: -120.489, critic_loss: 18.562, alpha_loss: 0.000
	q1: 120.080, target_q: 120.249, sampled_q: 120.814, logp: 2.996, alpha: 0.108
	batch_reward: 1.325, batch_reward_max: 5.499, batch_reward_min: -2.279

2023-03-11 11:42:33 - 
[#Step 70000] eval_reward: 4290.683, eval_step: 1000, eval_time: 2, time: 1.380
	actor_loss: -155.506, critic_loss: 18.786, alpha_loss: -0.018
	q1: 155.057, target_q: 154.748, sampled_q: 155.898, logp: 3.141, alpha: 0.125
	batch_reward: 1.575, batch_reward_max: 6.777, batch_reward_min: -1.795

2023-03-11 11:42:45 - 
[#Step 80000] eval_reward: 4754.716, eval_step: 1000, eval_time: 2, time: 1.584
	actor_loss: -191.659, critic_loss: 17.594, alpha_loss: 0.003
	q1: 191.471, target_q: 191.135, sampled_q: 192.095, logp: 2.976, alpha: 0.147
	batch_reward: 2.037, batch_reward_max: 7.073, batch_reward_min: -1.772

2023-03-11 11:42:57 - 
[#Step 90000] eval_reward: 5378.405, eval_step: 1000, eval_time: 2, time: 1.786
	actor_loss: -217.524, critic_loss: 21.945, alpha_loss: 0.055
	q1: 216.957, target_q: 217.610, sampled_q: 217.968, logp: 2.667, alpha: 0.166
	batch_reward: 2.280, batch_reward_max: 7.089, batch_reward_min: -2.028

2023-03-11 11:43:09 - 
[#Step 100000] eval_reward: 5546.341, eval_step: 1000, eval_time: 2, time: 1.985
	actor_loss: -259.181, critic_loss: 20.789, alpha_loss: -0.040
	q1: 259.078, target_q: 258.728, sampled_q: 259.776, logp: 3.214, alpha: 0.185
	batch_reward: 2.688, batch_reward_max: 7.594, batch_reward_min: -1.344

2023-03-11 11:43:21 - 
[#Step 110000] eval_reward: 5657.468, eval_step: 1000, eval_time: 2, time: 2.186
	actor_loss: -259.007, critic_loss: 15.113, alpha_loss: 0.042
	q1: 259.072, target_q: 258.852, sampled_q: 259.552, logp: 2.787, alpha: 0.196
	batch_reward: 2.668, batch_reward_max: 7.010, batch_reward_min: -1.889

2023-03-11 11:43:33 - 
[#Step 120000] eval_reward: 5902.733, eval_step: 1000, eval_time: 2, time: 2.388
	actor_loss: -291.130, critic_loss: 10.803, alpha_loss: 0.026
	q1: 291.106, target_q: 291.179, sampled_q: 291.709, logp: 2.871, alpha: 0.202
	batch_reward: 3.001, batch_reward_max: 7.153, batch_reward_min: -1.666

2023-03-11 11:43:45 - 
[#Step 130000] eval_reward: 5934.794, eval_step: 1000, eval_time: 2, time: 2.591
	actor_loss: -300.531, critic_loss: 16.963, alpha_loss: 0.084
	q1: 300.456, target_q: 300.745, sampled_q: 301.080, logp: 2.604, alpha: 0.211
	batch_reward: 3.082, batch_reward_max: 7.495, batch_reward_min: -1.504

2023-03-11 11:43:57 - 
[#Step 140000] eval_reward: 5921.428, eval_step: 1000, eval_time: 2, time: 2.790
	actor_loss: -320.907, critic_loss: 19.348, alpha_loss: 0.071
	q1: 321.035, target_q: 321.271, sampled_q: 321.496, logp: 2.676, alpha: 0.220
	batch_reward: 3.259, batch_reward_max: 7.400, batch_reward_min: -1.415

2023-03-11 11:44:09 - 
[#Step 150000] eval_reward: 6158.621, eval_step: 1000, eval_time: 2, time: 2.990
	actor_loss: -333.308, critic_loss: 20.180, alpha_loss: 0.039
	q1: 333.269, target_q: 333.738, sampled_q: 333.959, logp: 2.828, alpha: 0.230
	batch_reward: 3.418, batch_reward_max: 8.154, batch_reward_min: -1.661

2023-03-11 11:44:21 - 
[#Step 160000] eval_reward: 6017.995, eval_step: 1000, eval_time: 2, time: 3.189
	actor_loss: -350.282, critic_loss: 17.398, alpha_loss: -0.115
	q1: 349.975, target_q: 350.259, sampled_q: 351.113, logp: 3.485, alpha: 0.238
	batch_reward: 3.580, batch_reward_max: 8.103, batch_reward_min: -1.193

2023-03-11 11:44:33 - 
[#Step 170000] eval_reward: 6438.712, eval_step: 1000, eval_time: 2, time: 3.387
	actor_loss: -346.140, critic_loss: 19.314, alpha_loss: 0.085
	q1: 346.569, target_q: 346.469, sampled_q: 346.780, logp: 2.648, alpha: 0.242
	batch_reward: 3.635, batch_reward_max: 8.284, batch_reward_min: -2.176

2023-03-11 11:44:45 - 
[#Step 180000] eval_reward: 6475.637, eval_step: 1000, eval_time: 2, time: 3.586
	actor_loss: -366.446, critic_loss: 15.356, alpha_loss: -0.045
	q1: 367.181, target_q: 367.062, sampled_q: 367.220, logp: 3.187, alpha: 0.243
	batch_reward: 3.841, batch_reward_max: 8.050, batch_reward_min: -1.442

2023-03-11 11:44:57 - 
[#Step 190000] eval_reward: 6563.233, eval_step: 1000, eval_time: 2, time: 3.787
	actor_loss: -370.909, critic_loss: 16.087, alpha_loss: 0.073
	q1: 371.066, target_q: 371.042, sampled_q: 371.579, logp: 2.706, alpha: 0.247
	batch_reward: 3.826, batch_reward_max: 8.087, batch_reward_min: -1.599

2023-03-11 11:45:09 - 
[#Step 200000] eval_reward: 6663.478, eval_step: 1000, eval_time: 2, time: 3.987
	actor_loss: -373.473, critic_loss: 26.898, alpha_loss: 0.013
	q1: 373.318, target_q: 373.423, sampled_q: 374.219, logp: 2.948, alpha: 0.253
	batch_reward: 3.674, batch_reward_max: 7.959, batch_reward_min: -3.022

2023-03-11 11:45:09 - Saving checkpoint at step: 1
2023-03-11 11:45:09 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/actor_1
2023-03-11 11:45:09 - Saving checkpoint at step: 1
2023-03-11 11:45:09 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/critic_1
2023-03-11 11:45:21 - 
[#Step 210000] eval_reward: 6652.667, eval_step: 1000, eval_time: 2, time: 4.189
	actor_loss: -397.619, critic_loss: 14.602, alpha_loss: -0.036
	q1: 397.860, target_q: 398.792, sampled_q: 398.428, logp: 3.141, alpha: 0.258
	batch_reward: 4.320, batch_reward_max: 9.062, batch_reward_min: -1.608

2023-03-11 11:45:34 - 
[#Step 220000] eval_reward: 6811.440, eval_step: 1000, eval_time: 2, time: 4.393
	actor_loss: -402.992, critic_loss: 16.643, alpha_loss: 0.051
	q1: 403.098, target_q: 403.449, sampled_q: 403.725, logp: 2.806, alpha: 0.261
	batch_reward: 4.179, batch_reward_max: 8.642, batch_reward_min: -2.982

2023-03-11 11:45:46 - 
[#Step 230000] eval_reward: 6953.815, eval_step: 1000, eval_time: 2, time: 4.594
	actor_loss: -412.371, critic_loss: 16.342, alpha_loss: -0.021
	q1: 412.884, target_q: 412.767, sampled_q: 413.179, logp: 3.080, alpha: 0.262
	batch_reward: 4.345, batch_reward_max: 8.334, batch_reward_min: -2.297

2023-03-11 11:45:58 - 
[#Step 240000] eval_reward: 6918.464, eval_step: 1000, eval_time: 2, time: 4.794
	actor_loss: -416.832, critic_loss: 17.346, alpha_loss: -0.013
	q1: 417.075, target_q: 417.404, sampled_q: 417.641, logp: 3.049, alpha: 0.265
	batch_reward: 4.301, batch_reward_max: 8.933, batch_reward_min: -1.238

2023-03-11 11:46:10 - 
[#Step 250000] eval_reward: 7104.987, eval_step: 1000, eval_time: 2, time: 4.992
	actor_loss: -421.384, critic_loss: 11.741, alpha_loss: 0.061
	q1: 421.939, target_q: 422.283, sampled_q: 422.121, logp: 2.771, alpha: 0.266
	batch_reward: 4.571, batch_reward_max: 8.830, batch_reward_min: -1.523

2023-03-11 11:46:22 - 
[#Step 260000] eval_reward: 7119.023, eval_step: 1000, eval_time: 2, time: 5.194
	actor_loss: -433.285, critic_loss: 13.556, alpha_loss: 0.071
	q1: 433.896, target_q: 433.941, sampled_q: 434.039, logp: 2.742, alpha: 0.275
	batch_reward: 4.616, batch_reward_max: 8.651, batch_reward_min: -1.515

2023-03-11 11:46:34 - 
[#Step 270000] eval_reward: 7328.311, eval_step: 1000, eval_time: 2, time: 5.393
	actor_loss: -442.900, critic_loss: 17.088, alpha_loss: -0.036
	q1: 443.429, target_q: 443.370, sampled_q: 443.767, logp: 3.128, alpha: 0.277
	batch_reward: 4.920, batch_reward_max: 8.874, batch_reward_min: -1.661

2023-03-11 11:46:46 - 
[#Step 280000] eval_reward: 7330.724, eval_step: 1000, eval_time: 2, time: 5.593
	actor_loss: -441.663, critic_loss: 14.234, alpha_loss: -0.001
	q1: 442.326, target_q: 442.478, sampled_q: 442.509, logp: 3.003, alpha: 0.282
	batch_reward: 4.656, batch_reward_max: 8.855, batch_reward_min: -1.242

2023-03-11 11:46:58 - 
[#Step 290000] eval_reward: 7386.877, eval_step: 1000, eval_time: 2, time: 5.791
	actor_loss: -446.662, critic_loss: 10.903, alpha_loss: 0.013
	q1: 447.112, target_q: 447.345, sampled_q: 447.496, logp: 2.954, alpha: 0.283
	batch_reward: 4.869, batch_reward_max: 9.167, batch_reward_min: -1.271

2023-03-11 11:47:10 - 
[#Step 300000] eval_reward: 7381.921, eval_step: 1000, eval_time: 2, time: 5.994
	actor_loss: -461.077, critic_loss: 12.398, alpha_loss: 0.134
	q1: 461.670, target_q: 461.719, sampled_q: 461.808, logp: 2.535, alpha: 0.288
	batch_reward: 4.873, batch_reward_max: 9.522, batch_reward_min: -1.783

2023-03-11 11:47:22 - 
[#Step 310000] eval_reward: 7559.612, eval_step: 1000, eval_time: 2, time: 6.192
	actor_loss: -466.247, critic_loss: 12.949, alpha_loss: 0.042
	q1: 466.729, target_q: 466.851, sampled_q: 467.079, logp: 2.856, alpha: 0.291
	batch_reward: 5.007, batch_reward_max: 9.545, batch_reward_min: -1.572

2023-03-11 11:47:34 - 
[#Step 320000] eval_reward: 7501.092, eval_step: 1000, eval_time: 2, time: 6.394
	actor_loss: -468.067, critic_loss: 16.673, alpha_loss: 0.098
	q1: 468.668, target_q: 468.128, sampled_q: 468.859, logp: 2.669, alpha: 0.297
	batch_reward: 4.969, batch_reward_max: 9.226, batch_reward_min: -2.043

2023-03-11 11:47:46 - 
[#Step 330000] eval_reward: 7639.234, eval_step: 1000, eval_time: 2, time: 6.595
	actor_loss: -461.079, critic_loss: 14.478, alpha_loss: -0.002
	q1: 461.380, target_q: 462.185, sampled_q: 461.974, logp: 3.008, alpha: 0.298
	batch_reward: 5.073, batch_reward_max: 8.727, batch_reward_min: -1.229

2023-03-11 11:47:58 - 
[#Step 340000] eval_reward: 7782.873, eval_step: 1000, eval_time: 2, time: 6.795
	actor_loss: -475.104, critic_loss: 10.408, alpha_loss: -0.037
	q1: 475.872, target_q: 475.763, sampled_q: 476.044, logp: 3.121, alpha: 0.301
	batch_reward: 5.073, batch_reward_max: 9.269, batch_reward_min: -1.564

2023-03-11 11:48:10 - 
[#Step 350000] eval_reward: 7681.402, eval_step: 1000, eval_time: 2, time: 6.996
	actor_loss: -478.754, critic_loss: 18.507, alpha_loss: -0.004
	q1: 479.388, target_q: 478.989, sampled_q: 479.691, logp: 3.013, alpha: 0.311
	batch_reward: 4.792, batch_reward_max: 9.328, batch_reward_min: -1.591

2023-03-11 11:48:22 - 
[#Step 360000] eval_reward: 7625.336, eval_step: 1000, eval_time: 2, time: 7.197
	actor_loss: -494.543, critic_loss: 11.508, alpha_loss: -0.111
	q1: 495.373, target_q: 495.843, sampled_q: 495.595, logp: 3.353, alpha: 0.314
	batch_reward: 5.332, batch_reward_max: 10.033, batch_reward_min: -1.555

2023-03-11 11:48:34 - 
[#Step 370000] eval_reward: 7876.567, eval_step: 1000, eval_time: 2, time: 7.395
	actor_loss: -489.986, critic_loss: 17.551, alpha_loss: -0.082
	q1: 490.528, target_q: 490.392, sampled_q: 491.029, logp: 3.256, alpha: 0.320
	batch_reward: 5.297, batch_reward_max: 9.339, batch_reward_min: -1.568

2023-03-11 11:48:46 - 
[#Step 380000] eval_reward: 7825.576, eval_step: 1000, eval_time: 2, time: 7.598
	actor_loss: -492.649, critic_loss: 17.973, alpha_loss: 0.020
	q1: 493.057, target_q: 493.950, sampled_q: 493.595, logp: 2.938, alpha: 0.322
	batch_reward: 5.202, batch_reward_max: 9.806, batch_reward_min: -1.438

2023-03-11 11:48:58 - 
[#Step 390000] eval_reward: 7962.058, eval_step: 1000, eval_time: 2, time: 7.795
	actor_loss: -500.726, critic_loss: 15.985, alpha_loss: -0.124
	q1: 501.530, target_q: 501.218, sampled_q: 501.819, logp: 3.385, alpha: 0.323
	batch_reward: 5.257, batch_reward_max: 9.533, batch_reward_min: -1.052

2023-03-11 11:49:10 - 
[#Step 400000] eval_reward: 8096.831, eval_step: 1000, eval_time: 2, time: 7.995
	actor_loss: -503.634, critic_loss: 24.642, alpha_loss: 0.027
	q1: 504.239, target_q: 503.502, sampled_q: 504.580, logp: 2.917, alpha: 0.324
	batch_reward: 5.355, batch_reward_max: 9.703, batch_reward_min: -1.936

2023-03-11 11:49:10 - Saving checkpoint at step: 2
2023-03-11 11:49:10 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/actor_2
2023-03-11 11:49:10 - Saving checkpoint at step: 2
2023-03-11 11:49:10 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/critic_2
2023-03-11 11:49:22 - 
[#Step 410000] eval_reward: 7992.049, eval_step: 1000, eval_time: 2, time: 8.197
	actor_loss: -515.290, critic_loss: 21.250, alpha_loss: -0.047
	q1: 516.076, target_q: 515.831, sampled_q: 516.325, logp: 3.144, alpha: 0.329
	batch_reward: 5.589, batch_reward_max: 9.686, batch_reward_min: -1.377

2023-03-11 11:49:34 - 
[#Step 420000] eval_reward: 8149.049, eval_step: 1000, eval_time: 2, time: 8.395
	actor_loss: -525.813, critic_loss: 19.447, alpha_loss: -0.024
	q1: 526.704, target_q: 527.123, sampled_q: 526.834, logp: 3.073, alpha: 0.332
	batch_reward: 5.776, batch_reward_max: 10.108, batch_reward_min: -1.318

2023-03-11 11:49:46 - 
[#Step 430000] eval_reward: 8282.041, eval_step: 1000, eval_time: 2, time: 8.593
	actor_loss: -510.184, critic_loss: 22.906, alpha_loss: 0.067
	q1: 510.456, target_q: 511.090, sampled_q: 511.114, logp: 2.797, alpha: 0.332
	batch_reward: 5.512, batch_reward_max: 9.658, batch_reward_min: -1.298

2023-03-11 11:49:58 - 
[#Step 440000] eval_reward: 8267.125, eval_step: 1000, eval_time: 2, time: 8.797
	actor_loss: -524.683, critic_loss: 19.457, alpha_loss: 0.117
	q1: 524.971, target_q: 524.329, sampled_q: 525.592, logp: 2.657, alpha: 0.342
	batch_reward: 5.656, batch_reward_max: 10.355, batch_reward_min: -1.488

2023-03-11 11:50:10 - 
[#Step 450000] eval_reward: 8082.603, eval_step: 1000, eval_time: 2, time: 8.996
	actor_loss: -526.299, critic_loss: 16.447, alpha_loss: 0.052
	q1: 527.015, target_q: 526.588, sampled_q: 527.295, logp: 2.852, alpha: 0.349
	batch_reward: 5.653, batch_reward_max: 9.887, batch_reward_min: -1.195

2023-03-11 11:50:22 - 
[#Step 460000] eval_reward: 8487.400, eval_step: 1000, eval_time: 2, time: 9.196
	actor_loss: -521.798, critic_loss: 24.302, alpha_loss: -0.072
	q1: 522.370, target_q: 522.562, sampled_q: 522.907, logp: 3.209, alpha: 0.346
	batch_reward: 5.576, batch_reward_max: 10.019, batch_reward_min: -1.184

2023-03-11 11:50:34 - 
[#Step 470000] eval_reward: 8381.158, eval_step: 1000, eval_time: 2, time: 9.393
	actor_loss: -536.742, critic_loss: 20.394, alpha_loss: 0.069
	q1: 537.140, target_q: 537.752, sampled_q: 537.724, logp: 2.804, alpha: 0.350
	batch_reward: 5.593, batch_reward_max: 9.368, batch_reward_min: -1.894

2023-03-11 11:50:46 - 
[#Step 480000] eval_reward: 8552.935, eval_step: 1000, eval_time: 2, time: 9.594
	actor_loss: -536.149, critic_loss: 22.650, alpha_loss: 0.072
	q1: 536.396, target_q: 536.606, sampled_q: 537.145, logp: 2.798, alpha: 0.356
	batch_reward: 5.888, batch_reward_max: 9.725, batch_reward_min: -1.851

2023-03-11 11:50:58 - 
[#Step 490000] eval_reward: 8717.987, eval_step: 1000, eval_time: 2, time: 9.794
	actor_loss: -544.000, critic_loss: 19.377, alpha_loss: -0.078
	q1: 544.979, target_q: 544.840, sampled_q: 545.143, logp: 3.221, alpha: 0.355
	batch_reward: 5.858, batch_reward_max: 9.542, batch_reward_min: -1.146

2023-03-11 11:51:10 - 
[#Step 500000] eval_reward: 8578.195, eval_step: 1000, eval_time: 2, time: 9.993
	actor_loss: -556.933, critic_loss: 25.787, alpha_loss: 0.023
	q1: 557.695, target_q: 556.939, sampled_q: 557.995, logp: 2.937, alpha: 0.362
	batch_reward: 6.103, batch_reward_max: 9.769, batch_reward_min: -1.176

2023-03-11 11:51:22 - 
[#Step 510000] eval_reward: 8651.064, eval_step: 1000, eval_time: 2, time: 10.193
	actor_loss: -553.490, critic_loss: 21.862, alpha_loss: 0.039
	q1: 553.919, target_q: 553.688, sampled_q: 554.543, logp: 2.894, alpha: 0.364
	batch_reward: 5.773, batch_reward_max: 10.063, batch_reward_min: -0.907

2023-03-11 11:51:34 - 
[#Step 520000] eval_reward: 8752.725, eval_step: 1000, eval_time: 2, time: 10.395
	actor_loss: -547.275, critic_loss: 17.736, alpha_loss: -0.062
	q1: 547.549, target_q: 547.797, sampled_q: 548.427, logp: 3.170, alpha: 0.363
	batch_reward: 5.780, batch_reward_max: 10.161, batch_reward_min: -0.954

2023-03-11 11:51:46 - 
[#Step 530000] eval_reward: 8763.844, eval_step: 1000, eval_time: 2, time: 10.591
	actor_loss: -555.763, critic_loss: 15.414, alpha_loss: -0.114
	q1: 556.419, target_q: 556.699, sampled_q: 556.984, logp: 3.309, alpha: 0.369
	batch_reward: 6.029, batch_reward_max: 10.408, batch_reward_min: -1.248

2023-03-11 11:51:57 - 
[#Step 540000] eval_reward: 8648.683, eval_step: 1000, eval_time: 2, time: 10.787
	actor_loss: -568.929, critic_loss: 16.622, alpha_loss: -0.010
	q1: 569.737, target_q: 569.355, sampled_q: 570.060, logp: 3.027, alpha: 0.374
	batch_reward: 6.193, batch_reward_max: 10.612, batch_reward_min: -1.180

2023-03-11 11:52:09 - 
[#Step 550000] eval_reward: 8748.259, eval_step: 1000, eval_time: 2, time: 10.989
	actor_loss: -558.314, critic_loss: 21.492, alpha_loss: 0.117
	q1: 559.031, target_q: 558.801, sampled_q: 559.322, logp: 2.687, alpha: 0.375
	batch_reward: 5.961, batch_reward_max: 9.896, batch_reward_min: -1.489

2023-03-11 11:52:21 - 
[#Step 560000] eval_reward: 8920.429, eval_step: 1000, eval_time: 2, time: 11.188
	actor_loss: -560.807, critic_loss: 18.671, alpha_loss: 0.075
	q1: 561.558, target_q: 561.652, sampled_q: 561.868, logp: 2.802, alpha: 0.379
	batch_reward: 5.950, batch_reward_max: 10.333, batch_reward_min: -1.504

2023-03-11 11:52:33 - 
[#Step 570000] eval_reward: 8989.875, eval_step: 1000, eval_time: 2, time: 11.389
	actor_loss: -584.766, critic_loss: 21.610, alpha_loss: -0.040
	q1: 585.301, target_q: 585.105, sampled_q: 585.948, logp: 3.106, alpha: 0.380
	batch_reward: 6.376, batch_reward_max: 11.235, batch_reward_min: -0.471

2023-03-11 11:52:45 - 
[#Step 580000] eval_reward: 9078.104, eval_step: 1000, eval_time: 2, time: 11.589
	actor_loss: -574.111, critic_loss: 25.469, alpha_loss: -0.220
	q1: 574.706, target_q: 574.590, sampled_q: 575.474, logp: 3.576, alpha: 0.381
	batch_reward: 6.007, batch_reward_max: 10.432, batch_reward_min: -1.217

2023-03-11 11:52:57 - 
[#Step 590000] eval_reward: 9142.199, eval_step: 1000, eval_time: 2, time: 11.787
	actor_loss: -583.213, critic_loss: 22.904, alpha_loss: -0.052
	q1: 583.668, target_q: 583.493, sampled_q: 584.412, logp: 3.135, alpha: 0.382
	batch_reward: 6.336, batch_reward_max: 10.469, batch_reward_min: -0.952

2023-03-11 11:53:09 - 
[#Step 600000] eval_reward: 8814.946, eval_step: 1000, eval_time: 2, time: 11.987
	actor_loss: -578.831, critic_loss: 18.617, alpha_loss: -0.040
	q1: 579.964, target_q: 580.103, sampled_q: 580.020, logp: 3.104, alpha: 0.383
	batch_reward: 6.375, batch_reward_max: 10.526, batch_reward_min: -1.532

2023-03-11 11:53:09 - Saving checkpoint at step: 3
2023-03-11 11:53:09 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/actor_3
2023-03-11 11:53:09 - Saving checkpoint at step: 3
2023-03-11 11:53:09 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/critic_3
2023-03-11 11:53:21 - 
[#Step 610000] eval_reward: 8936.046, eval_step: 1000, eval_time: 2, time: 12.188
	actor_loss: -584.886, critic_loss: 24.573, alpha_loss: -0.110
	q1: 585.756, target_q: 586.947, sampled_q: 586.154, logp: 3.285, alpha: 0.386
	batch_reward: 6.250, batch_reward_max: 10.226, batch_reward_min: -1.586

2023-03-11 11:53:33 - 
[#Step 620000] eval_reward: 9043.733, eval_step: 1000, eval_time: 2, time: 12.387
	actor_loss: -595.104, critic_loss: 28.246, alpha_loss: -0.104
	q1: 595.807, target_q: 595.676, sampled_q: 596.387, logp: 3.264, alpha: 0.393
	batch_reward: 6.441, batch_reward_max: 10.959, batch_reward_min: -1.014

2023-03-11 11:53:45 - 
[#Step 630000] eval_reward: 9238.175, eval_step: 1000, eval_time: 2, time: 12.586
	actor_loss: -589.810, critic_loss: 19.062, alpha_loss: 0.226
	q1: 590.838, target_q: 591.180, sampled_q: 590.749, logp: 2.417, alpha: 0.388
	batch_reward: 6.295, batch_reward_max: 10.764, batch_reward_min: -1.075

2023-03-11 11:53:57 - 
[#Step 640000] eval_reward: 9296.523, eval_step: 1000, eval_time: 2, time: 12.786
	actor_loss: -592.883, critic_loss: 28.148, alpha_loss: 0.023
	q1: 593.666, target_q: 594.536, sampled_q: 594.045, logp: 2.943, alpha: 0.395
	batch_reward: 6.237, batch_reward_max: 10.305, batch_reward_min: -1.924

2023-03-11 11:54:09 - 
[#Step 650000] eval_reward: 9383.371, eval_step: 1000, eval_time: 2, time: 12.984
	actor_loss: -593.399, critic_loss: 20.695, alpha_loss: -0.028
	q1: 594.018, target_q: 594.089, sampled_q: 594.593, logp: 3.071, alpha: 0.389
	batch_reward: 6.300, batch_reward_max: 10.468, batch_reward_min: -1.166

2023-03-11 11:54:21 - 
[#Step 660000] eval_reward: 9127.554, eval_step: 1000, eval_time: 2, time: 13.180
	actor_loss: -595.501, critic_loss: 22.851, alpha_loss: -0.044
	q1: 596.058, target_q: 596.524, sampled_q: 596.734, logp: 3.110, alpha: 0.396
	batch_reward: 6.244, batch_reward_max: 11.522, batch_reward_min: -2.065

2023-03-11 11:54:33 - 
[#Step 670000] eval_reward: 9283.896, eval_step: 1000, eval_time: 2, time: 13.383
	actor_loss: -603.374, critic_loss: 25.013, alpha_loss: -0.047
	q1: 604.290, target_q: 604.349, sampled_q: 604.624, logp: 3.118, alpha: 0.401
	batch_reward: 6.573, batch_reward_max: 10.670, batch_reward_min: -0.958

2023-03-11 11:54:45 - 
[#Step 680000] eval_reward: 9225.730, eval_step: 1000, eval_time: 2, time: 13.581
	actor_loss: -603.220, critic_loss: 17.893, alpha_loss: 0.100
	q1: 604.150, target_q: 604.625, sampled_q: 604.311, logp: 2.748, alpha: 0.397
	batch_reward: 6.361, batch_reward_max: 10.276, batch_reward_min: -1.100

2023-03-11 11:54:57 - 
[#Step 690000] eval_reward: 9543.205, eval_step: 1000, eval_time: 2, time: 13.781
	actor_loss: -604.044, critic_loss: 32.143, alpha_loss: -0.297
	q1: 604.568, target_q: 604.231, sampled_q: 605.542, logp: 3.743, alpha: 0.400
	batch_reward: 6.400, batch_reward_max: 10.795, batch_reward_min: -1.056

2023-03-11 11:55:09 - 
[#Step 700000] eval_reward: 9546.523, eval_step: 1000, eval_time: 2, time: 13.981
	actor_loss: -613.287, critic_loss: 27.326, alpha_loss: 0.009
	q1: 613.938, target_q: 614.423, sampled_q: 614.508, logp: 2.977, alpha: 0.410
	batch_reward: 6.625, batch_reward_max: 11.091, batch_reward_min: -1.237

2023-03-11 11:55:21 - 
[#Step 710000] eval_reward: 9485.076, eval_step: 1000, eval_time: 2, time: 14.180
	actor_loss: -606.873, critic_loss: 22.450, alpha_loss: 0.099
	q1: 607.668, target_q: 607.550, sampled_q: 608.011, logp: 2.759, alpha: 0.412
	batch_reward: 6.343, batch_reward_max: 10.398, batch_reward_min: -1.171

2023-03-11 11:55:33 - 
[#Step 720000] eval_reward: 9214.917, eval_step: 1000, eval_time: 2, time: 14.377
	actor_loss: -620.091, critic_loss: 33.015, alpha_loss: 0.135
	q1: 620.846, target_q: 620.422, sampled_q: 621.194, logp: 2.674, alpha: 0.413
	batch_reward: 6.626, batch_reward_max: 11.037, batch_reward_min: -0.337

2023-03-11 11:55:45 - 
[#Step 730000] eval_reward: 9331.507, eval_step: 1000, eval_time: 2, time: 14.576
	actor_loss: -618.894, critic_loss: 26.416, alpha_loss: -0.021
	q1: 619.827, target_q: 619.979, sampled_q: 620.154, logp: 3.051, alpha: 0.413
	batch_reward: 6.681, batch_reward_max: 11.375, batch_reward_min: -0.895

2023-03-11 11:55:57 - 
[#Step 740000] eval_reward: 9546.178, eval_step: 1000, eval_time: 2, time: 14.775
	actor_loss: -627.253, critic_loss: 22.454, alpha_loss: -0.042
	q1: 627.964, target_q: 627.200, sampled_q: 628.531, logp: 3.103, alpha: 0.412
	batch_reward: 6.704, batch_reward_max: 11.135, batch_reward_min: -1.404

2023-03-11 11:56:08 - 
[#Step 750000] eval_reward: 9800.122, eval_step: 1000, eval_time: 2, time: 14.974
	actor_loss: -621.290, critic_loss: 19.878, alpha_loss: 0.089
	q1: 622.213, target_q: 622.704, sampled_q: 622.465, logp: 2.788, alpha: 0.421
	batch_reward: 6.593, batch_reward_max: 10.608, batch_reward_min: -1.539

2023-03-11 11:56:21 - 
[#Step 760000] eval_reward: 9471.388, eval_step: 1000, eval_time: 2, time: 15.176
	actor_loss: -619.917, critic_loss: 25.230, alpha_loss: 0.137
	q1: 620.504, target_q: 620.541, sampled_q: 621.037, logp: 2.672, alpha: 0.419
	batch_reward: 6.452, batch_reward_max: 11.329, batch_reward_min: -1.621

2023-03-11 11:56:32 - 
[#Step 770000] eval_reward: 9650.355, eval_step: 1000, eval_time: 2, time: 15.373
	actor_loss: -639.464, critic_loss: 24.531, alpha_loss: 0.037
	q1: 640.769, target_q: 641.344, sampled_q: 640.683, logp: 2.911, alpha: 0.419
	batch_reward: 6.939, batch_reward_max: 11.139, batch_reward_min: -1.061

2023-03-11 11:56:44 - 
[#Step 780000] eval_reward: 9793.963, eval_step: 1000, eval_time: 2, time: 15.571
	actor_loss: -637.799, critic_loss: 24.147, alpha_loss: 0.006
	q1: 638.950, target_q: 638.589, sampled_q: 639.056, logp: 2.985, alpha: 0.421
	batch_reward: 6.814, batch_reward_max: 11.025, batch_reward_min: -1.103

2023-03-11 11:56:56 - 
[#Step 790000] eval_reward: 9864.379, eval_step: 1000, eval_time: 2, time: 15.771
	actor_loss: -629.845, critic_loss: 26.818, alpha_loss: 0.084
	q1: 630.609, target_q: 630.566, sampled_q: 631.037, logp: 2.803, alpha: 0.425
	batch_reward: 6.625, batch_reward_max: 11.104, batch_reward_min: -1.601

2023-03-11 11:57:08 - 
[#Step 800000] eval_reward: 9804.517, eval_step: 1000, eval_time: 2, time: 15.971
	actor_loss: -643.821, critic_loss: 33.067, alpha_loss: -0.070
	q1: 644.225, target_q: 645.893, sampled_q: 645.174, logp: 3.163, alpha: 0.428
	batch_reward: 6.916, batch_reward_max: 11.282, batch_reward_min: -1.122

2023-03-11 11:57:08 - Saving checkpoint at step: 4
2023-03-11 11:57:08 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/actor_4
2023-03-11 11:57:08 - Saving checkpoint at step: 4
2023-03-11 11:57:08 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/critic_4
2023-03-11 11:57:20 - 
[#Step 810000] eval_reward: 9819.561, eval_step: 1000, eval_time: 2, time: 16.171
	actor_loss: -637.225, critic_loss: 19.955, alpha_loss: -0.057
	q1: 638.398, target_q: 637.921, sampled_q: 638.550, logp: 3.136, alpha: 0.423
	batch_reward: 6.741, batch_reward_max: 11.200, batch_reward_min: -1.541

2023-03-11 11:57:32 - 
[#Step 820000] eval_reward: 9676.908, eval_step: 1000, eval_time: 2, time: 16.371
	actor_loss: -640.713, critic_loss: 23.741, alpha_loss: -0.019
	q1: 641.645, target_q: 642.279, sampled_q: 642.016, logp: 3.044, alpha: 0.428
	batch_reward: 6.824, batch_reward_max: 10.515, batch_reward_min: -1.480

2023-03-11 11:57:44 - 
[#Step 830000] eval_reward: 9951.399, eval_step: 1000, eval_time: 2, time: 16.571
	actor_loss: -637.400, critic_loss: 31.230, alpha_loss: -0.265
	q1: 637.990, target_q: 638.800, sampled_q: 638.954, logp: 3.617, alpha: 0.429
	batch_reward: 6.721, batch_reward_max: 10.938, batch_reward_min: -1.528

2023-03-11 11:57:56 - 
[#Step 840000] eval_reward: 10132.809, eval_step: 1000, eval_time: 2, time: 16.773
	actor_loss: -654.909, critic_loss: 30.867, alpha_loss: -0.062
	q1: 655.583, target_q: 655.113, sampled_q: 656.289, logp: 3.140, alpha: 0.439
	batch_reward: 7.075, batch_reward_max: 11.212, batch_reward_min: -1.529

2023-03-11 11:58:09 - 
[#Step 850000] eval_reward: 9591.303, eval_step: 1000, eval_time: 2, time: 16.975
	actor_loss: -648.912, critic_loss: 21.432, alpha_loss: 0.156
	q1: 649.581, target_q: 648.974, sampled_q: 650.063, logp: 2.643, alpha: 0.435
	batch_reward: 7.034, batch_reward_max: 11.063, batch_reward_min: -1.285

2023-03-11 11:58:21 - 
[#Step 860000] eval_reward: 9816.181, eval_step: 1000, eval_time: 2, time: 17.175
	actor_loss: -651.645, critic_loss: 21.485, alpha_loss: -0.070
	q1: 652.206, target_q: 652.521, sampled_q: 653.041, logp: 3.158, alpha: 0.442
	batch_reward: 6.770, batch_reward_max: 10.733, batch_reward_min: -2.095

2023-03-11 11:58:33 - 
[#Step 870000] eval_reward: 10161.862, eval_step: 1000, eval_time: 2, time: 17.374
	actor_loss: -650.870, critic_loss: 39.437, alpha_loss: 0.086
	q1: 651.244, target_q: 651.015, sampled_q: 652.083, logp: 2.802, alpha: 0.433
	batch_reward: 6.701, batch_reward_max: 11.141, batch_reward_min: -0.955

2023-03-11 11:58:45 - 
[#Step 880000] eval_reward: 10012.926, eval_step: 1000, eval_time: 2, time: 17.575
	actor_loss: -669.615, critic_loss: 27.614, alpha_loss: -0.147
	q1: 670.621, target_q: 670.264, sampled_q: 671.081, logp: 3.334, alpha: 0.440
	batch_reward: 7.118, batch_reward_max: 11.910, batch_reward_min: -0.956

2023-03-11 11:58:56 - 
[#Step 890000] eval_reward: 10322.123, eval_step: 1000, eval_time: 2, time: 17.773
	actor_loss: -668.354, critic_loss: 35.912, alpha_loss: -0.082
	q1: 669.622, target_q: 668.845, sampled_q: 669.761, logp: 3.187, alpha: 0.442
	batch_reward: 7.202, batch_reward_max: 11.364, batch_reward_min: -0.782

2023-03-11 11:59:09 - 
[#Step 900000] eval_reward: 9810.268, eval_step: 1000, eval_time: 2, time: 17.974
	actor_loss: -661.448, critic_loss: 29.501, alpha_loss: 0.072
	q1: 662.353, target_q: 662.670, sampled_q: 662.697, logp: 2.837, alpha: 0.440
	batch_reward: 7.125, batch_reward_max: 11.639, batch_reward_min: -0.927

2023-03-11 11:59:21 - 
[#Step 910000] eval_reward: 10267.467, eval_step: 1000, eval_time: 2, time: 18.176
	actor_loss: -664.267, critic_loss: 20.266, alpha_loss: 0.078
	q1: 665.179, target_q: 664.979, sampled_q: 665.544, logp: 2.827, alpha: 0.452
	batch_reward: 6.988, batch_reward_max: 11.615, batch_reward_min: -1.615

2023-03-11 11:59:33 - 
[#Step 920000] eval_reward: 10382.948, eval_step: 1000, eval_time: 2, time: 18.377
	actor_loss: -673.145, critic_loss: 39.930, alpha_loss: -0.139
	q1: 674.075, target_q: 673.359, sampled_q: 674.636, logp: 3.308, alpha: 0.451
	batch_reward: 7.125, batch_reward_max: 11.228, batch_reward_min: -0.840

2023-03-11 11:59:45 - 
[#Step 930000] eval_reward: 10285.388, eval_step: 1000, eval_time: 2, time: 18.577
	actor_loss: -677.281, critic_loss: 79.627, alpha_loss: 0.000
	q1: 677.800, target_q: 678.671, sampled_q: 678.626, logp: 3.000, alpha: 0.449
	batch_reward: 7.154, batch_reward_max: 11.327, batch_reward_min: -0.504

2023-03-11 11:59:57 - 
[#Step 940000] eval_reward: 10553.822, eval_step: 1000, eval_time: 2, time: 18.776
	actor_loss: -680.129, critic_loss: 21.220, alpha_loss: -0.018
	q1: 680.991, target_q: 680.944, sampled_q: 681.492, logp: 3.040, alpha: 0.448
	batch_reward: 7.194, batch_reward_max: 11.380, batch_reward_min: -1.314

2023-03-11 12:00:09 - 
[#Step 950000] eval_reward: 10602.838, eval_step: 1000, eval_time: 2, time: 18.975
	actor_loss: -686.575, critic_loss: 34.010, alpha_loss: -0.103
	q1: 687.084, target_q: 688.208, sampled_q: 688.034, logp: 3.229, alpha: 0.452
	batch_reward: 7.289, batch_reward_max: 11.206, batch_reward_min: -0.664

2023-03-11 12:00:15 - 
[#Step 955000] eval_reward: 9970.870, eval_step: 1000, eval_time: 2, time: 19.089
	actor_loss: -675.677, critic_loss: 36.372, alpha_loss: -0.009
	q1: 676.308, target_q: 676.997, sampled_q: 677.048, logp: 3.020, alpha: 0.454
	batch_reward: 7.142, batch_reward_max: 11.927, batch_reward_min: -0.450

2023-03-11 12:00:22 - 
[#Step 960000] eval_reward: 10409.903, eval_step: 1000, eval_time: 2, time: 19.205
	actor_loss: -673.963, critic_loss: 28.764, alpha_loss: -0.043
	q1: 674.984, target_q: 675.283, sampled_q: 675.356, logp: 3.096, alpha: 0.450
	batch_reward: 6.856, batch_reward_max: 11.508, batch_reward_min: -0.998

2023-03-11 12:00:29 - 
[#Step 965000] eval_reward: 10387.214, eval_step: 1000, eval_time: 2, time: 19.320
	actor_loss: -693.254, critic_loss: 51.637, alpha_loss: 0.047
	q1: 693.600, target_q: 693.588, sampled_q: 694.571, logp: 2.898, alpha: 0.454
	batch_reward: 7.433, batch_reward_max: 12.180, batch_reward_min: -1.717

2023-03-11 12:00:36 - 
[#Step 970000] eval_reward: 9903.665, eval_step: 1000, eval_time: 2, time: 19.440
	actor_loss: -684.719, critic_loss: 192.305, alpha_loss: -0.168
	q1: 685.838, target_q: 684.928, sampled_q: 686.265, logp: 3.366, alpha: 0.460
	batch_reward: 7.260, batch_reward_max: 11.652, batch_reward_min: -0.766

2023-03-11 12:00:43 - 
[#Step 975000] eval_reward: 10454.151, eval_step: 1000, eval_time: 2, time: 19.556
	actor_loss: -681.573, critic_loss: 28.879, alpha_loss: -0.018
	q1: 682.190, target_q: 681.529, sampled_q: 682.952, logp: 3.039, alpha: 0.454
	batch_reward: 7.209, batch_reward_max: 11.551, batch_reward_min: -1.051

2023-03-11 12:00:50 - 
[#Step 980000] eval_reward: 10613.271, eval_step: 1000, eval_time: 2, time: 19.673
	actor_loss: -694.349, critic_loss: 39.991, alpha_loss: -0.077
	q1: 695.578, target_q: 695.475, sampled_q: 695.811, logp: 3.166, alpha: 0.462
	batch_reward: 7.453, batch_reward_max: 11.735, batch_reward_min: -0.555

2023-03-11 12:00:57 - 
[#Step 985000] eval_reward: 10521.626, eval_step: 1000, eval_time: 2, time: 19.790
	actor_loss: -682.800, critic_loss: 19.578, alpha_loss: -0.007
	q1: 683.355, target_q: 683.676, sampled_q: 684.176, logp: 3.016, alpha: 0.456
	batch_reward: 7.154, batch_reward_max: 11.741, batch_reward_min: -1.308

2023-03-11 12:01:04 - 
[#Step 990000] eval_reward: 10698.819, eval_step: 1000, eval_time: 2, time: 19.907
	actor_loss: -691.986, critic_loss: 27.687, alpha_loss: 0.001
	q1: 692.822, target_q: 692.140, sampled_q: 693.369, logp: 2.997, alpha: 0.462
	batch_reward: 7.320, batch_reward_max: 11.276, batch_reward_min: -0.861

2023-03-11 12:01:11 - 
[#Step 995000] eval_reward: 10540.478, eval_step: 1000, eval_time: 2, time: 20.024
	actor_loss: -694.521, critic_loss: 25.260, alpha_loss: -0.000
	q1: 695.172, target_q: 696.062, sampled_q: 695.912, logp: 3.000, alpha: 0.464
	batch_reward: 7.236, batch_reward_max: 11.676, batch_reward_min: -0.848

2023-03-11 12:01:19 - 
[#Step 1000000] eval_reward: 10677.986, eval_step: 1000, eval_time: 2, time: 20.141
	actor_loss: -686.085, critic_loss: 27.266, alpha_loss: 0.101
	q1: 686.561, target_q: 688.171, sampled_q: 687.370, logp: 2.782, alpha: 0.462
	batch_reward: 7.216, batch_reward_max: 11.754, batch_reward_min: -1.188

2023-03-11 12:01:19 - Saving checkpoint at step: 5
2023-03-11 12:01:19 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/actor_5
2023-03-11 12:01:19 - Saving checkpoint at step: 5
2023-03-11 12:01:19 - Saved checkpoint at saved_models/halfcheetah-v4/sac_s2_20230311_114110/critic_5
