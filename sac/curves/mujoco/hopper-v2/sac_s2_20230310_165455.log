2023-03-10 16:54:55 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: Hopper-v2
eval_episodes: 10
eval_freq: 5000
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: orthogonal
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
seed: 2
start_timesteps: 10000
tau: 0.005

2023-03-10 16:55:03 - 
[#Step 10000] eval_reward: 32.374, eval_time: 0

2023-03-10 16:55:17 - 
[#Step 20000] eval_reward: 413.446, eval_step: 191, eval_time: 1, time: 0.353
	actor_loss: -99.246, critic_loss: 57.445, alpha_loss: -0.057
	q1: 94.409, target_q: 95.496, logp: 1.763, alpha: 0.215
	batch_reward: 1.373, batch_reward_max: 3.701, batch_reward_min: -0.469

2023-03-10 16:55:28 - 
[#Step 30000] eval_reward: 336.660, eval_step: 124, eval_time: 0, time: 0.542
	actor_loss: -155.847, critic_loss: 41.307, alpha_loss: 0.024
	q1: 152.194, target_q: 151.591, logp: 1.395, alpha: 0.228
	batch_reward: 1.763, batch_reward_max: 4.090, batch_reward_min: -0.135

2023-03-10 16:55:40 - 
[#Step 40000] eval_reward: 381.927, eval_step: 178, eval_time: 1, time: 0.738
	actor_loss: -172.559, critic_loss: 35.929, alpha_loss: 0.060
	q1: 170.051, target_q: 169.312, logp: 1.264, alpha: 0.255
	batch_reward: 1.917, batch_reward_max: 4.974, batch_reward_min: -0.367

2023-03-10 16:55:51 - 
[#Step 50000] eval_reward: 13.077, eval_step: 23, eval_time: 0, time: 0.919
	actor_loss: -169.238, critic_loss: 63.280, alpha_loss: -0.017
	q1: 166.094, target_q: 165.568, logp: 1.579, alpha: 0.217
	batch_reward: 2.088, batch_reward_max: 4.902, batch_reward_min: -0.364

2023-03-10 16:56:02 - 
[#Step 60000] eval_reward: 689.227, eval_step: 238, eval_time: 1, time: 1.111
	actor_loss: -181.816, critic_loss: 28.177, alpha_loss: -0.017
	q1: 179.046, target_q: 179.266, logp: 1.582, alpha: 0.206
	batch_reward: 2.171, batch_reward_max: 5.138, batch_reward_min: -0.603

2023-03-10 16:56:13 - 
[#Step 70000] eval_reward: 299.399, eval_step: 116, eval_time: 0, time: 1.297
	actor_loss: -174.281, critic_loss: 28.877, alpha_loss: -0.040
	q1: 172.063, target_q: 172.132, logp: 1.707, alpha: 0.193
	batch_reward: 2.173, batch_reward_max: 5.216, batch_reward_min: -0.220

2023-03-10 16:56:25 - 
[#Step 80000] eval_reward: 977.453, eval_step: 317, eval_time: 1, time: 1.494
	actor_loss: -180.432, critic_loss: 52.642, alpha_loss: -0.019
	q1: 177.701, target_q: 178.990, logp: 1.587, alpha: 0.216
	batch_reward: 2.231, batch_reward_max: 5.037, batch_reward_min: 0.008

2023-03-10 16:56:37 - 
[#Step 90000] eval_reward: 972.207, eval_step: 317, eval_time: 1, time: 1.693
	actor_loss: -185.109, critic_loss: 22.850, alpha_loss: 0.001
	q1: 183.837, target_q: 184.243, logp: 1.496, alpha: 0.191
	batch_reward: 2.426, batch_reward_max: 5.125, batch_reward_min: -0.309

2023-03-10 16:56:49 - 
[#Step 100000] eval_reward: 1218.632, eval_step: 410, eval_time: 1, time: 1.896
	actor_loss: -189.347, critic_loss: 46.142, alpha_loss: -0.035
	q1: 187.395, target_q: 187.767, logp: 1.695, alpha: 0.182
	batch_reward: 2.524, batch_reward_max: 5.333, batch_reward_min: -0.273

2023-03-10 16:57:02 - 
[#Step 110000] eval_reward: 1282.689, eval_step: 410, eval_time: 1, time: 2.106
	actor_loss: -195.183, critic_loss: 30.185, alpha_loss: 0.013
	q1: 193.314, target_q: 193.662, logp: 1.420, alpha: 0.166
	batch_reward: 2.487, batch_reward_max: 4.942, batch_reward_min: 0.065

2023-03-10 16:57:15 - 
[#Step 120000] eval_reward: 1783.874, eval_step: 561, eval_time: 2, time: 2.323
	actor_loss: -203.384, critic_loss: 11.239, alpha_loss: -0.007
	q1: 202.704, target_q: 202.499, logp: 1.544, alpha: 0.156
	batch_reward: 2.580, batch_reward_max: 5.441, batch_reward_min: 0.034

2023-03-10 16:57:28 - 
[#Step 130000] eval_reward: 1738.733, eval_step: 574, eval_time: 2, time: 2.539
	actor_loss: -206.895, critic_loss: 13.534, alpha_loss: 0.005
	q1: 205.758, target_q: 206.005, logp: 1.464, alpha: 0.144
	batch_reward: 2.513, batch_reward_max: 5.089, batch_reward_min: -0.589

2023-03-10 16:57:40 - 
[#Step 140000] eval_reward: 1243.266, eval_step: 378, eval_time: 1, time: 2.745
	actor_loss: -217.983, critic_loss: 15.848, alpha_loss: -0.010
	q1: 216.500, target_q: 216.455, logp: 1.573, alpha: 0.142
	batch_reward: 2.594, batch_reward_max: 5.556, batch_reward_min: -0.608

2023-03-10 16:57:52 - 
[#Step 150000] eval_reward: 1061.839, eval_step: 323, eval_time: 1, time: 2.947
	actor_loss: -207.246, critic_loss: 10.134, alpha_loss: -0.047
	q1: 205.850, target_q: 205.603, logp: 1.842, alpha: 0.137
	batch_reward: 2.732, batch_reward_max: 5.695, batch_reward_min: 0.154

2023-03-10 16:58:04 - 
[#Step 160000] eval_reward: 817.929, eval_step: 249, eval_time: 1, time: 3.139
	actor_loss: -219.553, critic_loss: 22.072, alpha_loss: -0.026
	q1: 218.528, target_q: 218.465, logp: 1.691, alpha: 0.134
	batch_reward: 2.651, batch_reward_max: 5.718, batch_reward_min: 0.023

2023-03-10 16:58:17 - 
[#Step 170000] eval_reward: 2013.848, eval_step: 625, eval_time: 2, time: 3.356
	actor_loss: -214.201, critic_loss: 15.697, alpha_loss: -0.037
	q1: 213.592, target_q: 213.038, logp: 1.789, alpha: 0.129
	batch_reward: 2.812, batch_reward_max: 5.540, batch_reward_min: 0.062

2023-03-10 16:58:29 - 
[#Step 180000] eval_reward: 1199.180, eval_step: 361, eval_time: 1, time: 3.560
	actor_loss: -222.446, critic_loss: 7.752, alpha_loss: -0.002
	q1: 220.735, target_q: 220.901, logp: 1.518, alpha: 0.123
	batch_reward: 2.821, batch_reward_max: 5.343, batch_reward_min: -0.236

2023-03-10 16:58:42 - 
[#Step 190000] eval_reward: 1520.975, eval_step: 466, eval_time: 2, time: 3.769
	actor_loss: -223.318, critic_loss: 19.502, alpha_loss: -0.012
	q1: 222.913, target_q: 222.714, logp: 1.595, alpha: 0.121
	batch_reward: 2.751, batch_reward_max: 5.709, batch_reward_min: -0.195

2023-03-10 16:58:55 - 
[#Step 200000] eval_reward: 2055.256, eval_step: 662, eval_time: 2, time: 3.995
	actor_loss: -227.079, critic_loss: 8.180, alpha_loss: 0.007
	q1: 225.598, target_q: 226.062, logp: 1.438, alpha: 0.119
	batch_reward: 2.738, batch_reward_max: 5.122, batch_reward_min: -0.174

2023-03-10 16:58:55 - Saving checkpoint at step: 1
2023-03-10 16:58:55 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/actor_1
2023-03-10 16:58:55 - Saving checkpoint at step: 1
2023-03-10 16:58:55 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/critic_1
2023-03-10 16:59:08 - 
[#Step 210000] eval_reward: 1546.056, eval_step: 471, eval_time: 2, time: 4.205
	actor_loss: -227.885, critic_loss: 24.777, alpha_loss: -0.009
	q1: 227.274, target_q: 226.922, logp: 1.580, alpha: 0.116
	batch_reward: 2.945, batch_reward_max: 5.886, batch_reward_min: 0.273

2023-03-10 16:59:22 - 
[#Step 220000] eval_reward: 2577.508, eval_step: 834, eval_time: 3, time: 4.436
	actor_loss: -226.467, critic_loss: 12.558, alpha_loss: -0.015
	q1: 226.178, target_q: 225.863, logp: 1.635, alpha: 0.112
	batch_reward: 2.915, batch_reward_max: 5.525, batch_reward_min: -0.112

2023-03-10 16:59:36 - 
[#Step 230000] eval_reward: 2991.917, eval_step: 1000, eval_time: 3, time: 4.677
	actor_loss: -236.621, critic_loss: 10.204, alpha_loss: -0.023
	q1: 235.280, target_q: 235.008, logp: 1.708, alpha: 0.110
	batch_reward: 2.727, batch_reward_max: 5.589, batch_reward_min: -0.301

2023-03-10 16:59:50 - 
[#Step 240000] eval_reward: 2695.171, eval_step: 926, eval_time: 3, time: 4.909
	actor_loss: -238.425, critic_loss: 7.351, alpha_loss: -0.002
	q1: 237.951, target_q: 237.551, logp: 1.520, alpha: 0.105
	batch_reward: 2.879, batch_reward_max: 5.373, batch_reward_min: 0.062

2023-03-10 17:00:03 - 
[#Step 250000] eval_reward: 1925.614, eval_step: 602, eval_time: 2, time: 5.128
	actor_loss: -248.786, critic_loss: 48.900, alpha_loss: -0.016
	q1: 247.771, target_q: 246.878, logp: 1.658, alpha: 0.104
	batch_reward: 2.776, batch_reward_max: 5.443, batch_reward_min: -0.250

2023-03-10 17:00:15 - 
[#Step 260000] eval_reward: 1252.572, eval_step: 378, eval_time: 1, time: 5.329
	actor_loss: -235.357, critic_loss: 8.135, alpha_loss: -0.006
	q1: 234.627, target_q: 234.707, logp: 1.554, alpha: 0.103
	batch_reward: 2.912, batch_reward_max: 5.689, batch_reward_min: -0.569

2023-03-10 17:00:28 - 
[#Step 270000] eval_reward: 2006.827, eval_step: 623, eval_time: 2, time: 5.545
	actor_loss: -249.993, critic_loss: 36.100, alpha_loss: 0.014
	q1: 249.541, target_q: 249.473, logp: 1.354, alpha: 0.098
	batch_reward: 2.870, batch_reward_max: 5.318, batch_reward_min: 0.477

2023-03-10 17:00:42 - 
[#Step 280000] eval_reward: 2698.717, eval_step: 854, eval_time: 3, time: 5.772
	actor_loss: -243.558, critic_loss: 4.450, alpha_loss: 0.012
	q1: 242.786, target_q: 242.582, logp: 1.373, alpha: 0.096
	batch_reward: 3.044, batch_reward_max: 5.438, batch_reward_min: 0.460

2023-03-10 17:00:55 - 
[#Step 290000] eval_reward: 2229.789, eval_step: 698, eval_time: 3, time: 5.999
	actor_loss: -250.376, critic_loss: 7.307, alpha_loss: -0.004
	q1: 249.989, target_q: 250.080, logp: 1.540, alpha: 0.093
	batch_reward: 2.930, batch_reward_max: 5.469, batch_reward_min: 0.065

2023-03-10 17:01:10 - 
[#Step 300000] eval_reward: 3151.270, eval_step: 1000, eval_time: 3, time: 6.240
	actor_loss: -253.922, critic_loss: 8.889, alpha_loss: 0.008
	q1: 253.350, target_q: 253.457, logp: 1.409, alpha: 0.093
	batch_reward: 2.959, batch_reward_max: 5.876, batch_reward_min: -0.100

2023-03-10 17:01:24 - 
[#Step 310000] eval_reward: 3090.762, eval_step: 1000, eval_time: 4, time: 6.480
	actor_loss: -245.948, critic_loss: 8.183, alpha_loss: 0.023
	q1: 246.067, target_q: 246.034, logp: 1.255, alpha: 0.093
	batch_reward: 3.010, batch_reward_max: 5.495, batch_reward_min: 0.661

2023-03-10 17:01:38 - 
[#Step 320000] eval_reward: 2586.546, eval_step: 809, eval_time: 3, time: 6.709
	actor_loss: -256.422, critic_loss: 4.017, alpha_loss: -0.002
	q1: 255.866, target_q: 256.148, logp: 1.522, alpha: 0.090
	batch_reward: 2.942, batch_reward_max: 5.426, batch_reward_min: 0.026

2023-03-10 17:01:51 - 
[#Step 330000] eval_reward: 1728.304, eval_step: 523, eval_time: 2, time: 6.921
	actor_loss: -259.456, critic_loss: 7.446, alpha_loss: 0.005
	q1: 257.485, target_q: 257.660, logp: 1.450, alpha: 0.091
	batch_reward: 2.934, batch_reward_max: 5.360, batch_reward_min: -0.239

2023-03-10 17:02:04 - 
[#Step 340000] eval_reward: 2489.918, eval_step: 767, eval_time: 3, time: 7.149
	actor_loss: -253.917, critic_loss: 9.207, alpha_loss: 0.000
	q1: 253.679, target_q: 253.155, logp: 1.497, alpha: 0.085
	batch_reward: 3.042, batch_reward_max: 5.544, batch_reward_min: 0.031

2023-03-10 17:02:19 - 
[#Step 350000] eval_reward: 3166.351, eval_step: 1000, eval_time: 4, time: 7.390
	actor_loss: -258.639, critic_loss: 6.004, alpha_loss: 0.011
	q1: 258.278, target_q: 258.022, logp: 1.373, alpha: 0.084
	batch_reward: 3.024, batch_reward_max: 5.275, batch_reward_min: 0.505

2023-03-10 17:02:32 - 
[#Step 360000] eval_reward: 2380.824, eval_step: 734, eval_time: 3, time: 7.615
	actor_loss: -267.693, critic_loss: 7.004, alpha_loss: 0.006
	q1: 267.578, target_q: 267.137, logp: 1.430, alpha: 0.082
	batch_reward: 3.002, batch_reward_max: 5.319, batch_reward_min: 0.641

2023-03-10 17:02:46 - 
[#Step 370000] eval_reward: 2357.413, eval_step: 739, eval_time: 2, time: 7.839
	actor_loss: -268.787, critic_loss: 3.596, alpha_loss: 0.004
	q1: 268.342, target_q: 268.499, logp: 1.446, alpha: 0.082
	batch_reward: 2.951, batch_reward_max: 5.484, batch_reward_min: -0.060

2023-03-10 17:02:58 - 
[#Step 380000] eval_reward: 1661.981, eval_step: 504, eval_time: 2, time: 8.048
	actor_loss: -262.870, critic_loss: 8.130, alpha_loss: -0.021
	q1: 262.750, target_q: 262.645, logp: 1.756, alpha: 0.082
	batch_reward: 3.058, batch_reward_max: 5.749, batch_reward_min: 0.261

2023-03-10 17:03:12 - 
[#Step 390000] eval_reward: 2595.542, eval_step: 788, eval_time: 3, time: 8.272
	actor_loss: -260.508, critic_loss: 10.308, alpha_loss: -0.006
	q1: 260.440, target_q: 261.219, logp: 1.579, alpha: 0.079
	batch_reward: 2.977, batch_reward_max: 5.002, batch_reward_min: 0.328

2023-03-10 17:03:24 - 
[#Step 400000] eval_reward: 1303.271, eval_step: 387, eval_time: 1, time: 8.475
	actor_loss: -269.332, critic_loss: 5.198, alpha_loss: -0.004
	q1: 269.369, target_q: 268.650, logp: 1.555, alpha: 0.077
	batch_reward: 3.059, batch_reward_max: 5.479, batch_reward_min: 0.271

2023-03-10 17:03:24 - Saving checkpoint at step: 2
2023-03-10 17:03:24 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/actor_2
2023-03-10 17:03:24 - Saving checkpoint at step: 2
2023-03-10 17:03:24 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/critic_2
2023-03-10 17:03:36 - 
[#Step 410000] eval_reward: 1189.274, eval_step: 356, eval_time: 1, time: 8.680
	actor_loss: -264.082, critic_loss: 6.358, alpha_loss: -0.008
	q1: 264.100, target_q: 264.252, logp: 1.603, alpha: 0.078
	batch_reward: 3.113, batch_reward_max: 5.354, batch_reward_min: 0.505

2023-03-10 17:03:51 - 
[#Step 420000] eval_reward: 3164.441, eval_step: 1000, eval_time: 3, time: 8.920
	actor_loss: -270.429, critic_loss: 4.532, alpha_loss: -0.003
	q1: 269.953, target_q: 269.420, logp: 1.535, alpha: 0.074
	batch_reward: 3.134, batch_reward_max: 5.730, batch_reward_min: -0.029

2023-03-10 17:04:05 - 
[#Step 430000] eval_reward: 3206.423, eval_step: 1000, eval_time: 4, time: 9.161
	actor_loss: -273.125, critic_loss: 6.858, alpha_loss: 0.003
	q1: 272.231, target_q: 272.057, logp: 1.458, alpha: 0.075
	batch_reward: 2.980, batch_reward_max: 5.101, batch_reward_min: -0.166

2023-03-10 17:04:19 - 
[#Step 440000] eval_reward: 2614.905, eval_step: 780, eval_time: 3, time: 9.389
	actor_loss: -273.006, critic_loss: 3.093, alpha_loss: 0.016
	q1: 272.874, target_q: 273.107, logp: 1.278, alpha: 0.073
	batch_reward: 3.067, batch_reward_max: 5.528, batch_reward_min: 0.707

2023-03-10 17:04:33 - 
[#Step 450000] eval_reward: 3034.317, eval_step: 932, eval_time: 3, time: 9.624
	actor_loss: -271.097, critic_loss: 2.886, alpha_loss: 0.004
	q1: 270.942, target_q: 270.535, logp: 1.441, alpha: 0.073
	batch_reward: 3.037, batch_reward_max: 5.326, batch_reward_min: 0.308

2023-03-10 17:04:47 - 
[#Step 460000] eval_reward: 2906.741, eval_step: 881, eval_time: 3, time: 9.859
	actor_loss: -264.475, critic_loss: 4.602, alpha_loss: -0.003
	q1: 264.320, target_q: 264.610, logp: 1.538, alpha: 0.072
	batch_reward: 3.035, batch_reward_max: 5.692, batch_reward_min: -0.225

2023-03-10 17:05:01 - 
[#Step 470000] eval_reward: 2747.838, eval_step: 836, eval_time: 3, time: 10.091
	actor_loss: -273.335, critic_loss: 4.405, alpha_loss: 0.010
	q1: 273.139, target_q: 273.652, logp: 1.364, alpha: 0.071
	batch_reward: 3.049, batch_reward_max: 5.218, batch_reward_min: 0.273

2023-03-10 17:05:15 - 
[#Step 480000] eval_reward: 3205.720, eval_step: 1000, eval_time: 4, time: 10.334
	actor_loss: -281.386, critic_loss: 5.113, alpha_loss: 0.008
	q1: 280.816, target_q: 280.981, logp: 1.389, alpha: 0.069
	batch_reward: 2.972, batch_reward_max: 5.593, batch_reward_min: 0.258

2023-03-10 17:05:30 - 
[#Step 490000] eval_reward: 3010.176, eval_step: 931, eval_time: 3, time: 10.573
	actor_loss: -271.402, critic_loss: 8.634, alpha_loss: -0.020
	q1: 270.293, target_q: 270.576, logp: 1.792, alpha: 0.068
	batch_reward: 2.991, batch_reward_max: 5.384, batch_reward_min: 0.147

2023-03-10 17:05:43 - 
[#Step 500000] eval_reward: 2222.671, eval_step: 698, eval_time: 2, time: 10.795
	actor_loss: -282.183, critic_loss: 2.481, alpha_loss: 0.005
	q1: 281.663, target_q: 281.694, logp: 1.431, alpha: 0.068
	batch_reward: 3.036, batch_reward_max: 5.335, batch_reward_min: 0.376

2023-03-10 17:05:56 - 
[#Step 510000] eval_reward: 2027.588, eval_step: 601, eval_time: 2, time: 11.008
	actor_loss: -281.956, critic_loss: 2.446, alpha_loss: -0.001
	q1: 281.747, target_q: 281.719, logp: 1.518, alpha: 0.066
	batch_reward: 3.034, batch_reward_max: 5.592, batch_reward_min: 0.411

2023-03-10 17:06:10 - 
[#Step 520000] eval_reward: 3175.509, eval_step: 978, eval_time: 3, time: 11.242
	actor_loss: -280.069, critic_loss: 3.409, alpha_loss: 0.002
	q1: 280.312, target_q: 279.828, logp: 1.467, alpha: 0.065
	batch_reward: 3.002, batch_reward_max: 4.928, batch_reward_min: 0.585

2023-03-10 17:06:24 - 
[#Step 530000] eval_reward: 3171.385, eval_step: 968, eval_time: 3, time: 11.478
	actor_loss: -275.731, critic_loss: 3.595, alpha_loss: 0.015
	q1: 275.454, target_q: 275.520, logp: 1.262, alpha: 0.065
	batch_reward: 3.076, batch_reward_max: 5.464, batch_reward_min: 0.633

2023-03-10 17:06:38 - 
[#Step 540000] eval_reward: 2645.896, eval_step: 800, eval_time: 3, time: 11.702
	actor_loss: -285.373, critic_loss: 2.341, alpha_loss: -0.000
	q1: 285.266, target_q: 285.374, logp: 1.505, alpha: 0.065
	batch_reward: 3.080, batch_reward_max: 5.262, batch_reward_min: 0.497

2023-03-10 17:06:51 - 
[#Step 550000] eval_reward: 2326.487, eval_step: 705, eval_time: 2, time: 11.924
	actor_loss: -276.590, critic_loss: 7.332, alpha_loss: -0.004
	q1: 276.481, target_q: 276.051, logp: 1.563, alpha: 0.065
	batch_reward: 3.128, batch_reward_max: 5.340, batch_reward_min: 0.485

2023-03-10 17:07:05 - 
[#Step 560000] eval_reward: 3169.687, eval_step: 1000, eval_time: 4, time: 12.166
	actor_loss: -277.635, critic_loss: 3.199, alpha_loss: -0.000
	q1: 277.637, target_q: 277.811, logp: 1.501, alpha: 0.062
	batch_reward: 3.028, batch_reward_max: 5.349, batch_reward_min: 0.657

2023-03-10 17:07:20 - 
[#Step 570000] eval_reward: 2966.680, eval_step: 932, eval_time: 3, time: 12.403
	actor_loss: -288.817, critic_loss: 2.759, alpha_loss: 0.021
	q1: 289.334, target_q: 289.160, logp: 1.175, alpha: 0.063
	batch_reward: 3.082, batch_reward_max: 5.731, batch_reward_min: 0.170

2023-03-10 17:07:34 - 
[#Step 580000] eval_reward: 3187.312, eval_step: 1000, eval_time: 4, time: 12.643
	actor_loss: -282.439, critic_loss: 5.254, alpha_loss: 0.002
	q1: 282.476, target_q: 282.759, logp: 1.474, alpha: 0.061
	batch_reward: 2.998, batch_reward_max: 5.189, batch_reward_min: 0.338

2023-03-10 17:07:48 - 
[#Step 590000] eval_reward: 3211.084, eval_step: 987, eval_time: 3, time: 12.881
	actor_loss: -284.424, critic_loss: 4.947, alpha_loss: 0.023
	q1: 284.537, target_q: 284.265, logp: 1.138, alpha: 0.063
	batch_reward: 3.131, batch_reward_max: 5.759, batch_reward_min: 0.725

2023-03-10 17:08:03 - 
[#Step 600000] eval_reward: 3190.249, eval_step: 1000, eval_time: 3, time: 13.121
	actor_loss: -283.837, critic_loss: 3.384, alpha_loss: 0.001
	q1: 283.650, target_q: 283.066, logp: 1.483, alpha: 0.061
	batch_reward: 3.017, batch_reward_max: 5.522, batch_reward_min: 0.390

2023-03-10 17:08:03 - Saving checkpoint at step: 3
2023-03-10 17:08:03 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/actor_3
2023-03-10 17:08:03 - Saving checkpoint at step: 3
2023-03-10 17:08:03 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/critic_3
2023-03-10 17:08:17 - 
[#Step 610000] eval_reward: 3149.427, eval_step: 1000, eval_time: 3, time: 13.360
	actor_loss: -286.449, critic_loss: 2.518, alpha_loss: -0.000
	q1: 286.390, target_q: 286.434, logp: 1.504, alpha: 0.060
	batch_reward: 3.148, batch_reward_max: 5.726, batch_reward_min: 0.554

2023-03-10 17:08:31 - 
[#Step 620000] eval_reward: 3225.120, eval_step: 1000, eval_time: 3, time: 13.598
	actor_loss: -288.055, critic_loss: 1.894, alpha_loss: -0.009
	q1: 287.917, target_q: 287.610, logp: 1.645, alpha: 0.059
	batch_reward: 3.127, batch_reward_max: 5.135, batch_reward_min: 0.543

2023-03-10 17:08:45 - 
[#Step 630000] eval_reward: 2825.669, eval_step: 876, eval_time: 3, time: 13.828
	actor_loss: -287.169, critic_loss: 3.761, alpha_loss: 0.014
	q1: 286.801, target_q: 286.558, logp: 1.244, alpha: 0.056
	batch_reward: 3.111, batch_reward_max: 5.612, batch_reward_min: -0.022

2023-03-10 17:08:59 - 
[#Step 640000] eval_reward: 3211.584, eval_step: 1000, eval_time: 3, time: 14.065
	actor_loss: -287.723, critic_loss: 2.256, alpha_loss: -0.001
	q1: 287.727, target_q: 287.621, logp: 1.517, alpha: 0.057
	batch_reward: 3.052, batch_reward_max: 5.165, batch_reward_min: 0.703

2023-03-10 17:09:14 - 
[#Step 650000] eval_reward: 3189.224, eval_step: 1000, eval_time: 4, time: 14.304
	actor_loss: -285.098, critic_loss: 3.550, alpha_loss: -0.004
	q1: 284.875, target_q: 285.510, logp: 1.566, alpha: 0.055
	batch_reward: 3.151, batch_reward_max: 5.159, batch_reward_min: 0.685

2023-03-10 17:09:28 - 
[#Step 660000] eval_reward: 3059.236, eval_step: 929, eval_time: 3, time: 14.537
	actor_loss: -283.644, critic_loss: 5.504, alpha_loss: 0.009
	q1: 283.613, target_q: 283.408, logp: 1.328, alpha: 0.055
	batch_reward: 3.117, batch_reward_max: 5.498, batch_reward_min: -0.342

2023-03-10 17:09:42 - 
[#Step 670000] eval_reward: 2923.992, eval_step: 883, eval_time: 3, time: 14.774
	actor_loss: -287.287, critic_loss: 4.757, alpha_loss: 0.013
	q1: 286.590, target_q: 286.641, logp: 1.264, alpha: 0.055
	batch_reward: 3.146, batch_reward_max: 5.746, batch_reward_min: 0.253

2023-03-10 17:09:55 - 
[#Step 680000] eval_reward: 2186.748, eval_step: 642, eval_time: 2, time: 14.996
	actor_loss: -296.839, critic_loss: 3.015, alpha_loss: -0.006
	q1: 296.659, target_q: 296.237, logp: 1.603, alpha: 0.054
	batch_reward: 3.041, batch_reward_max: 4.843, batch_reward_min: 0.396

2023-03-10 17:10:10 - 
[#Step 690000] eval_reward: 3263.336, eval_step: 1000, eval_time: 4, time: 15.239
	actor_loss: -297.338, critic_loss: 4.869, alpha_loss: -0.003
	q1: 297.442, target_q: 297.147, logp: 1.562, alpha: 0.055
	batch_reward: 3.148, batch_reward_max: 4.731, batch_reward_min: 0.692

2023-03-10 17:10:23 - 
[#Step 700000] eval_reward: 2127.886, eval_step: 617, eval_time: 2, time: 15.457
	actor_loss: -298.288, critic_loss: 3.504, alpha_loss: 0.004
	q1: 297.900, target_q: 298.141, logp: 1.427, alpha: 0.053
	batch_reward: 3.065, batch_reward_max: 5.137, batch_reward_min: -0.427

2023-03-10 17:10:36 - 
[#Step 710000] eval_reward: 1740.395, eval_step: 517, eval_time: 2, time: 15.668
	actor_loss: -292.051, critic_loss: 2.446, alpha_loss: -0.005
	q1: 292.013, target_q: 291.808, logp: 1.593, alpha: 0.053
	batch_reward: 3.184, batch_reward_max: 5.564, batch_reward_min: 0.613

2023-03-10 17:10:49 - 
[#Step 720000] eval_reward: 2368.581, eval_step: 699, eval_time: 2, time: 15.890
	actor_loss: -289.419, critic_loss: 3.675, alpha_loss: 0.008
	q1: 289.673, target_q: 289.437, logp: 1.343, alpha: 0.052
	batch_reward: 3.171, batch_reward_max: 5.687, batch_reward_min: 0.462

2023-03-10 17:11:03 - 
[#Step 730000] eval_reward: 3234.026, eval_step: 1000, eval_time: 4, time: 16.131
	actor_loss: -291.205, critic_loss: 2.144, alpha_loss: -0.009
	q1: 291.206, target_q: 291.286, logp: 1.671, alpha: 0.051
	batch_reward: 3.109, batch_reward_max: 5.972, batch_reward_min: 0.438

2023-03-10 17:11:18 - 
[#Step 740000] eval_reward: 3007.201, eval_step: 910, eval_time: 3, time: 16.369
	actor_loss: -296.460, critic_loss: 2.301, alpha_loss: -0.003
	q1: 296.370, target_q: 296.745, logp: 1.562, alpha: 0.051
	batch_reward: 3.100, batch_reward_max: 4.918, batch_reward_min: 0.575

2023-03-10 17:11:32 - 
[#Step 750000] eval_reward: 3204.456, eval_step: 973, eval_time: 3, time: 16.609
	actor_loss: -297.486, critic_loss: 3.700, alpha_loss: 0.012
	q1: 297.724, target_q: 297.920, logp: 1.276, alpha: 0.052
	batch_reward: 3.085, batch_reward_max: 5.442, batch_reward_min: 0.711

2023-03-10 17:11:46 - 
[#Step 760000] eval_reward: 3262.237, eval_step: 996, eval_time: 3, time: 16.849
	actor_loss: -295.702, critic_loss: 2.786, alpha_loss: 0.008
	q1: 295.476, target_q: 295.212, logp: 1.339, alpha: 0.052
	batch_reward: 3.108, batch_reward_max: 5.606, batch_reward_min: 0.095

2023-03-10 17:12:01 - 
[#Step 770000] eval_reward: 3270.656, eval_step: 1000, eval_time: 3, time: 17.092
	actor_loss: -298.901, critic_loss: 2.602, alpha_loss: 0.018
	q1: 298.991, target_q: 299.051, logp: 1.137, alpha: 0.050
	batch_reward: 3.149, batch_reward_max: 5.433, batch_reward_min: 0.407

2023-03-10 17:12:16 - 
[#Step 780000] eval_reward: 3107.804, eval_step: 942, eval_time: 3, time: 17.335
	actor_loss: -298.595, critic_loss: 7.958, alpha_loss: -0.002
	q1: 298.782, target_q: 297.153, logp: 1.534, alpha: 0.050
	batch_reward: 3.141, batch_reward_max: 6.139, batch_reward_min: 0.641

2023-03-10 17:12:30 - 
[#Step 790000] eval_reward: 3240.084, eval_step: 1000, eval_time: 4, time: 17.583
	actor_loss: -300.666, critic_loss: 2.602, alpha_loss: -0.012
	q1: 300.110, target_q: 300.377, logp: 1.755, alpha: 0.048
	batch_reward: 3.087, batch_reward_max: 4.422, batch_reward_min: 0.781

2023-03-10 17:12:45 - 
[#Step 800000] eval_reward: 3264.566, eval_step: 1000, eval_time: 4, time: 17.830
	actor_loss: -299.108, critic_loss: 1.743, alpha_loss: 0.014
	q1: 299.002, target_q: 298.987, logp: 1.204, alpha: 0.049
	batch_reward: 3.065, batch_reward_max: 5.325, batch_reward_min: 0.674

2023-03-10 17:12:45 - Saving checkpoint at step: 4
2023-03-10 17:12:45 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/actor_4
2023-03-10 17:12:45 - Saving checkpoint at step: 4
2023-03-10 17:12:45 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/critic_4
2023-03-10 17:12:58 - 
[#Step 810000] eval_reward: 1604.615, eval_step: 456, eval_time: 2, time: 18.045
	actor_loss: -291.999, critic_loss: 4.020, alpha_loss: 0.001
	q1: 291.685, target_q: 291.873, logp: 1.477, alpha: 0.049
	batch_reward: 3.188, batch_reward_max: 5.596, batch_reward_min: 0.279

2023-03-10 17:13:13 - 
[#Step 820000] eval_reward: 3246.123, eval_step: 1000, eval_time: 4, time: 18.292
	actor_loss: -295.521, critic_loss: 2.583, alpha_loss: 0.006
	q1: 295.475, target_q: 295.628, logp: 1.374, alpha: 0.046
	batch_reward: 3.130, batch_reward_max: 5.300, batch_reward_min: 0.477

2023-03-10 17:13:27 - 
[#Step 830000] eval_reward: 3051.393, eval_step: 938, eval_time: 3, time: 18.531
	actor_loss: -301.322, critic_loss: 2.021, alpha_loss: -0.002
	q1: 301.272, target_q: 301.362, logp: 1.547, alpha: 0.048
	batch_reward: 3.024, batch_reward_max: 5.376, batch_reward_min: 0.188

2023-03-10 17:13:42 - 
[#Step 840000] eval_reward: 3246.587, eval_step: 989, eval_time: 4, time: 18.772
	actor_loss: -301.103, critic_loss: 4.044, alpha_loss: -0.005
	q1: 301.031, target_q: 300.535, logp: 1.603, alpha: 0.048
	batch_reward: 3.133, batch_reward_max: 5.521, batch_reward_min: 0.663

2023-03-10 17:13:56 - 
[#Step 850000] eval_reward: 3280.834, eval_step: 1000, eval_time: 3, time: 19.011
	actor_loss: -299.051, critic_loss: 1.148, alpha_loss: -0.009
	q1: 299.187, target_q: 298.855, logp: 1.683, alpha: 0.049
	batch_reward: 3.178, batch_reward_max: 5.249, batch_reward_min: 0.208

2023-03-10 17:14:10 - 
[#Step 860000] eval_reward: 3259.916, eval_step: 1000, eval_time: 3, time: 19.249
	actor_loss: -300.232, critic_loss: 2.065, alpha_loss: 0.011
	q1: 300.293, target_q: 299.924, logp: 1.273, alpha: 0.047
	batch_reward: 3.109, batch_reward_max: 6.204, batch_reward_min: 0.478

2023-03-10 17:14:25 - 
[#Step 870000] eval_reward: 3244.108, eval_step: 1000, eval_time: 3, time: 19.488
	actor_loss: -293.829, critic_loss: 6.289, alpha_loss: -0.015
	q1: 293.208, target_q: 293.366, logp: 1.819, alpha: 0.048
	batch_reward: 3.109, batch_reward_max: 5.593, batch_reward_min: -0.608

2023-03-10 17:14:39 - 
[#Step 880000] eval_reward: 3260.677, eval_step: 1000, eval_time: 3, time: 19.728
	actor_loss: -305.096, critic_loss: 2.911, alpha_loss: -0.001
	q1: 305.140, target_q: 304.616, logp: 1.512, alpha: 0.047
	batch_reward: 3.103, batch_reward_max: 4.757, batch_reward_min: 0.359

2023-03-10 17:14:54 - 
[#Step 890000] eval_reward: 3286.861, eval_step: 1000, eval_time: 4, time: 19.971
	actor_loss: -302.119, critic_loss: 1.302, alpha_loss: 0.005
	q1: 302.518, target_q: 302.157, logp: 1.390, alpha: 0.047
	batch_reward: 3.153, batch_reward_max: 5.630, batch_reward_min: 0.685

2023-03-10 17:15:08 - 
[#Step 900000] eval_reward: 3246.067, eval_step: 1000, eval_time: 4, time: 20.213
	actor_loss: -298.423, critic_loss: 12.472, alpha_loss: -0.003
	q1: 298.284, target_q: 298.011, logp: 1.555, alpha: 0.045
	batch_reward: 3.176, batch_reward_max: 5.428, batch_reward_min: 0.463

2023-03-10 17:15:22 - 
[#Step 910000] eval_reward: 3019.892, eval_step: 900, eval_time: 3, time: 20.446
	actor_loss: -296.396, critic_loss: 5.540, alpha_loss: 0.001
	q1: 296.493, target_q: 296.344, logp: 1.487, alpha: 0.048
	batch_reward: 3.147, batch_reward_max: 5.114, batch_reward_min: 0.225

2023-03-10 17:15:36 - 
[#Step 920000] eval_reward: 3160.747, eval_step: 958, eval_time: 3, time: 20.683
	actor_loss: -300.572, critic_loss: 2.411, alpha_loss: -0.003
	q1: 299.908, target_q: 299.944, logp: 1.571, alpha: 0.045
	batch_reward: 3.183, batch_reward_max: 5.684, batch_reward_min: 0.317

2023-03-10 17:15:50 - 
[#Step 930000] eval_reward: 3095.627, eval_step: 942, eval_time: 3, time: 20.913
	actor_loss: -301.640, critic_loss: 3.874, alpha_loss: -0.006
	q1: 301.669, target_q: 301.427, logp: 1.638, alpha: 0.045
	batch_reward: 3.178, batch_reward_max: 5.521, batch_reward_min: 0.801

2023-03-10 17:16:04 - 
[#Step 940000] eval_reward: 3072.320, eval_step: 936, eval_time: 3, time: 21.145
	actor_loss: -303.033, critic_loss: 1.519, alpha_loss: 0.002
	q1: 303.004, target_q: 303.081, logp: 1.456, alpha: 0.046
	batch_reward: 3.131, batch_reward_max: 5.479, batch_reward_min: 0.848

2023-03-10 17:16:18 - 
[#Step 950000] eval_reward: 3294.726, eval_step: 1000, eval_time: 3, time: 21.381
	actor_loss: -301.847, critic_loss: 2.033, alpha_loss: 0.000
	q1: 301.725, target_q: 302.178, logp: 1.499, alpha: 0.045
	batch_reward: 3.205, batch_reward_max: 5.840, batch_reward_min: 0.619

2023-03-10 17:16:27 - 
[#Step 955000] eval_reward: 2706.695, eval_step: 807, eval_time: 3, time: 21.519
	actor_loss: -302.881, critic_loss: 2.806, alpha_loss: -0.000
	q1: 302.772, target_q: 302.441, logp: 1.507, alpha: 0.044
	batch_reward: 3.129, batch_reward_max: 5.087, batch_reward_min: 0.475

2023-03-10 17:16:35 - 
[#Step 960000] eval_reward: 3280.139, eval_step: 1000, eval_time: 3, time: 21.667
	actor_loss: -305.023, critic_loss: 9.166, alpha_loss: -0.022
	q1: 303.656, target_q: 303.867, logp: 2.003, alpha: 0.044
	batch_reward: 3.152, batch_reward_max: 4.957, batch_reward_min: 0.599

2023-03-10 17:16:44 - 
[#Step 965000] eval_reward: 3257.910, eval_step: 1000, eval_time: 3, time: 21.817
	actor_loss: -304.644, critic_loss: 1.412, alpha_loss: 0.005
	q1: 304.601, target_q: 304.710, logp: 1.388, alpha: 0.045
	batch_reward: 3.250, batch_reward_max: 5.364, batch_reward_min: 0.607

2023-03-10 17:16:53 - 
[#Step 970000] eval_reward: 3071.973, eval_step: 929, eval_time: 3, time: 21.964
	actor_loss: -300.217, critic_loss: 2.032, alpha_loss: -0.004
	q1: 300.349, target_q: 300.267, logp: 1.595, alpha: 0.045
	batch_reward: 3.278, batch_reward_max: 5.251, batch_reward_min: 0.723

2023-03-10 17:17:02 - 
[#Step 975000] eval_reward: 3048.098, eval_step: 922, eval_time: 3, time: 22.107
	actor_loss: -301.220, critic_loss: 2.816, alpha_loss: -0.011
	q1: 301.241, target_q: 300.982, logp: 1.745, alpha: 0.044
	batch_reward: 3.163, batch_reward_max: 5.224, batch_reward_min: 0.673

2023-03-10 17:17:09 - 
[#Step 980000] eval_reward: 1850.391, eval_step: 545, eval_time: 2, time: 22.231
	actor_loss: -304.139, critic_loss: 1.914, alpha_loss: 0.010
	q1: 304.117, target_q: 304.257, logp: 1.293, alpha: 0.046
	batch_reward: 3.158, batch_reward_max: 5.792, batch_reward_min: 0.141

2023-03-10 17:17:17 - 
[#Step 985000] eval_reward: 2422.164, eval_step: 707, eval_time: 3, time: 22.363
	actor_loss: -300.193, critic_loss: 2.418, alpha_loss: 0.005
	q1: 300.352, target_q: 300.054, logp: 1.381, alpha: 0.044
	batch_reward: 3.166, batch_reward_max: 5.282, batch_reward_min: 0.507

2023-03-10 17:17:25 - 
[#Step 990000] eval_reward: 1913.480, eval_step: 554, eval_time: 2, time: 22.485
	actor_loss: -303.616, critic_loss: 2.007, alpha_loss: 0.009
	q1: 303.696, target_q: 303.687, logp: 1.305, alpha: 0.045
	batch_reward: 3.097, batch_reward_max: 5.936, batch_reward_min: 0.467

2023-03-10 17:17:32 - 
[#Step 995000] eval_reward: 2085.908, eval_step: 601, eval_time: 2, time: 22.610
	actor_loss: -305.277, critic_loss: 3.055, alpha_loss: -0.013
	q1: 304.911, target_q: 304.481, logp: 1.772, alpha: 0.046
	batch_reward: 3.119, batch_reward_max: 4.742, batch_reward_min: 0.158

2023-03-10 17:17:40 - 
[#Step 1000000] eval_reward: 2874.275, eval_step: 858, eval_time: 3, time: 22.750
	actor_loss: -296.619, critic_loss: 1.933, alpha_loss: -0.014
	q1: 296.391, target_q: 296.788, logp: 1.806, alpha: 0.044
	batch_reward: 3.218, batch_reward_max: 5.794, batch_reward_min: 0.678

2023-03-10 17:17:40 - Saving checkpoint at step: 5
2023-03-10 17:17:40 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/actor_5
2023-03-10 17:17:40 - Saving checkpoint at step: 5
2023-03-10 17:17:40 - Saved checkpoint at saved_models/hopper-v2/sac_s2_20230310_165455/critic_5
