2023-03-10 21:01:46 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: Ant-v2
eval_episodes: 10
eval_freq: 5000
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: orthogonal
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
seed: 4
start_timesteps: 10000
tau: 0.005

2023-03-10 21:01:56 - 
[#Step 10000] eval_reward: -195.507, eval_time: 1

2023-03-10 21:02:14 - 
[#Step 20000] eval_reward: -30.029, eval_step: 325, eval_time: 2, time: 0.469
	actor_loss: -40.509, critic_loss: 8.972, alpha_loss: 0.620
	q1: 40.045, target_q: 40.270, logp: -2.108, alpha: 0.102
	batch_reward: -0.382, batch_reward_max: 2.082, batch_reward_min: -2.432

2023-03-10 21:02:32 - 
[#Step 30000] eval_reward: 547.515, eval_step: 966, eval_time: 5, time: 0.771
	actor_loss: -30.660, critic_loss: 2.844, alpha_loss: 0.004
	q1: 30.243, target_q: 30.022, logp: 3.814, alpha: 0.022
	batch_reward: -0.202, batch_reward_max: 1.799, batch_reward_min: -3.037

2023-03-10 21:02:50 - 
[#Step 40000] eval_reward: 531.552, eval_step: 882, eval_time: 5, time: 1.067
	actor_loss: -32.299, critic_loss: 2.908, alpha_loss: -0.007
	q1: 31.854, target_q: 31.876, logp: 4.308, alpha: 0.024
	batch_reward: -0.001, batch_reward_max: 2.154, batch_reward_min: -2.481

2023-03-10 21:03:06 - 
[#Step 50000] eval_reward: 492.042, eval_step: 607, eval_time: 3, time: 1.341
	actor_loss: -34.327, critic_loss: 2.024, alpha_loss: -0.006
	q1: 34.162, target_q: 34.136, logp: 4.216, alpha: 0.026
	batch_reward: 0.168, batch_reward_max: 1.963, batch_reward_min: -2.068

2023-03-10 21:03:23 - 
[#Step 60000] eval_reward: 447.033, eval_step: 572, eval_time: 3, time: 1.616
	actor_loss: -39.862, critic_loss: 2.707, alpha_loss: -0.005
	q1: 39.592, target_q: 39.736, logp: 4.177, alpha: 0.030
	batch_reward: 0.296, batch_reward_max: 2.477, batch_reward_min: -2.520

2023-03-10 21:03:39 - 
[#Step 70000] eval_reward: 583.924, eval_step: 628, eval_time: 3, time: 1.894
	actor_loss: -45.008, critic_loss: 3.075, alpha_loss: -0.003
	q1: 44.705, target_q: 44.800, logp: 4.101, alpha: 0.031
	batch_reward: 0.324, batch_reward_max: 2.709, batch_reward_min: -2.381

2023-03-10 21:03:55 - 
[#Step 80000] eval_reward: 495.186, eval_step: 516, eval_time: 3, time: 2.164
	actor_loss: -50.214, critic_loss: 2.725, alpha_loss: 0.010
	q1: 50.018, target_q: 50.113, logp: 3.711, alpha: 0.034
	batch_reward: 0.498, batch_reward_max: 2.620, batch_reward_min: -1.780

2023-03-10 21:04:12 - 
[#Step 90000] eval_reward: 694.870, eval_step: 630, eval_time: 3, time: 2.443
	actor_loss: -52.799, critic_loss: 3.805, alpha_loss: 0.006
	q1: 52.492, target_q: 52.483, logp: 3.831, alpha: 0.035
	batch_reward: 0.424, batch_reward_max: 2.588, batch_reward_min: -3.120

2023-03-10 21:04:30 - 
[#Step 100000] eval_reward: 863.502, eval_step: 869, eval_time: 4, time: 2.738
	actor_loss: -57.451, critic_loss: 3.491, alpha_loss: -0.001
	q1: 56.987, target_q: 57.068, logp: 4.014, alpha: 0.037
	batch_reward: 0.540, batch_reward_max: 2.619, batch_reward_min: -1.559

2023-03-10 21:04:48 - 
[#Step 110000] eval_reward: 818.132, eval_step: 840, eval_time: 4, time: 3.034
	actor_loss: -60.968, critic_loss: 4.390, alpha_loss: -0.011
	q1: 60.856, target_q: 60.952, logp: 4.288, alpha: 0.039
	batch_reward: 0.664, batch_reward_max: 3.050, batch_reward_min: -1.636

2023-03-10 21:05:03 - 
[#Step 120000] eval_reward: 517.029, eval_step: 391, eval_time: 2, time: 3.292
	actor_loss: -66.273, critic_loss: 3.522, alpha_loss: 0.001
	q1: 66.144, target_q: 66.033, logp: 3.966, alpha: 0.040
	batch_reward: 0.670, batch_reward_max: 3.302, batch_reward_min: -1.399

2023-03-10 21:05:22 - 
[#Step 130000] eval_reward: 1552.992, eval_step: 1000, eval_time: 5, time: 3.606
	actor_loss: -71.630, critic_loss: 5.722, alpha_loss: -0.009
	q1: 71.404, target_q: 71.147, logp: 4.205, alpha: 0.042
	batch_reward: 0.680, batch_reward_max: 2.925, batch_reward_min: -1.878

2023-03-10 21:05:38 - 
[#Step 140000] eval_reward: 850.000, eval_step: 562, eval_time: 3, time: 3.880
	actor_loss: -75.638, critic_loss: 6.935, alpha_loss: 0.006
	q1: 75.283, target_q: 75.209, logp: 3.870, alpha: 0.043
	batch_reward: 0.657, batch_reward_max: 3.595, batch_reward_min: -1.757

2023-03-10 21:05:55 - 
[#Step 150000] eval_reward: 1003.647, eval_step: 752, eval_time: 4, time: 4.164
	actor_loss: -77.518, critic_loss: 7.014, alpha_loss: 0.005
	q1: 77.090, target_q: 76.878, logp: 3.880, alpha: 0.045
	batch_reward: 0.818, batch_reward_max: 3.618, batch_reward_min: -1.807

2023-03-10 21:06:12 - 
[#Step 160000] eval_reward: 1112.730, eval_step: 556, eval_time: 3, time: 4.436
	actor_loss: -87.420, critic_loss: 6.596, alpha_loss: 0.001
	q1: 87.299, target_q: 87.439, logp: 3.986, alpha: 0.046
	batch_reward: 0.903, batch_reward_max: 3.860, batch_reward_min: -2.487

2023-03-10 21:06:29 - 
[#Step 170000] eval_reward: 1389.996, eval_step: 678, eval_time: 4, time: 4.723
	actor_loss: -92.206, critic_loss: 7.623, alpha_loss: -0.001
	q1: 92.192, target_q: 92.079, logp: 4.026, alpha: 0.048
	batch_reward: 1.005, batch_reward_max: 3.794, batch_reward_min: -1.878

2023-03-10 21:06:47 - 
[#Step 180000] eval_reward: 1600.481, eval_step: 707, eval_time: 4, time: 5.015
	actor_loss: -93.918, critic_loss: 6.594, alpha_loss: 0.002
	q1: 93.812, target_q: 93.734, logp: 3.955, alpha: 0.049
	batch_reward: 0.954, batch_reward_max: 3.781, batch_reward_min: -2.390

2023-03-10 21:07:03 - 
[#Step 190000] eval_reward: 1355.900, eval_step: 649, eval_time: 3, time: 5.296
	actor_loss: -105.165, critic_loss: 12.905, alpha_loss: -0.017
	q1: 105.040, target_q: 105.524, logp: 4.328, alpha: 0.051
	batch_reward: 1.040, batch_reward_max: 3.665, batch_reward_min: -1.698

2023-03-10 21:07:21 - 
[#Step 200000] eval_reward: 1400.360, eval_step: 842, eval_time: 4, time: 5.596
	actor_loss: -104.275, critic_loss: 10.073, alpha_loss: 0.002
	q1: 104.283, target_q: 104.550, logp: 3.961, alpha: 0.051
	batch_reward: 0.992, batch_reward_max: 4.164, batch_reward_min: -1.871

2023-03-10 21:07:21 - Saving checkpoint at step: 1
2023-03-10 21:07:21 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/actor_1
2023-03-10 21:07:21 - Saving checkpoint at step: 1
2023-03-10 21:07:21 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/critic_1
2023-03-10 21:07:39 - 
[#Step 210000] eval_reward: 1222.518, eval_step: 709, eval_time: 4, time: 5.884
	actor_loss: -117.908, critic_loss: 10.836, alpha_loss: -0.003
	q1: 117.642, target_q: 117.429, logp: 4.061, alpha: 0.053
	batch_reward: 1.185, batch_reward_max: 4.216, batch_reward_min: -1.547

2023-03-10 21:07:55 - 
[#Step 220000] eval_reward: 1405.782, eval_step: 649, eval_time: 3, time: 6.161
	actor_loss: -114.546, critic_loss: 9.261, alpha_loss: 0.014
	q1: 114.447, target_q: 115.083, logp: 3.756, alpha: 0.056
	batch_reward: 1.143, batch_reward_max: 3.952, batch_reward_min: -1.322

2023-03-10 21:08:14 - 
[#Step 230000] eval_reward: 2749.789, eval_step: 1000, eval_time: 5, time: 6.476
	actor_loss: -126.658, critic_loss: 16.529, alpha_loss: 0.000
	q1: 126.242, target_q: 126.378, logp: 3.996, alpha: 0.056
	batch_reward: 1.263, batch_reward_max: 4.304, batch_reward_min: -2.529

2023-03-10 21:08:33 - 
[#Step 240000] eval_reward: 2783.775, eval_step: 916, eval_time: 5, time: 6.782
	actor_loss: -120.121, critic_loss: 8.222, alpha_loss: 0.007
	q1: 119.775, target_q: 120.315, logp: 3.882, alpha: 0.058
	batch_reward: 1.170, batch_reward_max: 4.482, batch_reward_min: -1.979

2023-03-10 21:08:50 - 
[#Step 250000] eval_reward: 1788.135, eval_step: 720, eval_time: 4, time: 7.078
	actor_loss: -132.738, critic_loss: 17.821, alpha_loss: -0.008
	q1: 132.711, target_q: 132.672, logp: 4.136, alpha: 0.060
	batch_reward: 1.302, batch_reward_max: 4.556, batch_reward_min: -1.825

2023-03-10 21:09:08 - 
[#Step 260000] eval_reward: 2516.807, eval_step: 859, eval_time: 4, time: 7.370
	actor_loss: -140.008, critic_loss: 10.096, alpha_loss: 0.008
	q1: 139.694, target_q: 139.499, logp: 3.870, alpha: 0.063
	batch_reward: 1.501, batch_reward_max: 5.011, batch_reward_min: -1.588

2023-03-10 21:09:25 - 
[#Step 270000] eval_reward: 2119.576, eval_step: 758, eval_time: 4, time: 7.661
	actor_loss: -138.385, critic_loss: 10.908, alpha_loss: 0.014
	q1: 138.317, target_q: 138.202, logp: 3.787, alpha: 0.066
	batch_reward: 1.442, batch_reward_max: 5.319, batch_reward_min: -2.702

2023-03-10 21:09:44 - 
[#Step 280000] eval_reward: 3313.304, eval_step: 983, eval_time: 5, time: 7.971
	actor_loss: -144.481, critic_loss: 25.082, alpha_loss: 0.001
	q1: 144.024, target_q: 143.365, logp: 3.986, alpha: 0.069
	batch_reward: 1.399, batch_reward_max: 5.264, batch_reward_min: -1.355

2023-03-10 21:10:02 - 
[#Step 290000] eval_reward: 3364.462, eval_step: 1000, eval_time: 5, time: 8.278
	actor_loss: -147.436, critic_loss: 17.571, alpha_loss: 0.022
	q1: 147.370, target_q: 146.997, logp: 3.692, alpha: 0.071
	batch_reward: 1.468, batch_reward_max: 5.177, batch_reward_min: -1.299

2023-03-10 21:10:19 - 
[#Step 300000] eval_reward: 2087.568, eval_step: 680, eval_time: 3, time: 8.562
	actor_loss: -161.576, critic_loss: 24.320, alpha_loss: 0.013
	q1: 161.802, target_q: 160.365, logp: 3.832, alpha: 0.074
	batch_reward: 1.573, batch_reward_max: 4.931, batch_reward_min: -1.748

2023-03-10 21:10:37 - 
[#Step 310000] eval_reward: 2731.117, eval_step: 798, eval_time: 4, time: 8.855
	actor_loss: -166.597, critic_loss: 29.046, alpha_loss: 0.015
	q1: 166.856, target_q: 166.427, logp: 3.812, alpha: 0.079
	batch_reward: 1.675, batch_reward_max: 5.616, batch_reward_min: -2.269

2023-03-10 21:10:55 - 
[#Step 320000] eval_reward: 2877.980, eval_step: 822, eval_time: 4, time: 9.151
	actor_loss: -171.336, critic_loss: 28.468, alpha_loss: 0.026
	q1: 171.023, target_q: 169.898, logp: 3.672, alpha: 0.079
	batch_reward: 1.633, batch_reward_max: 5.559, batch_reward_min: -1.220

2023-03-10 21:11:13 - 
[#Step 330000] eval_reward: 3436.921, eval_step: 852, eval_time: 4, time: 9.448
	actor_loss: -186.660, critic_loss: 17.385, alpha_loss: -0.007
	q1: 186.501, target_q: 186.184, logp: 4.079, alpha: 0.084
	batch_reward: 1.844, batch_reward_max: 5.990, batch_reward_min: -1.578

2023-03-10 21:11:30 - 
[#Step 340000] eval_reward: 3630.083, eval_step: 873, eval_time: 4, time: 9.745
	actor_loss: -187.066, critic_loss: 35.632, alpha_loss: 0.006
	q1: 186.887, target_q: 187.022, logp: 3.925, alpha: 0.086
	batch_reward: 1.874, batch_reward_max: 5.462, batch_reward_min: -0.789

2023-03-10 21:11:49 - 
[#Step 350000] eval_reward: 4052.598, eval_step: 1000, eval_time: 5, time: 10.060
	actor_loss: -188.489, critic_loss: 34.991, alpha_loss: 0.012
	q1: 188.351, target_q: 188.873, logp: 3.869, alpha: 0.089
	batch_reward: 1.897, batch_reward_max: 5.867, batch_reward_min: -0.619

2023-03-10 21:12:06 - 
[#Step 360000] eval_reward: 2335.199, eval_step: 605, eval_time: 3, time: 10.336
	actor_loss: -208.646, critic_loss: 26.514, alpha_loss: -0.036
	q1: 208.692, target_q: 208.450, logp: 4.380, alpha: 0.095
	batch_reward: 2.048, batch_reward_max: 5.733, batch_reward_min: -1.984

2023-03-10 21:12:24 - 
[#Step 370000] eval_reward: 3670.464, eval_step: 918, eval_time: 5, time: 10.636
	actor_loss: -201.252, critic_loss: 29.795, alpha_loss: 0.006
	q1: 201.021, target_q: 201.009, logp: 3.942, alpha: 0.096
	batch_reward: 1.943, batch_reward_max: 5.734, batch_reward_min: -2.381

2023-03-10 21:12:42 - 
[#Step 380000] eval_reward: 4197.510, eval_step: 1000, eval_time: 5, time: 10.942
	actor_loss: -210.223, critic_loss: 47.148, alpha_loss: 0.023
	q1: 209.641, target_q: 209.884, logp: 3.769, alpha: 0.098
	batch_reward: 1.980, batch_reward_max: 5.647, batch_reward_min: -1.762

2023-03-10 21:12:59 - 
[#Step 390000] eval_reward: 3053.007, eval_step: 699, eval_time: 4, time: 11.230
	actor_loss: -219.712, critic_loss: 33.270, alpha_loss: 0.034
	q1: 219.443, target_q: 220.012, logp: 3.662, alpha: 0.100
	batch_reward: 2.025, batch_reward_max: 5.824, batch_reward_min: -1.862

2023-03-10 21:13:18 - 
[#Step 400000] eval_reward: 4100.332, eval_step: 936, eval_time: 5, time: 11.532
	actor_loss: -227.067, critic_loss: 34.120, alpha_loss: -0.006
	q1: 226.514, target_q: 225.978, logp: 4.056, alpha: 0.103
	batch_reward: 2.125, batch_reward_max: 6.117, batch_reward_min: -1.296

2023-03-10 21:13:18 - Saving checkpoint at step: 2
2023-03-10 21:13:18 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/actor_2
2023-03-10 21:13:18 - Saving checkpoint at step: 2
2023-03-10 21:13:18 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/critic_2
2023-03-10 21:13:36 - 
[#Step 410000] eval_reward: 4519.627, eval_step: 1000, eval_time: 5, time: 11.838
	actor_loss: -241.850, critic_loss: 29.200, alpha_loss: 0.028
	q1: 241.598, target_q: 241.639, logp: 3.738, alpha: 0.108
	batch_reward: 2.400, batch_reward_max: 5.792, batch_reward_min: -1.388

2023-03-10 21:13:53 - 
[#Step 420000] eval_reward: 2823.909, eval_step: 730, eval_time: 4, time: 12.126
	actor_loss: -242.572, critic_loss: 43.455, alpha_loss: -0.004
	q1: 242.109, target_q: 241.380, logp: 4.035, alpha: 0.109
	batch_reward: 2.288, batch_reward_max: 5.926, batch_reward_min: -1.599

2023-03-10 21:14:11 - 
[#Step 430000] eval_reward: 4280.139, eval_step: 920, eval_time: 5, time: 12.431
	actor_loss: -243.961, critic_loss: 24.946, alpha_loss: 0.024
	q1: 244.025, target_q: 243.779, logp: 3.782, alpha: 0.111
	batch_reward: 2.408, batch_reward_max: 5.964, batch_reward_min: -1.126

2023-03-10 21:14:30 - 
[#Step 440000] eval_reward: 4531.884, eval_step: 988, eval_time: 5, time: 12.732
	actor_loss: -238.876, critic_loss: 24.759, alpha_loss: 0.012
	q1: 238.412, target_q: 239.146, logp: 3.893, alpha: 0.112
	batch_reward: 2.329, batch_reward_max: 6.084, batch_reward_min: -1.963

2023-03-10 21:14:48 - 
[#Step 450000] eval_reward: 4351.201, eval_step: 951, eval_time: 5, time: 13.036
	actor_loss: -245.856, critic_loss: 37.982, alpha_loss: -0.012
	q1: 245.696, target_q: 245.900, logp: 4.105, alpha: 0.115
	batch_reward: 2.362, batch_reward_max: 6.533, batch_reward_min: -1.713

2023-03-10 21:15:06 - 
[#Step 460000] eval_reward: 4350.750, eval_step: 904, eval_time: 5, time: 13.335
	actor_loss: -262.743, critic_loss: 41.586, alpha_loss: -0.005
	q1: 262.712, target_q: 263.223, logp: 4.039, alpha: 0.117
	batch_reward: 2.583, batch_reward_max: 6.452, batch_reward_min: -2.044

2023-03-10 21:15:23 - 
[#Step 470000] eval_reward: 3783.189, eval_step: 798, eval_time: 4, time: 13.628
	actor_loss: -257.575, critic_loss: 347.331, alpha_loss: -0.023
	q1: 257.218, target_q: 256.821, logp: 4.197, alpha: 0.118
	batch_reward: 2.462, batch_reward_max: 6.599, batch_reward_min: -1.075

2023-03-10 21:15:41 - 
[#Step 480000] eval_reward: 3691.846, eval_step: 802, eval_time: 4, time: 13.927
	actor_loss: -270.633, critic_loss: 44.038, alpha_loss: -0.016
	q1: 270.884, target_q: 271.306, logp: 4.137, alpha: 0.120
	batch_reward: 2.631, batch_reward_max: 6.135, batch_reward_min: -1.017

2023-03-10 21:16:00 - 
[#Step 490000] eval_reward: 4788.283, eval_step: 984, eval_time: 5, time: 14.235
	actor_loss: -262.214, critic_loss: 33.456, alpha_loss: -0.020
	q1: 262.030, target_q: 262.246, logp: 4.167, alpha: 0.121
	batch_reward: 2.463, batch_reward_max: 6.574, batch_reward_min: -1.830

2023-03-10 21:16:17 - 
[#Step 500000] eval_reward: 4032.856, eval_step: 838, eval_time: 4, time: 14.528
	actor_loss: -266.986, critic_loss: 40.397, alpha_loss: -0.016
	q1: 266.950, target_q: 267.012, logp: 4.136, alpha: 0.121
	batch_reward: 2.523, batch_reward_max: 5.917, batch_reward_min: -3.546

2023-03-10 21:16:35 - 
[#Step 510000] eval_reward: 4048.153, eval_step: 876, eval_time: 4, time: 14.825
	actor_loss: -295.044, critic_loss: 43.269, alpha_loss: -0.093
	q1: 294.558, target_q: 294.699, logp: 4.743, alpha: 0.125
	batch_reward: 2.719, batch_reward_max: 6.140, batch_reward_min: -2.324

2023-03-10 21:16:53 - 
[#Step 520000] eval_reward: 4438.151, eval_step: 907, eval_time: 5, time: 15.129
	actor_loss: -277.309, critic_loss: 37.689, alpha_loss: 0.008
	q1: 277.024, target_q: 276.774, logp: 3.940, alpha: 0.126
	batch_reward: 2.683, batch_reward_max: 6.504, batch_reward_min: -1.355

2023-03-10 21:17:11 - 
[#Step 530000] eval_reward: 4376.278, eval_step: 885, eval_time: 4, time: 15.426
	actor_loss: -281.913, critic_loss: 26.919, alpha_loss: -0.001
	q1: 282.104, target_q: 282.060, logp: 4.006, alpha: 0.129
	batch_reward: 2.720, batch_reward_max: 7.125, batch_reward_min: -1.154

2023-03-10 21:17:29 - 
[#Step 540000] eval_reward: 4417.906, eval_step: 887, eval_time: 4, time: 15.725
	actor_loss: -303.781, critic_loss: 65.219, alpha_loss: -0.058
	q1: 303.921, target_q: 303.466, logp: 4.446, alpha: 0.131
	batch_reward: 3.006, batch_reward_max: 6.369, batch_reward_min: -1.918

2023-03-10 21:17:47 - 
[#Step 550000] eval_reward: 4378.579, eval_step: 865, eval_time: 4, time: 16.019
	actor_loss: -284.449, critic_loss: 27.448, alpha_loss: 0.061
	q1: 284.255, target_q: 285.348, logp: 3.534, alpha: 0.130
	batch_reward: 2.709, batch_reward_max: 6.547, batch_reward_min: -1.706

2023-03-10 21:18:04 - 
[#Step 560000] eval_reward: 3708.990, eval_step: 744, eval_time: 4, time: 16.299
	actor_loss: -299.669, critic_loss: 37.167, alpha_loss: -0.061
	q1: 299.495, target_q: 299.873, logp: 4.470, alpha: 0.130
	batch_reward: 2.988, batch_reward_max: 6.894, batch_reward_min: -1.120

2023-03-10 21:18:21 - 
[#Step 570000] eval_reward: 3573.241, eval_step: 785, eval_time: 4, time: 16.586
	actor_loss: -298.143, critic_loss: 47.810, alpha_loss: -0.021
	q1: 297.702, target_q: 297.844, logp: 4.155, alpha: 0.133
	batch_reward: 2.799, batch_reward_max: 6.688, batch_reward_min: -1.726

2023-03-10 21:18:38 - 
[#Step 580000] eval_reward: 4185.996, eval_step: 835, eval_time: 4, time: 16.881
	actor_loss: -295.509, critic_loss: 59.640, alpha_loss: -0.014
	q1: 295.643, target_q: 295.659, logp: 4.109, alpha: 0.132
	batch_reward: 2.920, batch_reward_max: 6.402, batch_reward_min: -1.540

2023-03-10 21:18:56 - 
[#Step 590000] eval_reward: 3945.869, eval_step: 862, eval_time: 4, time: 17.173
	actor_loss: -312.358, critic_loss: 32.242, alpha_loss: -0.063
	q1: 312.359, target_q: 312.046, logp: 4.470, alpha: 0.134
	batch_reward: 3.161, batch_reward_max: 7.329, batch_reward_min: -0.534

2023-03-10 21:19:15 - 
[#Step 600000] eval_reward: 4952.171, eval_step: 1000, eval_time: 5, time: 17.487
	actor_loss: -306.714, critic_loss: 52.894, alpha_loss: 0.023
	q1: 306.307, target_q: 305.846, logp: 3.829, alpha: 0.136
	batch_reward: 3.063, batch_reward_max: 7.173, batch_reward_min: -0.826

2023-03-10 21:19:15 - Saving checkpoint at step: 3
2023-03-10 21:19:15 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/actor_3
2023-03-10 21:19:15 - Saving checkpoint at step: 3
2023-03-10 21:19:15 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/critic_3
2023-03-10 21:19:33 - 
[#Step 610000] eval_reward: 5123.068, eval_step: 997, eval_time: 5, time: 17.796
	actor_loss: -311.228, critic_loss: 89.752, alpha_loss: -0.004
	q1: 311.088, target_q: 310.534, logp: 4.026, alpha: 0.136
	batch_reward: 3.131, batch_reward_max: 6.603, batch_reward_min: -0.763

2023-03-10 21:19:52 - 
[#Step 620000] eval_reward: 5002.747, eval_step: 1000, eval_time: 5, time: 18.107
	actor_loss: -316.789, critic_loss: 30.231, alpha_loss: -0.015
	q1: 316.825, target_q: 316.277, logp: 4.108, alpha: 0.137
	batch_reward: 3.114, batch_reward_max: 6.670, batch_reward_min: -2.188

2023-03-10 21:20:11 - 
[#Step 630000] eval_reward: 5199.075, eval_step: 990, eval_time: 5, time: 18.416
	actor_loss: -314.246, critic_loss: 74.150, alpha_loss: -0.018
	q1: 314.708, target_q: 314.901, logp: 4.130, alpha: 0.139
	batch_reward: 3.209, batch_reward_max: 6.701, batch_reward_min: -1.907

2023-03-10 21:20:29 - 
[#Step 640000] eval_reward: 5216.656, eval_step: 1000, eval_time: 5, time: 18.721
	actor_loss: -321.897, critic_loss: 29.704, alpha_loss: -0.010
	q1: 321.866, target_q: 322.596, logp: 4.069, alpha: 0.142
	batch_reward: 3.235, batch_reward_max: 6.675, batch_reward_min: -1.345

2023-03-10 21:20:47 - 
[#Step 650000] eval_reward: 4450.083, eval_step: 929, eval_time: 5, time: 19.029
	actor_loss: -308.064, critic_loss: 29.117, alpha_loss: 0.047
	q1: 308.290, target_q: 308.612, logp: 3.667, alpha: 0.141
	batch_reward: 3.196, batch_reward_max: 6.357, batch_reward_min: -1.661

2023-03-10 21:21:05 - 
[#Step 660000] eval_reward: 4459.873, eval_step: 860, eval_time: 4, time: 19.327
	actor_loss: -313.209, critic_loss: 50.792, alpha_loss: 0.030
	q1: 313.164, target_q: 312.960, logp: 3.787, alpha: 0.141
	batch_reward: 3.081, batch_reward_max: 6.924, batch_reward_min: -1.568

2023-03-10 21:21:24 - 
[#Step 670000] eval_reward: 4569.607, eval_step: 864, eval_time: 5, time: 19.633
	actor_loss: -313.603, critic_loss: 30.875, alpha_loss: 0.056
	q1: 313.319, target_q: 313.125, logp: 3.603, alpha: 0.142
	batch_reward: 3.066, batch_reward_max: 6.579, batch_reward_min: -4.611

2023-03-10 21:21:42 - 
[#Step 680000] eval_reward: 5039.995, eval_step: 948, eval_time: 5, time: 19.947
	actor_loss: -321.594, critic_loss: 84.160, alpha_loss: 0.002
	q1: 321.645, target_q: 321.287, logp: 3.988, alpha: 0.142
	batch_reward: 3.154, batch_reward_max: 7.009, batch_reward_min: -1.292

2023-03-10 21:22:01 - 
[#Step 690000] eval_reward: 5140.910, eval_step: 981, eval_time: 5, time: 20.259
	actor_loss: -328.827, critic_loss: 37.263, alpha_loss: 0.036
	q1: 328.646, target_q: 329.534, logp: 3.737, alpha: 0.138
	batch_reward: 3.295, batch_reward_max: 6.750, batch_reward_min: -1.328

2023-03-10 21:22:20 - 
[#Step 700000] eval_reward: 5082.129, eval_step: 1000, eval_time: 5, time: 20.571
	actor_loss: -327.060, critic_loss: 40.834, alpha_loss: 0.013
	q1: 327.310, target_q: 325.906, logp: 3.910, alpha: 0.141
	batch_reward: 3.242, batch_reward_max: 6.819, batch_reward_min: -2.309

2023-03-10 21:22:37 - 
[#Step 710000] eval_reward: 4158.151, eval_step: 807, eval_time: 4, time: 20.862
	actor_loss: -329.517, critic_loss: 24.748, alpha_loss: 0.067
	q1: 329.788, target_q: 329.767, logp: 3.538, alpha: 0.144
	batch_reward: 3.337, batch_reward_max: 6.769, batch_reward_min: -1.291

2023-03-10 21:22:55 - 
[#Step 720000] eval_reward: 4410.339, eval_step: 831, eval_time: 4, time: 21.155
	actor_loss: -329.570, critic_loss: 34.217, alpha_loss: 0.033
	q1: 329.425, target_q: 329.013, logp: 3.771, alpha: 0.146
	batch_reward: 3.240, batch_reward_max: 6.868, batch_reward_min: -1.387

2023-03-10 21:23:13 - 
[#Step 730000] eval_reward: 4940.434, eval_step: 910, eval_time: 4, time: 21.451
	actor_loss: -339.945, critic_loss: 36.904, alpha_loss: -0.015
	q1: 339.642, target_q: 339.883, logp: 4.105, alpha: 0.146
	batch_reward: 3.487, batch_reward_max: 6.727, batch_reward_min: -1.865

2023-03-10 21:23:31 - 
[#Step 740000] eval_reward: 5417.505, eval_step: 1000, eval_time: 5, time: 21.755
	actor_loss: -350.133, critic_loss: 38.964, alpha_loss: -0.005
	q1: 350.399, target_q: 349.978, logp: 4.035, alpha: 0.145
	batch_reward: 3.509, batch_reward_max: 6.521, batch_reward_min: -1.192

2023-03-10 21:23:49 - 
[#Step 750000] eval_reward: 5389.904, eval_step: 1000, eval_time: 5, time: 22.064
	actor_loss: -360.891, critic_loss: 40.676, alpha_loss: -0.008
	q1: 360.557, target_q: 361.105, logp: 4.054, alpha: 0.149
	batch_reward: 3.745, batch_reward_max: 6.767, batch_reward_min: -1.492

2023-03-10 21:24:08 - 
[#Step 760000] eval_reward: 5424.666, eval_step: 1000, eval_time: 5, time: 22.365
	actor_loss: -345.298, critic_loss: 39.597, alpha_loss: -0.069
	q1: 345.228, target_q: 345.578, logp: 4.464, alpha: 0.150
	batch_reward: 3.384, batch_reward_max: 7.020, batch_reward_min: -1.209

2023-03-10 21:24:26 - 
[#Step 770000] eval_reward: 5149.804, eval_step: 959, eval_time: 5, time: 22.667
	actor_loss: -344.141, critic_loss: 43.557, alpha_loss: -0.039
	q1: 344.245, target_q: 344.255, logp: 4.248, alpha: 0.159
	batch_reward: 3.436, batch_reward_max: 6.951, batch_reward_min: -2.101

2023-03-10 21:24:44 - 
[#Step 780000] eval_reward: 5250.380, eval_step: 964, eval_time: 5, time: 22.979
	actor_loss: -365.297, critic_loss: 47.287, alpha_loss: -0.052
	q1: 365.528, target_q: 365.837, logp: 4.322, alpha: 0.162
	batch_reward: 3.780, batch_reward_max: 7.079, batch_reward_min: -1.165

2023-03-10 21:25:02 - 
[#Step 790000] eval_reward: 4842.159, eval_step: 901, eval_time: 4, time: 23.277
	actor_loss: -362.504, critic_loss: 81.222, alpha_loss: -0.049
	q1: 362.234, target_q: 361.703, logp: 4.292, alpha: 0.169
	batch_reward: 3.424, batch_reward_max: 6.632, batch_reward_min: -2.894

2023-03-10 21:25:20 - 
[#Step 800000] eval_reward: 4478.077, eval_step: 972, eval_time: 5, time: 23.579
	actor_loss: -378.599, critic_loss: 63.537, alpha_loss: -0.005
	q1: 378.812, target_q: 378.303, logp: 4.029, alpha: 0.158
	batch_reward: 3.644, batch_reward_max: 6.962, batch_reward_min: -1.530

2023-03-10 21:25:20 - Saving checkpoint at step: 4
2023-03-10 21:25:20 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/actor_4
2023-03-10 21:25:20 - Saving checkpoint at step: 4
2023-03-10 21:25:20 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/critic_4
2023-03-10 21:25:38 - 
[#Step 810000] eval_reward: 4604.055, eval_step: 844, eval_time: 4, time: 23.869
	actor_loss: -364.607, critic_loss: 55.364, alpha_loss: 0.007
	q1: 364.612, target_q: 364.139, logp: 3.953, alpha: 0.151
	batch_reward: 3.570, batch_reward_max: 6.992, batch_reward_min: -1.313

2023-03-10 21:25:56 - 
[#Step 820000] eval_reward: 5186.901, eval_step: 1000, eval_time: 5, time: 24.177
	actor_loss: -370.762, critic_loss: 62.987, alpha_loss: -0.047
	q1: 370.642, target_q: 371.011, logp: 4.302, alpha: 0.157
	batch_reward: 3.705, batch_reward_max: 7.292, batch_reward_min: -1.460

2023-03-10 21:26:14 - 
[#Step 830000] eval_reward: 4816.866, eval_step: 888, eval_time: 5, time: 24.476
	actor_loss: -361.466, critic_loss: 38.912, alpha_loss: 0.020
	q1: 360.822, target_q: 361.183, logp: 3.867, alpha: 0.151
	batch_reward: 3.408, batch_reward_max: 7.014, batch_reward_min: -1.215

2023-03-10 21:26:33 - 
[#Step 840000] eval_reward: 5471.561, eval_step: 1000, eval_time: 5, time: 24.784
	actor_loss: -362.723, critic_loss: 34.856, alpha_loss: 0.029
	q1: 362.772, target_q: 362.547, logp: 3.802, alpha: 0.149
	batch_reward: 3.774, batch_reward_max: 6.815, batch_reward_min: -1.778

2023-03-10 21:26:51 - 
[#Step 850000] eval_reward: 5525.659, eval_step: 1000, eval_time: 5, time: 25.093
	actor_loss: -358.337, critic_loss: 56.214, alpha_loss: 0.015
	q1: 357.853, target_q: 357.457, logp: 3.898, alpha: 0.148
	batch_reward: 3.447, batch_reward_max: 7.654, batch_reward_min: -1.611

2023-03-10 21:27:09 - 
[#Step 860000] eval_reward: 4844.677, eval_step: 892, eval_time: 4, time: 25.389
	actor_loss: -371.104, critic_loss: 41.237, alpha_loss: -0.011
	q1: 371.126, target_q: 370.820, logp: 4.075, alpha: 0.149
	batch_reward: 3.705, batch_reward_max: 6.991, batch_reward_min: -0.471

2023-03-10 21:27:27 - 
[#Step 870000] eval_reward: 5523.960, eval_step: 1000, eval_time: 5, time: 25.694
	actor_loss: -373.733, critic_loss: 42.937, alpha_loss: -0.022
	q1: 374.378, target_q: 374.139, logp: 4.147, alpha: 0.151
	batch_reward: 3.825, batch_reward_max: 7.193, batch_reward_min: -1.919

2023-03-10 21:27:45 - 
[#Step 880000] eval_reward: 5151.286, eval_step: 939, eval_time: 5, time: 25.997
	actor_loss: -384.846, critic_loss: 45.170, alpha_loss: 0.042
	q1: 385.087, target_q: 385.868, logp: 3.721, alpha: 0.150
	batch_reward: 3.999, batch_reward_max: 6.846, batch_reward_min: -1.590

2023-03-10 21:28:04 - 
[#Step 890000] eval_reward: 5011.339, eval_step: 927, eval_time: 5, time: 26.303
	actor_loss: -362.778, critic_loss: 40.229, alpha_loss: 0.036
	q1: 362.405, target_q: 363.080, logp: 3.761, alpha: 0.152
	batch_reward: 3.522, batch_reward_max: 6.708, batch_reward_min: -3.546

2023-03-10 21:28:22 - 
[#Step 900000] eval_reward: 5544.536, eval_step: 1000, eval_time: 5, time: 26.604
	actor_loss: -369.639, critic_loss: 38.766, alpha_loss: 0.016
	q1: 369.488, target_q: 370.386, logp: 3.896, alpha: 0.150
	batch_reward: 3.631, batch_reward_max: 7.187, batch_reward_min: -0.764

2023-03-10 21:28:40 - 
[#Step 910000] eval_reward: 5566.960, eval_step: 1000, eval_time: 5, time: 26.910
	actor_loss: -373.761, critic_loss: 43.159, alpha_loss: 0.051
	q1: 373.823, target_q: 374.866, logp: 3.665, alpha: 0.152
	batch_reward: 3.763, batch_reward_max: 6.845, batch_reward_min: -1.183

2023-03-10 21:28:59 - 
[#Step 920000] eval_reward: 5406.845, eval_step: 973, eval_time: 5, time: 27.216
	actor_loss: -368.487, critic_loss: 39.417, alpha_loss: -0.015
	q1: 368.238, target_q: 367.628, logp: 4.098, alpha: 0.150
	batch_reward: 3.684, batch_reward_max: 7.144, batch_reward_min: -2.529

2023-03-10 21:29:17 - 
[#Step 930000] eval_reward: 4812.947, eval_step: 945, eval_time: 5, time: 27.519
	actor_loss: -380.099, critic_loss: 39.640, alpha_loss: 0.000
	q1: 380.225, target_q: 379.618, logp: 3.997, alpha: 0.151
	batch_reward: 3.745, batch_reward_max: 7.047, batch_reward_min: -1.791

2023-03-10 21:29:35 - 
[#Step 940000] eval_reward: 5566.928, eval_step: 1000, eval_time: 5, time: 27.822
	actor_loss: -384.247, critic_loss: 42.791, alpha_loss: -0.028
	q1: 384.409, target_q: 385.165, logp: 4.183, alpha: 0.155
	batch_reward: 3.898, batch_reward_max: 7.126, batch_reward_min: -1.581

2023-03-10 21:29:53 - 
[#Step 950000] eval_reward: 4847.738, eval_step: 926, eval_time: 5, time: 28.123
	actor_loss: -388.180, critic_loss: 48.950, alpha_loss: -0.012
	q1: 387.918, target_q: 386.913, logp: 4.080, alpha: 0.151
	batch_reward: 3.881, batch_reward_max: 7.416, batch_reward_min: -1.108

2023-03-10 21:30:04 - 
[#Step 955000] eval_reward: 4303.369, eval_step: 912, eval_time: 5, time: 28.312
	actor_loss: -394.403, critic_loss: 33.269, alpha_loss: -0.100
	q1: 394.267, target_q: 394.822, logp: 4.651, alpha: 0.154
	batch_reward: 3.996, batch_reward_max: 6.941, batch_reward_min: -0.824

2023-03-10 21:30:16 - 
[#Step 960000] eval_reward: 5607.233, eval_step: 1000, eval_time: 5, time: 28.505
	actor_loss: -391.493, critic_loss: 46.301, alpha_loss: -0.103
	q1: 390.652, target_q: 390.536, logp: 4.680, alpha: 0.151
	batch_reward: 3.900, batch_reward_max: 7.091, batch_reward_min: -1.760

2023-03-10 21:30:28 - 
[#Step 965000] eval_reward: 5643.755, eval_step: 1000, eval_time: 5, time: 28.698
	actor_loss: -385.188, critic_loss: 74.151, alpha_loss: 0.007
	q1: 385.619, target_q: 386.226, logp: 3.954, alpha: 0.151
	batch_reward: 3.874, batch_reward_max: 7.255, batch_reward_min: -1.393

2023-03-10 21:30:39 - 
[#Step 970000] eval_reward: 5569.712, eval_step: 1000, eval_time: 5, time: 28.889
	actor_loss: -372.398, critic_loss: 32.010, alpha_loss: 0.064
	q1: 372.411, target_q: 371.726, logp: 3.586, alpha: 0.154
	batch_reward: 3.683, batch_reward_max: 6.956, batch_reward_min: -1.584

2023-03-10 21:30:50 - 
[#Step 975000] eval_reward: 5084.454, eval_step: 913, eval_time: 5, time: 29.076
	actor_loss: -383.191, critic_loss: 87.572, alpha_loss: 0.007
	q1: 383.023, target_q: 384.199, logp: 3.956, alpha: 0.151
	batch_reward: 3.732, batch_reward_max: 6.998, batch_reward_min: -3.503

2023-03-10 21:31:02 - 
[#Step 980000] eval_reward: 5677.344, eval_step: 1000, eval_time: 5, time: 29.270
	actor_loss: -398.094, critic_loss: 98.814, alpha_loss: -0.032
	q1: 397.805, target_q: 398.073, logp: 4.207, alpha: 0.152
	batch_reward: 4.019, batch_reward_max: 7.149, batch_reward_min: -2.043

2023-03-10 21:31:12 - 
[#Step 985000] eval_reward: 4303.223, eval_step: 768, eval_time: 4, time: 29.445
	actor_loss: -392.565, critic_loss: 25.938, alpha_loss: -0.074
	q1: 392.750, target_q: 393.261, logp: 4.485, alpha: 0.152
	batch_reward: 3.903, batch_reward_max: 7.428, batch_reward_min: -1.180

2023-03-10 21:31:24 - 
[#Step 990000] eval_reward: 5639.366, eval_step: 1000, eval_time: 5, time: 29.642
	actor_loss: -393.474, critic_loss: 60.447, alpha_loss: -0.041
	q1: 393.131, target_q: 393.631, logp: 4.271, alpha: 0.150
	batch_reward: 3.990, batch_reward_max: 6.930, batch_reward_min: -0.926

2023-03-10 21:31:36 - 
[#Step 995000] eval_reward: 5235.045, eval_step: 912, eval_time: 5, time: 29.832
	actor_loss: -392.162, critic_loss: 50.151, alpha_loss: -0.039
	q1: 392.484, target_q: 392.472, logp: 4.262, alpha: 0.151
	batch_reward: 3.950, batch_reward_max: 7.299, batch_reward_min: -1.540

2023-03-10 21:31:47 - 
[#Step 1000000] eval_reward: 5543.424, eval_step: 1000, eval_time: 5, time: 30.023
	actor_loss: -377.489, critic_loss: 45.568, alpha_loss: -0.053
	q1: 377.523, target_q: 376.758, logp: 4.355, alpha: 0.149
	batch_reward: 3.895, batch_reward_max: 6.673, batch_reward_min: -0.663

2023-03-10 21:31:47 - Saving checkpoint at step: 5
2023-03-10 21:31:47 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/actor_5
2023-03-10 21:31:47 - Saving checkpoint at step: 5
2023-03-10 21:31:47 - Saved checkpoint at saved_models/ant-v2/sac_s4_20230310_210146/critic_5
