2023-04-03 10:42:11 - Exp configurations:
batch_size: 256
ckpt_freq: 200000
env_name: Walker2d-v3
eval_episodes: 10
eval_freq: 5000
expl_noise: 0.1
gamma: 0.99
hidden_dims: !!python/tuple
- 256
- 256
initializer: glorot_uniform
log_dir: logs
lr: 0.0003
max_timesteps: 1000000
model_dir: saved_models
noise_clip: 0.5
policy_freq: 2
policy_noise: 0.2
seed: 0
start_timesteps: 25000
tau: 0.005

2023-04-03 10:42:16 - 
[#Step 10000] eval_reward: -13.21, eval_step: 27, eval_time: 0

2023-04-03 10:42:17 - 
[#Step 20000] eval_reward: -13.19, eval_step: 27, eval_time: 0

2023-04-03 10:42:25 - 
[#Step 30000] eval_reward: 166.98, eval_step: 282, eval_time: 1, time: 0.23
	critic_loss: 3.84, actor_loss: -14.87
	q1: 12.46, max_q1: 44.73, min_q1: -5.20
	batch_reward: 0.14, batch_reward_max: 4.20, batch_reward_min: -2.09

2023-04-03 10:42:36 - 
[#Step 40000] eval_reward: 167.69, eval_step: 240, eval_time: 1, time: 0.41
	critic_loss: 15.83, actor_loss: -46.45
	q1: 43.48, max_q1: 81.76, min_q1: -13.13
	batch_reward: 0.34, batch_reward_max: 6.18, batch_reward_min: -1.75

2023-04-03 10:42:45 - 
[#Step 50000] eval_reward: 333.58, eval_step: 163, eval_time: 0, time: 0.57
	critic_loss: 24.78, actor_loss: -61.71
	q1: 57.58, max_q1: 100.27, min_q1: -31.81
	batch_reward: 0.51, batch_reward_max: 4.09, batch_reward_min: -2.89

2023-04-03 10:42:55 - 
[#Step 60000] eval_reward: 332.12, eval_step: 191, eval_time: 0, time: 0.73
	critic_loss: 27.71, actor_loss: -78.72
	q1: 74.96, max_q1: 125.55, min_q1: -7.75
	batch_reward: 0.64, batch_reward_max: 5.22, batch_reward_min: -1.61

2023-04-03 10:43:04 - 
[#Step 70000] eval_reward: 382.30, eval_step: 222, eval_time: 1, time: 0.89
	critic_loss: 30.42, actor_loss: -91.37
	q1: 87.97, max_q1: 140.78, min_q1: -4.63
	batch_reward: 0.88, batch_reward_max: 4.34, batch_reward_min: -1.66

2023-04-03 10:43:14 - 
[#Step 80000] eval_reward: 228.64, eval_step: 159, eval_time: 0, time: 1.04
	critic_loss: 26.62, actor_loss: -97.10
	q1: 93.95, max_q1: 154.60, min_q1: -33.45
	batch_reward: 1.08, batch_reward_max: 5.30, batch_reward_min: -2.90

2023-04-03 10:43:23 - 
[#Step 90000] eval_reward: 258.93, eval_step: 146, eval_time: 0, time: 1.20
	critic_loss: 22.46, actor_loss: -100.15
	q1: 96.01, max_q1: 133.79, min_q1: -3.39
	batch_reward: 1.03, batch_reward_max: 5.29, batch_reward_min: -2.54

2023-04-03 10:43:33 - 
[#Step 100000] eval_reward: 351.02, eval_step: 193, eval_time: 0, time: 1.36
	critic_loss: 21.54, actor_loss: -101.31
	q1: 97.04, max_q1: 163.48, min_q1: -10.72
	batch_reward: 1.10, batch_reward_max: 4.23, batch_reward_min: -2.63

2023-04-03 10:43:43 - 
[#Step 110000] eval_reward: 334.33, eval_step: 189, eval_time: 0, time: 1.52
	critic_loss: 32.85, actor_loss: -106.12
	q1: 102.03, max_q1: 148.81, min_q1: -8.56
	batch_reward: 1.07, batch_reward_max: 4.37, batch_reward_min: -1.57

2023-04-03 10:43:53 - 
[#Step 120000] eval_reward: 624.13, eval_step: 335, eval_time: 1, time: 1.69
	critic_loss: 21.79, actor_loss: -107.01
	q1: 102.84, max_q1: 165.38, min_q1: -17.87
	batch_reward: 1.20, batch_reward_max: 4.88, batch_reward_min: -2.52

2023-04-03 10:44:02 - 
[#Step 130000] eval_reward: 364.80, eval_step: 176, eval_time: 0, time: 1.85
	critic_loss: 23.33, actor_loss: -108.62
	q1: 105.13, max_q1: 162.36, min_q1: -5.90
	batch_reward: 1.23, batch_reward_max: 5.59, batch_reward_min: -2.47

2023-04-03 10:44:12 - 
[#Step 140000] eval_reward: 597.52, eval_step: 239, eval_time: 1, time: 2.01
	critic_loss: 26.67, actor_loss: -111.40
	q1: 107.66, max_q1: 166.00, min_q1: -21.83
	batch_reward: 1.33, batch_reward_max: 4.49, batch_reward_min: -1.22

2023-04-03 10:44:21 - 
[#Step 150000] eval_reward: 356.65, eval_step: 191, eval_time: 0, time: 2.17
	critic_loss: 21.28, actor_loss: -111.99
	q1: 109.58, max_q1: 167.70, min_q1: -8.01
	batch_reward: 1.25, batch_reward_max: 4.33, batch_reward_min: -2.93

2023-04-03 10:44:31 - 
[#Step 160000] eval_reward: 458.05, eval_step: 207, eval_time: 1, time: 2.33
	critic_loss: 29.03, actor_loss: -114.33
	q1: 111.74, max_q1: 185.93, min_q1: -6.33
	batch_reward: 1.45, batch_reward_max: 4.91, batch_reward_min: -2.07

2023-04-03 10:44:41 - 
[#Step 170000] eval_reward: 553.45, eval_step: 251, eval_time: 1, time: 2.49
	critic_loss: 30.37, actor_loss: -109.55
	q1: 106.60, max_q1: 167.86, min_q1: -36.85
	batch_reward: 1.50, batch_reward_max: 6.35, batch_reward_min: -1.55

2023-04-03 10:44:50 - 
[#Step 180000] eval_reward: 383.96, eval_step: 184, eval_time: 0, time: 2.65
	critic_loss: 33.41, actor_loss: -112.25
	q1: 109.07, max_q1: 178.43, min_q1: -23.31
	batch_reward: 1.55, batch_reward_max: 5.75, batch_reward_min: -1.21

2023-04-03 10:45:00 - 
[#Step 190000] eval_reward: 790.29, eval_step: 306, eval_time: 1, time: 2.82
	critic_loss: 29.44, actor_loss: -118.76
	q1: 116.95, max_q1: 182.31, min_q1: -16.04
	batch_reward: 1.66, batch_reward_max: 5.82, batch_reward_min: -0.86

2023-04-03 10:45:10 - 
[#Step 200000] eval_reward: 470.88, eval_step: 181, eval_time: 0, time: 2.98
	critic_loss: 29.90, actor_loss: -119.57
	q1: 116.18, max_q1: 175.06, min_q1: -26.11
	batch_reward: 1.57, batch_reward_max: 5.47, batch_reward_min: -1.21

2023-04-03 10:45:10 - Saving checkpoint at step: 1
2023-04-03 10:45:10 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/actor_1
2023-04-03 10:45:10 - Saving checkpoint at step: 1
2023-04-03 10:45:10 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/critic_1
2023-04-03 10:45:19 - 
[#Step 210000] eval_reward: 466.72, eval_step: 208, eval_time: 0, time: 3.14
	critic_loss: 32.92, actor_loss: -121.69
	q1: 119.17, max_q1: 186.79, min_q1: -23.04
	batch_reward: 1.56, batch_reward_max: 5.52, batch_reward_min: -2.45

2023-04-03 10:45:29 - 
[#Step 220000] eval_reward: 571.37, eval_step: 208, eval_time: 1, time: 3.30
	critic_loss: 42.51, actor_loss: -125.26
	q1: 121.98, max_q1: 192.61, min_q1: -15.19
	batch_reward: 1.73, batch_reward_max: 5.87, batch_reward_min: -1.71

2023-04-03 10:45:39 - 
[#Step 230000] eval_reward: 1277.84, eval_step: 454, eval_time: 1, time: 3.47
	critic_loss: 33.80, actor_loss: -132.04
	q1: 129.00, max_q1: 207.78, min_q1: -14.38
	batch_reward: 1.95, batch_reward_max: 6.24, batch_reward_min: -2.24

2023-04-03 10:45:49 - 
[#Step 240000] eval_reward: 1058.46, eval_step: 345, eval_time: 1, time: 3.63
	critic_loss: 38.68, actor_loss: -134.89
	q1: 131.97, max_q1: 213.90, min_q1: -20.48
	batch_reward: 1.82, batch_reward_max: 6.17, batch_reward_min: -1.44

2023-04-03 10:45:59 - 
[#Step 250000] eval_reward: 867.37, eval_step: 287, eval_time: 1, time: 3.80
	critic_loss: 22.56, actor_loss: -141.42
	q1: 138.58, max_q1: 213.98, min_q1: -21.83
	batch_reward: 1.95, batch_reward_max: 6.10, batch_reward_min: -0.83

2023-04-03 10:46:09 - 
[#Step 260000] eval_reward: 1454.78, eval_step: 514, eval_time: 1, time: 3.97
	critic_loss: 35.50, actor_loss: -144.67
	q1: 140.81, max_q1: 221.96, min_q1: -6.18
	batch_reward: 1.84, batch_reward_max: 6.45, batch_reward_min: -1.24

2023-04-03 10:46:19 - 
[#Step 270000] eval_reward: 1407.72, eval_step: 476, eval_time: 1, time: 4.13
	critic_loss: 36.10, actor_loss: -147.28
	q1: 144.49, max_q1: 219.39, min_q1: -18.20
	batch_reward: 2.00, batch_reward_max: 6.01, batch_reward_min: -1.99

2023-04-03 10:46:29 - 
[#Step 280000] eval_reward: 1072.50, eval_step: 372, eval_time: 1, time: 4.30
	critic_loss: 24.79, actor_loss: -152.05
	q1: 149.18, max_q1: 228.75, min_q1: -2.01
	batch_reward: 1.93, batch_reward_max: 6.24, batch_reward_min: -1.68

2023-04-03 10:46:40 - 
[#Step 290000] eval_reward: 1576.32, eval_step: 545, eval_time: 1, time: 4.47
	critic_loss: 27.58, actor_loss: -155.02
	q1: 151.98, max_q1: 231.80, min_q1: -9.19
	batch_reward: 1.99, batch_reward_max: 5.99, batch_reward_min: -1.61

2023-04-03 10:46:51 - 
[#Step 300000] eval_reward: 2338.34, eval_step: 867, eval_time: 2, time: 4.66
	critic_loss: 29.88, actor_loss: -154.52
	q1: 151.64, max_q1: 240.65, min_q1: -7.99
	batch_reward: 2.07, batch_reward_max: 5.82, batch_reward_min: -1.81

2023-04-03 10:47:02 - 
[#Step 310000] eval_reward: 2516.65, eval_step: 926, eval_time: 2, time: 4.85
	critic_loss: 35.33, actor_loss: -157.24
	q1: 154.96, max_q1: 250.02, min_q1: -3.82
	batch_reward: 2.13, batch_reward_max: 5.53, batch_reward_min: -2.00

2023-04-03 10:47:13 - 
[#Step 320000] eval_reward: 1463.74, eval_step: 521, eval_time: 1, time: 5.02
	critic_loss: 38.79, actor_loss: -162.76
	q1: 159.62, max_q1: 244.24, min_q1: -7.91
	batch_reward: 2.02, batch_reward_max: 5.45, batch_reward_min: -2.31

2023-04-03 10:47:24 - 
[#Step 330000] eval_reward: 2355.09, eval_step: 800, eval_time: 2, time: 5.21
	critic_loss: 26.20, actor_loss: -164.08
	q1: 161.99, max_q1: 237.27, min_q1: 3.85
	batch_reward: 2.23, batch_reward_max: 6.07, batch_reward_min: -0.60

2023-04-03 10:47:35 - 
[#Step 340000] eval_reward: 2031.74, eval_step: 700, eval_time: 2, time: 5.39
	critic_loss: 41.84, actor_loss: -166.67
	q1: 163.65, max_q1: 243.16, min_q1: -7.35
	batch_reward: 2.03, batch_reward_max: 8.04, batch_reward_min: -0.81

2023-04-03 10:47:46 - 
[#Step 350000] eval_reward: 2594.94, eval_step: 893, eval_time: 2, time: 5.58
	critic_loss: 45.78, actor_loss: -175.11
	q1: 171.80, max_q1: 256.53, min_q1: -10.74
	batch_reward: 2.20, batch_reward_max: 6.30, batch_reward_min: -1.71

2023-04-03 10:47:57 - 
[#Step 360000] eval_reward: 2717.52, eval_step: 931, eval_time: 2, time: 5.77
	critic_loss: 32.19, actor_loss: -176.95
	q1: 173.85, max_q1: 257.52, min_q1: -53.61
	batch_reward: 2.14, batch_reward_max: 5.67, batch_reward_min: -1.28

2023-04-03 10:48:09 - 
[#Step 370000] eval_reward: 2663.11, eval_step: 908, eval_time: 2, time: 5.96
	critic_loss: 36.09, actor_loss: -172.54
	q1: 169.25, max_q1: 271.09, min_q1: -2.39
	batch_reward: 1.99, batch_reward_max: 6.33, batch_reward_min: -1.61

2023-04-03 10:48:20 - 
[#Step 380000] eval_reward: 2801.40, eval_step: 922, eval_time: 2, time: 6.15
	critic_loss: 25.69, actor_loss: -175.81
	q1: 173.33, max_q1: 257.67, min_q1: -6.14
	batch_reward: 2.29, batch_reward_max: 5.33, batch_reward_min: -0.79

2023-04-03 10:48:32 - 
[#Step 390000] eval_reward: 2989.85, eval_step: 996, eval_time: 2, time: 6.34
	critic_loss: 34.34, actor_loss: -178.91
	q1: 176.25, max_q1: 260.33, min_q1: 15.65
	batch_reward: 2.21, batch_reward_max: 6.18, batch_reward_min: -2.02

2023-04-03 10:48:42 - 
[#Step 400000] eval_reward: 1711.81, eval_step: 571, eval_time: 1, time: 6.52
	critic_loss: 29.77, actor_loss: -175.84
	q1: 173.55, max_q1: 252.30, min_q1: -3.22
	batch_reward: 2.27, batch_reward_max: 6.04, batch_reward_min: -1.99

2023-04-03 10:48:42 - Saving checkpoint at step: 2
2023-04-03 10:48:42 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/actor_2
2023-04-03 10:48:42 - Saving checkpoint at step: 2
2023-04-03 10:48:42 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/critic_2
2023-04-03 10:48:54 - 
[#Step 410000] eval_reward: 2324.27, eval_step: 777, eval_time: 2, time: 6.71
	critic_loss: 39.84, actor_loss: -181.64
	q1: 177.72, max_q1: 258.85, min_q1: -59.49
	batch_reward: 2.20, batch_reward_max: 6.38, batch_reward_min: -1.37

2023-04-03 10:49:05 - 
[#Step 420000] eval_reward: 2557.93, eval_step: 819, eval_time: 2, time: 6.89
	critic_loss: 35.54, actor_loss: -183.32
	q1: 180.11, max_q1: 256.69, min_q1: 2.62
	batch_reward: 2.24, batch_reward_max: 5.90, batch_reward_min: -0.82

2023-04-03 10:49:16 - 
[#Step 430000] eval_reward: 2371.77, eval_step: 765, eval_time: 2, time: 7.08
	critic_loss: 31.39, actor_loss: -188.10
	q1: 186.21, max_q1: 275.58, min_q1: 4.29
	batch_reward: 2.33, batch_reward_max: 5.75, batch_reward_min: -2.28

2023-04-03 10:49:27 - 
[#Step 440000] eval_reward: 2003.56, eval_step: 651, eval_time: 2, time: 7.27
	critic_loss: 26.79, actor_loss: -192.26
	q1: 189.94, max_q1: 263.42, min_q1: -6.75
	batch_reward: 2.31, batch_reward_max: 5.72, batch_reward_min: -2.23

2023-04-03 10:49:39 - 
[#Step 450000] eval_reward: 3143.66, eval_step: 1000, eval_time: 2, time: 7.46
	critic_loss: 36.07, actor_loss: -182.86
	q1: 179.72, max_q1: 261.65, min_q1: -33.31
	batch_reward: 2.39, batch_reward_max: 5.77, batch_reward_min: -2.70

2023-04-03 10:49:51 - 
[#Step 460000] eval_reward: 2908.98, eval_step: 1000, eval_time: 2, time: 7.66
	critic_loss: 31.35, actor_loss: -189.62
	q1: 187.50, max_q1: 268.80, min_q1: 1.85
	batch_reward: 2.32, batch_reward_max: 5.88, batch_reward_min: -1.30

2023-04-03 10:50:02 - 
[#Step 470000] eval_reward: 2959.65, eval_step: 1000, eval_time: 2, time: 7.85
	critic_loss: 36.64, actor_loss: -195.69
	q1: 193.17, max_q1: 270.96, min_q1: 7.60
	batch_reward: 2.37, batch_reward_max: 5.73, batch_reward_min: -0.97

2023-04-03 10:50:14 - 
[#Step 480000] eval_reward: 3173.84, eval_step: 1000, eval_time: 3, time: 8.05
	critic_loss: 25.57, actor_loss: -199.65
	q1: 197.69, max_q1: 270.81, min_q1: 13.27
	batch_reward: 2.34, batch_reward_max: 5.27, batch_reward_min: -0.48

2023-04-03 10:50:27 - 
[#Step 490000] eval_reward: 3178.09, eval_step: 1000, eval_time: 3, time: 8.26
	critic_loss: 30.08, actor_loss: -197.49
	q1: 194.92, max_q1: 272.33, min_q1: -10.48
	batch_reward: 2.27, batch_reward_max: 5.62, batch_reward_min: -1.45

2023-04-03 10:50:39 - 
[#Step 500000] eval_reward: 3072.81, eval_step: 1000, eval_time: 3, time: 8.46
	critic_loss: 28.62, actor_loss: -196.35
	q1: 194.39, max_q1: 267.02, min_q1: -8.17
	batch_reward: 2.30, batch_reward_max: 6.36, batch_reward_min: -1.33

2023-04-03 10:50:50 - 
[#Step 510000] eval_reward: 3105.51, eval_step: 1000, eval_time: 2, time: 8.65
	critic_loss: 31.39, actor_loss: -198.93
	q1: 196.58, max_q1: 282.32, min_q1: -1.05
	batch_reward: 2.51, batch_reward_max: 7.26, batch_reward_min: -0.99

2023-04-03 10:51:02 - 
[#Step 520000] eval_reward: 3087.02, eval_step: 1000, eval_time: 3, time: 8.85
	critic_loss: 41.38, actor_loss: -207.07
	q1: 204.93, max_q1: 271.69, min_q1: -1.44
	batch_reward: 2.41, batch_reward_max: 5.65, batch_reward_min: -1.42

2023-04-03 10:51:14 - 
[#Step 530000] eval_reward: 3132.91, eval_step: 1000, eval_time: 3, time: 9.04
	critic_loss: 34.50, actor_loss: -205.24
	q1: 202.66, max_q1: 285.03, min_q1: 15.73
	batch_reward: 2.48, batch_reward_max: 6.68, batch_reward_min: -1.33

2023-04-03 10:51:25 - 
[#Step 540000] eval_reward: 3193.58, eval_step: 1000, eval_time: 2, time: 9.24
	critic_loss: 21.35, actor_loss: -205.68
	q1: 203.57, max_q1: 277.27, min_q1: -14.06
	batch_reward: 2.40, batch_reward_max: 5.98, batch_reward_min: -1.65

2023-04-03 10:51:37 - 
[#Step 550000] eval_reward: 3017.89, eval_step: 1000, eval_time: 2, time: 9.43
	critic_loss: 33.44, actor_loss: -211.06
	q1: 208.28, max_q1: 282.25, min_q1: 3.45
	batch_reward: 2.42, batch_reward_max: 6.24, batch_reward_min: -1.52

2023-04-03 10:51:49 - 
[#Step 560000] eval_reward: 3049.56, eval_step: 1000, eval_time: 3, time: 9.63
	critic_loss: 33.99, actor_loss: -216.53
	q1: 214.02, max_q1: 292.88, min_q1: 37.79
	batch_reward: 2.57, batch_reward_max: 6.63, batch_reward_min: -0.56

2023-04-03 10:52:00 - 
[#Step 570000] eval_reward: 2240.36, eval_step: 741, eval_time: 2, time: 9.82
	critic_loss: 25.48, actor_loss: -210.78
	q1: 207.93, max_q1: 286.79, min_q1: -7.86
	batch_reward: 2.53, batch_reward_max: 6.24, batch_reward_min: -1.24

2023-04-03 10:52:12 - 
[#Step 580000] eval_reward: 3274.28, eval_step: 1000, eval_time: 2, time: 10.01
	critic_loss: 22.85, actor_loss: -215.49
	q1: 212.48, max_q1: 310.34, min_q1: 0.34
	batch_reward: 2.38, batch_reward_max: 5.98, batch_reward_min: -1.67

2023-04-03 10:52:23 - 
[#Step 590000] eval_reward: 3262.63, eval_step: 1000, eval_time: 2, time: 10.20
	critic_loss: 47.65, actor_loss: -215.02
	q1: 211.90, max_q1: 290.56, min_q1: -23.48
	batch_reward: 2.43, batch_reward_max: 7.09, batch_reward_min: -0.84

2023-04-03 10:52:35 - 
[#Step 600000] eval_reward: 3224.40, eval_step: 1000, eval_time: 3, time: 10.40
	critic_loss: 32.32, actor_loss: -222.31
	q1: 218.78, max_q1: 293.99, min_q1: -5.04
	batch_reward: 2.56, batch_reward_max: 5.53, batch_reward_min: -0.63

2023-04-03 10:52:35 - Saving checkpoint at step: 3
2023-04-03 10:52:35 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/actor_3
2023-04-03 10:52:35 - Saving checkpoint at step: 3
2023-04-03 10:52:35 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/critic_3
2023-04-03 10:52:47 - 
[#Step 610000] eval_reward: 3200.55, eval_step: 968, eval_time: 2, time: 10.59
	critic_loss: 47.21, actor_loss: -219.53
	q1: 216.81, max_q1: 288.91, min_q1: 13.91
	batch_reward: 2.55, batch_reward_max: 6.40, batch_reward_min: -1.37

2023-04-03 10:52:58 - 
[#Step 620000] eval_reward: 3266.11, eval_step: 1000, eval_time: 2, time: 10.78
	critic_loss: 30.28, actor_loss: -228.59
	q1: 225.91, max_q1: 296.70, min_q1: 11.91
	batch_reward: 2.56, batch_reward_max: 6.59, batch_reward_min: -1.43

2023-04-03 10:53:10 - 
[#Step 630000] eval_reward: 3265.95, eval_step: 1000, eval_time: 2, time: 10.98
	critic_loss: 46.36, actor_loss: -220.34
	q1: 217.79, max_q1: 303.54, min_q1: -21.72
	batch_reward: 2.56, batch_reward_max: 5.53, batch_reward_min: -1.12

2023-04-03 10:53:21 - 
[#Step 640000] eval_reward: 3224.16, eval_step: 1000, eval_time: 2, time: 11.17
	critic_loss: 26.17, actor_loss: -227.93
	q1: 225.71, max_q1: 306.40, min_q1: -19.23
	batch_reward: 2.54, batch_reward_max: 5.66, batch_reward_min: -1.68

2023-04-03 10:53:33 - 
[#Step 650000] eval_reward: 3282.51, eval_step: 1000, eval_time: 2, time: 11.36
	critic_loss: 23.87, actor_loss: -228.75
	q1: 226.82, max_q1: 310.38, min_q1: -6.95
	batch_reward: 2.56, batch_reward_max: 6.25, batch_reward_min: -1.11

2023-04-03 10:53:44 - 
[#Step 660000] eval_reward: 3452.60, eval_step: 1000, eval_time: 2, time: 11.55
	critic_loss: 31.13, actor_loss: -235.71
	q1: 232.58, max_q1: 305.54, min_q1: -2.99
	batch_reward: 2.64, batch_reward_max: 6.08, batch_reward_min: -0.82

2023-04-03 10:53:56 - 
[#Step 670000] eval_reward: 3265.94, eval_step: 1000, eval_time: 2, time: 11.74
	critic_loss: 22.76, actor_loss: -230.91
	q1: 228.49, max_q1: 306.90, min_q1: 15.49
	batch_reward: 2.46, batch_reward_max: 5.83, batch_reward_min: -1.12

2023-04-03 10:54:07 - 
[#Step 680000] eval_reward: 3165.69, eval_step: 1000, eval_time: 2, time: 11.93
	critic_loss: 26.58, actor_loss: -235.30
	q1: 233.21, max_q1: 321.74, min_q1: 9.95
	batch_reward: 2.52, batch_reward_max: 7.13, batch_reward_min: -0.82

2023-04-03 10:54:19 - 
[#Step 690000] eval_reward: 3238.05, eval_step: 1000, eval_time: 2, time: 12.12
	critic_loss: 23.43, actor_loss: -235.84
	q1: 233.86, max_q1: 304.73, min_q1: -0.36
	batch_reward: 2.54, batch_reward_max: 6.99, batch_reward_min: -2.01

2023-04-03 10:54:30 - 
[#Step 700000] eval_reward: 3291.45, eval_step: 1000, eval_time: 2, time: 12.32
	critic_loss: 32.40, actor_loss: -239.55
	q1: 237.49, max_q1: 308.18, min_q1: 3.71
	batch_reward: 2.75, batch_reward_max: 6.81, batch_reward_min: -1.15

2023-04-03 10:54:42 - 
[#Step 710000] eval_reward: 3184.22, eval_step: 1000, eval_time: 2, time: 12.51
	critic_loss: 39.07, actor_loss: -239.51
	q1: 236.99, max_q1: 315.15, min_q1: 2.01
	batch_reward: 2.57, batch_reward_max: 5.44, batch_reward_min: -0.91

2023-04-03 10:54:53 - 
[#Step 720000] eval_reward: 3447.91, eval_step: 1000, eval_time: 2, time: 12.70
	critic_loss: 24.89, actor_loss: -230.19
	q1: 227.69, max_q1: 323.01, min_q1: -21.58
	batch_reward: 2.60, batch_reward_max: 5.63, batch_reward_min: -1.24

2023-04-03 10:55:05 - 
[#Step 730000] eval_reward: 3294.98, eval_step: 1000, eval_time: 2, time: 12.90
	critic_loss: 29.28, actor_loss: -238.09
	q1: 235.90, max_q1: 313.97, min_q1: -8.60
	batch_reward: 2.51, batch_reward_max: 6.63, batch_reward_min: -1.09

2023-04-03 10:55:17 - 
[#Step 740000] eval_reward: 3266.71, eval_step: 1000, eval_time: 3, time: 13.10
	critic_loss: 32.54, actor_loss: -239.21
	q1: 236.88, max_q1: 322.73, min_q1: -1.78
	batch_reward: 2.67, batch_reward_max: 6.51, batch_reward_min: -0.86

2023-04-03 10:55:30 - 
[#Step 750000] eval_reward: 3248.99, eval_step: 1000, eval_time: 3, time: 13.31
	critic_loss: 27.82, actor_loss: -243.22
	q1: 240.61, max_q1: 322.28, min_q1: 15.71
	batch_reward: 2.67, batch_reward_max: 4.95, batch_reward_min: -2.50

2023-04-03 10:55:41 - 
[#Step 760000] eval_reward: 3288.35, eval_step: 1000, eval_time: 2, time: 13.50
	critic_loss: 67.38, actor_loss: -236.24
	q1: 234.34, max_q1: 319.28, min_q1: -28.26
	batch_reward: 2.66, batch_reward_max: 6.39, batch_reward_min: -1.06

2023-04-03 10:55:53 - 
[#Step 770000] eval_reward: 3246.32, eval_step: 1000, eval_time: 2, time: 13.69
	critic_loss: 39.90, actor_loss: -242.84
	q1: 240.42, max_q1: 320.17, min_q1: 1.16
	batch_reward: 2.65, batch_reward_max: 6.31, batch_reward_min: -1.03

2023-04-03 10:56:04 - 
[#Step 780000] eval_reward: 3438.35, eval_step: 1000, eval_time: 2, time: 13.88
	critic_loss: 36.92, actor_loss: -238.19
	q1: 235.28, max_q1: 320.99, min_q1: -27.41
	batch_reward: 2.60, batch_reward_max: 5.63, batch_reward_min: -0.88

2023-04-03 10:56:16 - 
[#Step 790000] eval_reward: 3303.53, eval_step: 1000, eval_time: 2, time: 14.08
	critic_loss: 37.60, actor_loss: -240.74
	q1: 237.75, max_q1: 325.26, min_q1: 5.03
	batch_reward: 2.61, batch_reward_max: 5.59, batch_reward_min: -1.19

2023-04-03 10:56:27 - 
[#Step 800000] eval_reward: 3398.08, eval_step: 1000, eval_time: 2, time: 14.27
	critic_loss: 26.59, actor_loss: -249.72
	q1: 247.41, max_q1: 332.40, min_q1: 0.52
	batch_reward: 2.77, batch_reward_max: 6.06, batch_reward_min: -1.52

2023-04-03 10:56:27 - Saving checkpoint at step: 4
2023-04-03 10:56:27 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/actor_4
2023-04-03 10:56:27 - Saving checkpoint at step: 4
2023-04-03 10:56:27 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/critic_4
2023-04-03 10:56:39 - 
[#Step 810000] eval_reward: 3418.41, eval_step: 1000, eval_time: 2, time: 14.46
	critic_loss: 25.88, actor_loss: -244.37
	q1: 241.93, max_q1: 319.87, min_q1: -13.53
	batch_reward: 2.69, batch_reward_max: 5.61, batch_reward_min: -1.61

2023-04-03 10:56:51 - 
[#Step 820000] eval_reward: 3426.41, eval_step: 1000, eval_time: 2, time: 14.66
	critic_loss: 21.66, actor_loss: -250.76
	q1: 249.05, max_q1: 325.33, min_q1: 3.54
	batch_reward: 2.79, batch_reward_max: 6.32, batch_reward_min: -0.93

2023-04-03 10:57:03 - 
[#Step 830000] eval_reward: 3402.76, eval_step: 1000, eval_time: 3, time: 14.86
	critic_loss: 29.15, actor_loss: -248.80
	q1: 246.78, max_q1: 335.28, min_q1: -23.62
	batch_reward: 2.78, batch_reward_max: 6.28, batch_reward_min: -1.13

2023-04-03 10:57:14 - 
[#Step 840000] eval_reward: 2965.00, eval_step: 914, eval_time: 2, time: 15.05
	critic_loss: 35.95, actor_loss: -246.76
	q1: 243.52, max_q1: 345.85, min_q1: -5.69
	batch_reward: 2.74, batch_reward_max: 6.07, batch_reward_min: -0.86

2023-04-03 10:57:26 - 
[#Step 850000] eval_reward: 3378.40, eval_step: 1000, eval_time: 2, time: 15.25
	critic_loss: 30.12, actor_loss: -248.75
	q1: 246.60, max_q1: 327.62, min_q1: 24.94
	batch_reward: 2.63, batch_reward_max: 6.19, batch_reward_min: -0.96

2023-04-03 10:57:38 - 
[#Step 860000] eval_reward: 3280.49, eval_step: 1000, eval_time: 3, time: 15.45
	critic_loss: 28.87, actor_loss: -245.25
	q1: 242.88, max_q1: 341.45, min_q1: -5.04
	batch_reward: 2.68, batch_reward_max: 5.82, batch_reward_min: -0.66

2023-04-03 10:57:50 - 
[#Step 870000] eval_reward: 3264.46, eval_step: 1000, eval_time: 2, time: 15.65
	critic_loss: 30.99, actor_loss: -252.11
	q1: 250.38, max_q1: 333.19, min_q1: 33.99
	batch_reward: 2.81, batch_reward_max: 6.32, batch_reward_min: -0.20

2023-04-03 10:58:01 - 
[#Step 880000] eval_reward: 3427.96, eval_step: 1000, eval_time: 2, time: 15.84
	critic_loss: 31.84, actor_loss: -253.75
	q1: 251.04, max_q1: 349.66, min_q1: -4.60
	batch_reward: 2.81, batch_reward_max: 5.56, batch_reward_min: -1.13

2023-04-03 10:58:13 - 
[#Step 890000] eval_reward: 3420.16, eval_step: 1000, eval_time: 2, time: 16.03
	critic_loss: 32.05, actor_loss: -253.31
	q1: 251.49, max_q1: 347.91, min_q1: -33.80
	batch_reward: 2.84, batch_reward_max: 5.49, batch_reward_min: -0.65

2023-04-03 10:58:24 - 
[#Step 900000] eval_reward: 3388.68, eval_step: 1000, eval_time: 2, time: 16.22
	critic_loss: 50.47, actor_loss: -260.78
	q1: 258.75, max_q1: 334.20, min_q1: 25.95
	batch_reward: 2.80, batch_reward_max: 6.46, batch_reward_min: -0.86

2023-04-03 10:58:36 - 
[#Step 910000] eval_reward: 3528.10, eval_step: 1000, eval_time: 2, time: 16.41
	critic_loss: 40.49, actor_loss: -254.73
	q1: 252.68, max_q1: 335.33, min_q1: 9.12
	batch_reward: 2.82, batch_reward_max: 6.05, batch_reward_min: -1.17

2023-04-03 10:58:47 - 
[#Step 920000] eval_reward: 3662.95, eval_step: 1000, eval_time: 2, time: 16.60
	critic_loss: 36.50, actor_loss: -251.60
	q1: 249.03, max_q1: 348.15, min_q1: -9.98
	batch_reward: 2.67, batch_reward_max: 6.04, batch_reward_min: -1.70

2023-04-03 10:58:59 - 
[#Step 930000] eval_reward: 3532.95, eval_step: 1000, eval_time: 2, time: 16.80
	critic_loss: 21.45, actor_loss: -258.15
	q1: 256.65, max_q1: 349.26, min_q1: -0.68
	batch_reward: 2.90, batch_reward_max: 6.06, batch_reward_min: -1.03

2023-04-03 10:59:11 - 
[#Step 940000] eval_reward: 3413.20, eval_step: 1000, eval_time: 2, time: 16.99
	critic_loss: 20.92, actor_loss: -253.27
	q1: 251.39, max_q1: 356.23, min_q1: 7.02
	batch_reward: 2.81, batch_reward_max: 6.34, batch_reward_min: -0.53

2023-04-03 10:59:22 - 
[#Step 950000] eval_reward: 3578.26, eval_step: 1000, eval_time: 2, time: 17.18
	critic_loss: 27.83, actor_loss: -247.78
	q1: 245.79, max_q1: 355.05, min_q1: -5.09
	batch_reward: 2.62, batch_reward_max: 5.55, batch_reward_min: -1.19

2023-04-03 10:59:29 - 
[#Step 955000] eval_reward: 3721.23, eval_step: 1000, eval_time: 2, time: 17.30
	critic_loss: 29.51, actor_loss: -256.44
	q1: 253.82, max_q1: 355.80, min_q1: -6.94
	batch_reward: 2.84, batch_reward_max: 6.03, batch_reward_min: -1.43

2023-04-03 10:59:36 - 
[#Step 960000] eval_reward: 3698.94, eval_step: 1000, eval_time: 2, time: 17.41
	critic_loss: 34.27, actor_loss: -262.38
	q1: 260.46, max_q1: 358.21, min_q1: -0.98
	batch_reward: 2.82, batch_reward_max: 6.17, batch_reward_min: -0.69

2023-04-03 10:59:43 - 
[#Step 965000] eval_reward: 3655.22, eval_step: 1000, eval_time: 2, time: 17.53
	critic_loss: 30.75, actor_loss: -260.70
	q1: 258.43, max_q1: 353.35, min_q1: 3.55
	batch_reward: 2.86, batch_reward_max: 6.19, batch_reward_min: -1.06

2023-04-03 10:59:50 - 
[#Step 970000] eval_reward: 3791.61, eval_step: 1000, eval_time: 2, time: 17.64
	critic_loss: 40.55, actor_loss: -263.17
	q1: 261.38, max_q1: 353.20, min_q1: 12.10
	batch_reward: 2.92, batch_reward_max: 6.08, batch_reward_min: -0.71

2023-04-03 10:59:57 - 
[#Step 975000] eval_reward: 3731.32, eval_step: 1000, eval_time: 2, time: 17.76
	critic_loss: 24.19, actor_loss: -261.79
	q1: 260.29, max_q1: 351.16, min_q1: 12.84
	batch_reward: 2.73, batch_reward_max: 6.01, batch_reward_min: -0.92

2023-04-03 11:00:03 - 
[#Step 980000] eval_reward: 3551.76, eval_step: 1000, eval_time: 2, time: 17.87
	critic_loss: 30.05, actor_loss: -260.56
	q1: 258.55, max_q1: 352.84, min_q1: -15.51
	batch_reward: 2.68, batch_reward_max: 6.43, batch_reward_min: -2.39

2023-04-03 11:00:10 - 
[#Step 985000] eval_reward: 3779.10, eval_step: 1000, eval_time: 2, time: 17.99
	critic_loss: 47.28, actor_loss: -266.55
	q1: 264.00, max_q1: 355.31, min_q1: 36.28
	batch_reward: 2.80, batch_reward_max: 5.67, batch_reward_min: -1.20

2023-04-03 11:00:17 - 
[#Step 990000] eval_reward: 3775.54, eval_step: 1000, eval_time: 2, time: 18.10
	critic_loss: 33.54, actor_loss: -256.76
	q1: 253.57, max_q1: 357.37, min_q1: -16.37
	batch_reward: 2.74, batch_reward_max: 6.30, batch_reward_min: -1.48

2023-04-03 11:00:24 - 
[#Step 995000] eval_reward: 3820.32, eval_step: 1000, eval_time: 2, time: 18.22
	critic_loss: 24.07, actor_loss: -262.08
	q1: 259.32, max_q1: 361.00, min_q1: -79.42
	batch_reward: 2.77, batch_reward_max: 5.99, batch_reward_min: -2.20

2023-04-03 11:00:31 - 
[#Step 1000000] eval_reward: 3654.77, eval_step: 1000, eval_time: 2, time: 18.33
	critic_loss: 22.05, actor_loss: -266.38
	q1: 264.57, max_q1: 356.35, min_q1: 0.70
	batch_reward: 2.87, batch_reward_max: 5.59, batch_reward_min: -1.06

2023-04-03 11:00:31 - Saving checkpoint at step: 5
2023-04-03 11:00:31 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/actor_5
2023-04-03 11:00:31 - Saving checkpoint at step: 5
2023-04-03 11:00:31 - Saved checkpoint at saved_models/td3/Walker2d-v3/s0_20230403_104211/critic_5
